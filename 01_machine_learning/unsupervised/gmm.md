# Gaussian Mixture Models (GMM)

Clustering probabiliste via m√©langes gaussiens (EM).

## Id√©e cl√©

**GMM (Gaussian Mixture Model)** est un mod√®le de **clustering probabiliste** qui suppose que les donn√©es proviennent d'un **m√©lange de plusieurs distributions gaussiennes**. Contrairement √† K-Means qui assigne chaque point √† un seul cluster (hard assignment), GMM calcule des **probabilit√©s d'appartenance** pour chaque cluster (soft assignment).

**Principe** :
1. Les donn√©es sont g√©n√©r√©es par K composantes gaussiennes
2. Chaque composante a sa propre **moyenne** (Œº‚Çñ), **covariance** (Œ£‚Çñ), et **poids** (œÄ‚Çñ)
3. Pour chaque point, calculer la probabilit√© d'appartenir √† chaque cluster
4. Utiliser l'algorithme **EM (Expectation-Maximization)** pour optimiser les param√®tres

**Formule de densit√©** :
```
p(x) = Œ£ œÄ‚Çñ ¬∑ N(x | Œº‚Çñ, Œ£‚Çñ)
      k=1..K

O√π:
- œÄ‚Çñ : poids de la composante k (Œ£œÄ‚Çñ = 1)
- N(x | Œº‚Çñ, Œ£‚Çñ) : distribution gaussienne
- K : nombre de composantes
```

**Probabilit√© d'appartenance (soft assignment)** :
```
p(k | x) = œÄ‚Çñ ¬∑ N(x | Œº‚Çñ, Œ£‚Çñ) / p(x)

‚Üí Probabilit√© que x appartienne au cluster k
```

**Algorithme EM** :
```
Initialisation: Œº‚Çñ, Œ£‚Çñ, œÄ‚Çñ al√©atoires

R√©p√©ter jusqu'√† convergence:
  E-step: Calculer p(k|x) pour chaque point x
  M-step: Mettre √† jour Œº‚Çñ, Œ£‚Çñ, œÄ‚Çñ avec maximum de vraisemblance
```

**Diff√©rence avec K-Means** :
| Aspect | K-Means | GMM |
|--------|---------|-----|
| **Assignment** | Hard (1 cluster) | Soft (probabilit√©s) |
| **Forme clusters** | Sph√©riques | Ellipso√Ødales (covariances) |
| **Output** | Labels | Probabilit√©s |
| **Algorithme** | Lloyd | EM |
| **Robustesse** | Sensible aux outliers | Plus robuste |
| **Vitesse** | Rapide | Plus lent |

## Exemples concrets

### 1. Clustering probabiliste : Donn√©es 2D

**Sc√©nario** : Identifier des clusters ellipso√Ødaux dans des donn√©es 2D.

**Code Python avec GMM** :
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs

# 1. G√©n√©rer donn√©es avec 3 clusters
X, y_true = make_blobs(
    n_samples=300, 
    centers=3,
    cluster_std=[1.0, 1.5, 0.5],  # Variances diff√©rentes
    random_state=42
)

# 2. Entra√Æner GMM
gmm = GaussianMixture(
    n_components=3,        # Nombre de composantes gaussiennes
    covariance_type='full', # Type de matrice de covariance
    max_iter=100,
    random_state=42
)
gmm.fit(X)

# 3. Pr√©dire les clusters
y_pred = gmm.predict(X)

# 4. Probabilit√©s d'appartenance (soft assignment)
probas = gmm.predict_proba(X)
print("Probabilit√©s pour le premier point:")
print(f"  Cluster 0: {probas[0, 0]:.2%}")
print(f"  Cluster 1: {probas[0, 1]:.2%}")
print(f"  Cluster 2: {probas[0, 2]:.2%}")

# 5. Param√®tres des composantes
print("\nParam√®tres des composantes gaussiennes:")
for i in range(3):
    print(f"\nComposante {i}:")
    print(f"  Moyenne: {gmm.means_[i]}")
    print(f"  Poids: {gmm.weights_[i]:.3f}")
    print(f"  Covariance shape: {gmm.covariances_[i].shape}")

# 6. Score (log-vraisemblance moyenne)
score = gmm.score(X)
print(f"\nLog-likelihood moyenne: {score:.3f}")

# 7. Visualisation
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Donn√©es originales
ax = axes[0]
ax.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.6)
ax.set_title('Donn√©es originales')
ax.set_xlabel('Feature 1')
ax.set_ylabel('Feature 2')

# Pr√©dictions GMM avec ellipses
ax = axes[1]
ax.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.6)

# Tracer les ellipses (2 std)
from matplotlib.patches import Ellipse

def draw_ellipse(position, covariance, ax, **kwargs):
    # D√©composition en valeurs propres
    eigenvalues, eigenvectors = np.linalg.eigh(covariance)
    angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))
    width, height = 2 * 2 * np.sqrt(eigenvalues)  # 2 std
    
    ellipse = Ellipse(position, width, height, angle=angle, **kwargs)
    ax.add_patch(ellipse)

for i in range(3):
    draw_ellipse(
        gmm.means_[i], 
        gmm.covariances_[i], 
        ax, 
        alpha=0.2, 
        edgecolor='red', 
        linewidth=2,
        facecolor='none'
    )
    # Marquer les centres
    ax.plot(gmm.means_[i, 0], gmm.means_[i, 1], 'rx', markersize=15, markeredgewidth=3)

ax.set_title('GMM Clustering (avec ellipses de covariance)')
ax.set_xlabel('Feature 1')
ax.set_ylabel('Feature 2')

plt.tight_layout()
plt.show()
```

---

### 2. Comparaison K-Means vs GMM

**Code pour montrer la diff√©rence** :
```python
from sklearn.cluster import KMeans

# Donn√©es avec clusters ellipso√Ødaux (non-sph√©riques)
np.random.seed(42)
X_ellipse = np.dot(
    np.random.randn(200, 2),
    [[2, 0], [0, 0.5]]  # Matrice d'√©tirement
) + [5, 5]

X_circle = np.random.randn(200, 2) + [0, 0]
X = np.vstack([X_ellipse, X_circle])

# K-Means (assume clusters sph√©riques)
kmeans = KMeans(n_clusters=2, random_state=42)
y_kmeans = kmeans.fit_predict(X)

# GMM (peut mod√©liser ellipses)
gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=42)
y_gmm = gmm.fit_predict(X)

# Visualisation
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# K-Means
ax = axes[0]
ax.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=30, alpha=0.6)
ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 
           c='red', s=200, marker='X', edgecolors='black', linewidth=2, label='Centres')
ax.set_title('K-Means (clusters sph√©riques)')
ax.legend()

# GMM
ax = axes[1]
ax.scatter(X[:, 0], X[:, 1], c=y_gmm, cmap='viridis', s=30, alpha=0.6)
for i in range(2):
    draw_ellipse(gmm.means_[i], gmm.covariances_[i], ax, 
                 alpha=0.2, edgecolor='red', linewidth=2, facecolor='none')
    ax.plot(gmm.means_[i, 0], gmm.means_[i, 1], 'rx', markersize=15, markeredgewidth=3)
ax.set_title('GMM (clusters ellipso√Ødaux)')

plt.tight_layout()
plt.show()
```

---

### 3. Density Estimation : G√©n√©rer de nouvelles donn√©es

**Sc√©nario** : Utiliser GMM pour mod√©liser la densit√© et g√©n√©rer des √©chantillons synth√©tiques.

**Code Python** :
```python
# 1. Entra√Æner GMM sur donn√©es existantes
X, _ = make_blobs(n_samples=300, centers=3, random_state=42)

gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
gmm.fit(X)

# 2. G√©n√©rer de nouveaux √©chantillons
n_samples = 500
X_new, y_new = gmm.sample(n_samples)

# 3. Calculer densit√© de probabilit√©
x_min, x_max = X[:, 0].min() - 2, X[:, 0].max() + 2
y_min, y_max = X[:, 1].min() - 2, X[:, 1].max() + 2
xx, yy = np.meshgrid(
    np.linspace(x_min, x_max, 100),
    np.linspace(y_min, y_max, 100)
)
Z = np.exp(gmm.score_samples(np.c_[xx.ravel(), yy.ravel()]))
Z = Z.reshape(xx.shape)

# 4. Visualisation
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Donn√©es originales + densit√©
ax = axes[0]
ax.contourf(xx, yy, Z, levels=20, cmap='Blues', alpha=0.6)
ax.scatter(X[:, 0], X[:, 1], c='red', s=30, alpha=0.6, label='Donn√©es originales')
ax.set_title('Donn√©es originales + Densit√© estim√©e')
ax.legend()

# √âchantillons g√©n√©r√©s
ax = axes[1]
ax.contourf(xx, yy, Z, levels=20, cmap='Blues', alpha=0.6)
ax.scatter(X_new[:, 0], X_new[:, 1], c=y_new, cmap='viridis', s=30, alpha=0.6, label='√âchantillons g√©n√©r√©s')
ax.set_title('Nouveaux √©chantillons g√©n√©r√©s par GMM')
ax.legend()

plt.tight_layout()
plt.show()

print(f"G√©n√©r√© {n_samples} nouveaux √©chantillons")
```

---

### 4. S√©lection du nombre de composantes (BIC/AIC)

**Code pour choisir K optimal** :
```python
from sklearn.mixture import GaussianMixture

# Donn√©es
X, _ = make_blobs(n_samples=300, centers=4, random_state=42)

# Tester diff√©rents nombres de composantes
n_components_range = range(1, 10)
bic_scores = []
aic_scores = []

for n_components in n_components_range:
    gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)
    gmm.fit(X)
    bic_scores.append(gmm.bic(X))
    aic_scores.append(gmm.aic(X))

# Visualisation
plt.figure(figsize=(10, 5))
plt.plot(n_components_range, bic_scores, 'o-', label='BIC')
plt.plot(n_components_range, aic_scores, 's-', label='AIC')
plt.xlabel('Nombre de composantes')
plt.ylabel('Score (plus bas = meilleur)')
plt.title('S√©lection du nombre de composantes GMM')
plt.legend()
plt.grid(True)
plt.show()

optimal_bic = n_components_range[np.argmin(bic_scores)]
optimal_aic = n_components_range[np.argmin(aic_scores)]
print(f"Optimal (BIC): {optimal_bic} composantes")
print(f"Optimal (AIC): {optimal_aic} composantes")
```

---

### 5. Types de covariance

**Code pour comparer les types de matrices de covariance** :
```python
# Donn√©es
X, _ = make_blobs(n_samples=300, centers=3, random_state=42)

# 4 types de covariance
covariance_types = ['full', 'tied', 'diag', 'spherical']
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.ravel()

for idx, cov_type in enumerate(covariance_types):
    gmm = GaussianMixture(n_components=3, covariance_type=cov_type, random_state=42)
    y_pred = gmm.fit_predict(X)
    
    ax = axes[idx]
    ax.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=30, alpha=0.6)
    
    # Tracer ellipses si 'full' ou 'tied'
    if cov_type in ['full', 'tied']:
        for i in range(3):
            if cov_type == 'full':
                cov = gmm.covariances_[i]
            else:  # tied
                cov = gmm.covariances_
            draw_ellipse(gmm.means_[i], cov, ax, 
                        alpha=0.2, edgecolor='red', linewidth=2, facecolor='none')
    
    ax.plot(gmm.means_[:, 0], gmm.means_[:, 1], 'rx', markersize=15, markeredgewidth=3)
    ax.set_title(f'{cov_type.capitalize()}\nBIC: {gmm.bic(X):.1f}')

plt.tight_layout()
plt.show()
```

**Explication des types** :
- **full** : Chaque composante a sa propre matrice de covariance compl√®te (K matrices de d√ód)
- **tied** : Toutes les composantes partagent la m√™me matrice de covariance (1 matrice de d√ód)
- **diag** : Matrices diagonales (variances seulement, pas de corr√©lations) (K matrices diagonales)
- **spherical** : Une seule variance par composante (K scalaires)

---

### 6. Anomaly Detection avec GMM

**Code pour d√©tecter des outliers** :
```python
# 1. Donn√©es normales
X_normal, _ = make_blobs(n_samples=300, centers=2, random_state=42)

# 2. Ajouter des outliers
np.random.seed(42)
X_outliers = np.random.uniform(low=-10, high=10, size=(20, 2))
X = np.vstack([X_normal, X_outliers])

# 3. Entra√Æner GMM
gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=42)
gmm.fit(X_normal)  # Entra√Æner seulement sur donn√©es normales

# 4. Calculer densit√©s (log-likelihood)
densities = gmm.score_samples(X)

# 5. Seuil pour anomalies (percentile)
threshold = np.percentile(densities, 5)  # 5% les plus faibles
is_anomaly = densities < threshold

# 6. Visualisation
plt.figure(figsize=(10, 6))
plt.scatter(X[~is_anomaly, 0], X[~is_anomaly, 1], 
            c='blue', s=30, alpha=0.6, label='Normal')
plt.scatter(X[is_anomaly, 0], X[is_anomaly, 1], 
            c='red', s=100, marker='X', label='Anomalies', edgecolors='black', linewidth=2)

# Contour de densit√©
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
Z = np.exp(gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])).reshape(xx.shape)
plt.contour(xx, yy, Z, levels=10, colors='green', alpha=0.3)

plt.title('Anomaly Detection avec GMM')
plt.legend()
plt.show()

print(f"Anomalies d√©tect√©es: {is_anomaly.sum()}/{len(X)}")
```

## Quand l'utiliser

- ‚úÖ **Clustering soft** : Besoin de probabilit√©s d'appartenance (pas juste labels)
- ‚úÖ **Clusters non-sph√©riques** : Ellipses, formes allong√©es (covariances)
- ‚úÖ **Density estimation** : Mod√©liser la distribution des donn√©es
- ‚úÖ **G√©n√©ration de donn√©es** : Cr√©er des √©chantillons synth√©tiques
- ‚úÖ **Anomaly detection** : Identifier outliers (faible densit√©)
- ‚úÖ **Donn√©es avec incertitude** : Soft assignments utiles

**Cas d'usage typiques** :
- üé® **Segmentation d'images** : Couleurs, r√©gions
- üîä **Traitement audio** : Reconnaissance de phon√®mes
- üìä **Analyse de donn√©es** : Identifier sous-populations
- üè• **M√©decine** : Groupes de patients, sous-types de maladies
- üí∞ **Finance** : Segmentation de clients, r√©gimes de march√©

**Quand NE PAS utiliser** :
- ‚ùå Besoin de hard assignments uniquement ‚Üí K-Means plus rapide
- ‚ùå Tr√®s grandes donn√©es (>100k) ‚Üí K-Means ou Mini-Batch K-Means
- ‚ùå Clusters non-gaussiens (formes complexes) ‚Üí DBSCAN, Hierarchical
- ‚ùå Haute dimensionnalit√© (>50 features) ‚Üí Co√ªteux en m√©moire

## Forces

‚úÖ **Soft clustering** : Probabilit√©s d'appartenance (incertitude)  
‚úÖ **Flexibilit√© formes** : Ellipses via matrices de covariance  
‚úÖ **Mod√®le g√©n√©ratif** : Peut g√©n√©rer de nouveaux √©chantillons  
‚úÖ **Density estimation** : Mod√©lise p(x) compl√®tement  
‚úÖ **Base th√©orique** : Maximum de vraisemblance bien d√©fini  
‚úÖ **Anomaly detection** : Naturel via densit√© faible

**Exemple de soft clustering** :
```python
# Point √† la fronti√®re entre 2 clusters
gmm = GaussianMixture(n_components=2, random_state=42)
gmm.fit(X)

# Point ambigu√´
x_ambiguous = np.array([[0, 0]])
probas = gmm.predict_proba(x_ambiguous)[0]

print(f"Probabilit√©s:")
print(f"  Cluster 0: {probas[0]:.1%}")  # 45%
print(f"  Cluster 1: {probas[1]:.1%}")  # 55%
# ‚Üí GMM capture l'incertitude !

# K-Means donne un choix binaire
kmeans = KMeans(n_clusters=2)
kmeans.fit(X)
label = kmeans.predict(x_ambiguous)[0]
print(f"\nK-Means: Cluster {label}")  # 0 ou 1 cat√©gorique
```

## Limites

‚ùå **Sensible √† l'initialisation** : Peut converger vers optimum local  
‚ùå **Choix de K difficile** : Nombre de composantes √† d√©terminer (BIC/AIC)  
‚ùå **Complexit√© O(K¬∑d¬≤¬∑n)** : Lent pour grandes donn√©es ou haute dimension  
‚ùå **Hypoth√®se gaussienne** : Clusters non-gaussiens mal mod√©lis√©s  
‚ùå **Singularit√©s** : Covariances peuvent devenir singuli√®res  
‚ùå **M√©moire** : Stockage de K matrices de covariance (d√ód)

**Probl√®me d'initialisation** :
```python
# M√™me donn√©es, initialisations diff√©rentes ‚Üí r√©sultats diff√©rents
scores = []
for i in range(10):
    gmm = GaussianMixture(n_components=3, random_state=i)
    gmm.fit(X)
    scores.append(gmm.score(X))

print(f"Log-likelihood min: {min(scores):.2f}")
print(f"Log-likelihood max: {max(scores):.2f}")
print(f"Diff√©rence: {max(scores) - min(scores):.2f}")

# Solution: Plusieurs initialisations (n_init)
gmm = GaussianMixture(n_components=3, n_init=10)  # Essayer 10 fois
```

**Singularit√©s de covariance** :
```python
# Si tous les points d'un cluster sont identiques
# ‚Üí Covariance = 0 ‚Üí Singularit√© !

# Solution: R√©gularisation
gmm = GaussianMixture(
    n_components=3, 
    reg_covar=1e-6  # Ajouter une petite constante √† la diagonale
)
```

## Variantes / liens

### Hyperparam√®tres cl√©s

```python
GaussianMixture(
    n_components=3,           # Nombre de composantes gaussiennes
    covariance_type='full',   # 'full', 'tied', 'diag', 'spherical'
    tol=1e-3,                 # Seuil de convergence EM
    max_iter=100,             # Nombre max d'it√©rations EM
    n_init=1,                 # Nombre d'initialisations diff√©rentes
    init_params='kmeans',     # 'kmeans', 'random', 'k-means++'
    reg_covar=1e-6,           # R√©gularisation pour √©viter singularit√©s
    random_state=42
)
```

**Recommandations** :
- **n_components** : Utiliser BIC/AIC pour s√©lectionner
- **covariance_type** : 
  - `'full'` : Maximum de flexibilit√© (d√©faut)
  - `'diag'` : Si features ind√©pendantes, plus rapide
  - `'spherical'` : Si clusters sph√©riques, tr√®s rapide
- **n_init** : Au moins 10 pour robustesse
- **reg_covar** : Augmenter si erreurs de singularit√©

### Algorithme EM d√©taill√©

**E-step (Expectation)** :
```python
# Pour chaque point x_i et composante k:
# Calculer responsabilit√© Œ≥(z_k) = p(k | x_i)

Œ≥[i, k] = œÄ[k] * N(x[i] | Œº[k], Œ£[k]) / Œ£_j œÄ[j] * N(x[i] | Œº[j], Œ£[j])
```

**M-step (Maximization)** :
```python
# Mettre √† jour param√®tres avec maximum de vraisemblance

N_k = Œ£_i Œ≥[i, k]  # Nombre effectif de points dans cluster k

Œº[k] = (1/N_k) * Œ£_i Œ≥[i, k] * x[i]
Œ£[k] = (1/N_k) * Œ£_i Œ≥[i, k] * (x[i] - Œº[k])(x[i] - Œº[k])^T
œÄ[k] = N_k / n
```

### Crit√®res de s√©lection

**BIC (Bayesian Information Criterion)** :
```
BIC = -2 * log-likelihood + p * log(n)

p = nombre de param√®tres libres
n = nombre d'exemples

‚Üí Plus petit = meilleur (p√©nalise complexit√©)
```

**AIC (Akaike Information Criterion)** :
```
AIC = -2 * log-likelihood + 2 * p

‚Üí P√©nalise moins la complexit√© que BIC
```

### Relations avec d'autres mod√®les

- **K-Means** : Cas particulier de GMM (covariances sph√©riques, identiques)
- **EM Algorithm** : Algorithme g√©n√©ral (GMM est une application)
- **Naive Bayes** : Utilise aussi gaussiennes (mais pour classification)
- **Hidden Markov Models** : GMM pour mod√©liser √©missions
- **Factor Analysis** : R√©duction de dimension probabiliste
- **Variational Autoencoders** : Extension deep learning de GMM

### Variantes avanc√©es

**1. Bayesian GMM** :
```python
from sklearn.mixture import BayesianGaussianMixture

# Approche bay√©sienne avec prior Dirichlet
bgmm = BayesianGaussianMixture(
    n_components=10,         # Max composantes
    weight_concentration_prior=1e-3,  # Prior sur poids (favorise peu de composantes)
    covariance_type='full'
)
bgmm.fit(X)

# D√©termine automatiquement le nombre effectif de composantes
effective_components = (bgmm.weights_ > 0.01).sum()
print(f"Composantes effectives: {effective_components}")
```

**2. GMM avec features manquantes** :
```python
# Imputation via EM
from sklearn.impute import IterativeImputer

imputer = IterativeImputer(estimator=GaussianMixture(n_components=2))
X_imputed = imputer.fit_transform(X_with_missing)
```

## R√©f√©rences

### Documentation et tutoriels
- **Scikit-learn** : [Gaussian Mixture](https://scikit-learn.org/stable/modules/mixture.html)
- **StatQuest** : [EM Algorithm](https://www.youtube.com/watch?v=REypj2sy_5U) (YouTube)

### Livres
- **"Pattern Recognition and Machine Learning"** (Bishop, 2006) - Chapitre 9
- **"The Elements of Statistical Learning"** (Hastie et al., 2009) - Chapitre 14
- **"Machine Learning: A Probabilistic Perspective"** (Murphy, 2012) - Chapitre 11

### Papers fondamentaux
- **EM Algorithm** : Dempster, Laird & Rubin, 1977 - "Maximum Likelihood from Incomplete Data via the EM Algorithm"
- **GMM** : McLachlan & Peel, 2000 - "Finite Mixture Models"
- **Bayesian GMM** : Rasmussen, 2000 - "The Infinite Gaussian Mixture Model"

### Th√©orie

**Algorithme EM** :
```
Objectif: Maximiser log p(X | Œ∏)

E-step:  Q(Œ∏ | Œ∏^old) = E[log p(X, Z | Œ∏) | X, Œ∏^old]
M-step:  Œ∏^new = argmax Q(Œ∏ | Œ∏^old)

Garantie: log p(X | Œ∏^new) ‚â• log p(X | Œ∏^old)

‚Üí Converge vers optimum local
```

**Comparaison de performance** :
```
Dataset: 10,000 points, 5 clusters

Algorithme     Temps    M√©moire    Soft?
K-Means        0.02s    1 MB       Non
GMM (full)     0.5s     5 MB       Oui
GMM (diag)     0.2s     2 MB       Oui
DBSCAN         1.5s     3 MB       Non

‚Üí GMM: Bon compromis si soft clustering n√©cessaire
```

### Tuning rapide (r√®gles empiriques)

**Workflow recommand√©** :
```python
# 1. Tester K-Means d'abord (baseline)
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)

# 2. Si besoins soft clustering ou clusters ellipso√Ødaux
# S√©lectionner K avec BIC
from sklearn.mixture import GaussianMixture

bic_scores = []
for k in range(1, 10):
    gmm = GaussianMixture(n_components=k, n_init=10)
    gmm.fit(X)
    bic_scores.append(gmm.bic(X))

optimal_k = np.argmin(bic_scores) + 1

# 3. Entra√Æner mod√®le final
gmm_final = GaussianMixture(
    n_components=optimal_k,
    covariance_type='full',
    n_init=10,
    random_state=42
)
gmm_final.fit(X)
```

**Choix du type de covariance** :
```python
# Si features ind√©pendantes (pas de corr√©lations)
‚Üí covariance_type='diag' (plus rapide)

# Si clusters de m√™me forme
‚Üí covariance_type='tied' (partage une covariance)

# Si clusters sph√©riques
‚Üí covariance_type='spherical' (comme K-Means)

# Sinon (flexibilit√© maximale)
‚Üí covariance_type='full' (d√©faut)
```
