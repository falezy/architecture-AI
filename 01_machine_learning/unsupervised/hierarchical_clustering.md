# Hierarchical Clustering

Clustering hi√©rarchique (dendrogramme).

## Id√©e cl√©

**Hierarchical Clustering** est un algorithme de clustering qui construit une **hi√©rarchie de clusters** repr√©sent√©e par un **dendrogramme** (arbre). Contrairement √† K-Means qui n√©cessite de sp√©cifier K √† l'avance, le clustering hi√©rarchique produit une structure compl√®te permettant de choisir n'importe quel nombre de clusters en coupant l'arbre √† diff√©rents niveaux.

**Deux approches** :

1. **Agglomerative (bottom-up)** : ‚≠ê Plus courant
   - D√©part : Chaque point est un cluster
   - R√©p√©ter : Fusionner les 2 clusters les plus proches
   - Fin : Un seul cluster contenant tous les points

2. **Divisive (top-down)** :
   - D√©part : Tous les points dans un cluster
   - R√©p√©ter : Diviser le cluster le plus h√©t√©rog√®ne
   - Fin : Chaque point est son propre cluster

**Algorithme agglom√©ratif** :
```
1. Initialiser: N clusters (1 point par cluster)
2. R√©p√©ter jusqu'√† 1 cluster:
   a. Calculer distances entre tous les paires de clusters
   b. Fusionner les 2 clusters les plus proches (selon linkage)
   c. Mettre √† jour la matrice de distances
3. R√©sultat: Dendrogramme
```

**Dendrogramme** :
```
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ        ‚îÇ
      ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê
      ‚îÇ     ‚îÇ  ‚îÇ     ‚îÇ
    ‚îå‚îÄ‚î¥‚îÄ‚îê ‚îå‚îÄ‚î¥‚îÄ‚îê‚îÇ   ‚îå‚îÄ‚î¥‚îÄ‚îê
    ‚Ä¢   ‚Ä¢ ‚Ä¢   ‚Ä¢‚Ä¢   ‚Ä¢   ‚Ä¢
   p1  p2 p3 p4p5  p6  p7

Height = distance de fusion
Couper √† hauteur h ‚Üí K clusters
```

**Linkage criteria (crit√®res de fusion)** :
- **Single** : Distance minimale entre points
- **Complete** : Distance maximale entre points
- **Average** : Distance moyenne entre tous les points
- **Ward** : Minimise la variance intra-cluster (meilleur en g√©n√©ral)

## Exemples concrets

### 1. Clustering hi√©rarchique avec dendrogramme

**Sc√©nario** : Grouper des donn√©es 2D et visualiser la hi√©rarchie.

**Code Python avec AgglomerativeClustering** :
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_blobs
from scipy.cluster.hierarchy import dendrogram, linkage

# 1. G√©n√©rer donn√©es
X, y_true = make_blobs(n_samples=50, centers=3, random_state=42)

# 2. Calculer matrice de distances et linkage pour dendrogramme
Z = linkage(X, method='ward')  # 'single', 'complete', 'average', 'ward'

# 3. Visualiser dendrogramme
plt.figure(figsize=(12, 5))
dendrogram(
    Z,
    truncate_mode='lastp',  # Montrer seulement les p derni√®res fusions
    p=12,
    leaf_rotation=90,
    leaf_font_size=10,
    show_contracted=True
)
plt.title('Dendrogramme - Ward Linkage')
plt.xlabel('Index du cluster ou (nombre de points)')
plt.ylabel('Distance')
plt.axhline(y=10, color='r', linestyle='--', label='Seuil de coupure')
plt.legend()
plt.show()

# 4. Clustering avec nombre de clusters fix√©
n_clusters = 3
model = AgglomerativeClustering(
    n_clusters=n_clusters,
    linkage='ward'
)
y_pred = model.fit_predict(X)

# 5. Visualisation des clusters
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=100, edgecolors='black')
plt.title(f'Hierarchical Clustering (Ward, K={n_clusters})')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Cluster')
plt.show()

print(f"Clusters trouv√©s: {n_clusters}")
print(f"Labels: {np.unique(y_pred)}")
```

---

### 2. Comparaison des linkage methods

**Code pour voir l'impact du crit√®re de liaison** :
```python
from sklearn.datasets import make_moons

# Donn√©es avec forme non-convexe
X, _ = make_moons(n_samples=200, noise=0.05, random_state=42)

# Tester 4 linkage methods
linkages = ['single', 'complete', 'average', 'ward']
fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.ravel()

for idx, linkage_method in enumerate(linkages):
    # Clustering
    model = AgglomerativeClustering(n_clusters=2, linkage=linkage_method)
    y_pred = model.fit_predict(X)
    
    # Visualisation
    ax = axes[idx]
    ax.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=50, edgecolors='black')
    ax.set_title(f'Linkage: {linkage_method.capitalize()}')
    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')

plt.tight_layout()
plt.show()
```

**R√©sultats typiques** :
- **Single** : Bon pour formes allong√©es/non-convexes (effet "cha√Æne")
- **Complete** : Clusters compacts, sph√©riques
- **Average** : Compromis
- **Ward** : Meilleur en g√©n√©ral, minimise variance

---

### 3. Dendrogramme d√©taill√© pour petits datasets

**Code pour comprendre la structure hi√©rarchique** :
```python
# Petit dataset pour visualisation claire
np.random.seed(42)
X_small = np.random.randn(10, 2) * 0.5
X_small[:3] += [2, 2]   # Cluster 1
X_small[3:6] += [0, 0]  # Cluster 2
X_small[6:] += [4, 0]   # Cluster 3

# Calcul linkage
Z = linkage(X_small, method='ward')

# Dendrogramme complet (tous les points)
plt.figure(figsize=(12, 6))
dendrogram(
    Z,
    labels=[f'P{i}' for i in range(len(X_small))],  # Labels des points
    leaf_rotation=0,
    leaf_font_size=12
)
plt.title('Dendrogramme Complet - 10 Points')
plt.xlabel('Points')
plt.ylabel('Distance de Ward')
plt.grid(True, alpha=0.3)
plt.show()

# Afficher la matrice de linkage
print("Matrice de linkage (Z):")
print("Cluster1  Cluster2  Distance  Nombre de points")
for i, row in enumerate(Z):
    print(f"{int(row[0]):8d}  {int(row[1]):8d}  {row[2]:8.2f}  {int(row[3]):8d}")
```

---

### 4. Choisir le nombre de clusters optimal

**Code avec m√©thode du coude sur la distance** :
```python
# Donn√©es
X, _ = make_blobs(n_samples=300, centers=5, random_state=42)

# Calculer linkage
Z = linkage(X, method='ward')

# Distances de fusion (derni√®res K fusions)
last_merges = Z[-20:, 2]  # 20 derni√®res distances

# M√©thode du coude
plt.figure(figsize=(10, 5))
plt.plot(range(1, len(last_merges)+1), last_merges[::-1], 'bo-')
plt.xlabel('Nombre de clusters')
plt.ylabel('Distance de fusion')
plt.title('M√©thode du coude - Distances de fusion')
plt.grid(True)
plt.show()

# Grand saut = nombre optimal de clusters
acceleration = np.diff(last_merges, 2)
optimal_clusters = len(last_merges) - np.argmax(acceleration[::-1])
print(f"Nombre optimal de clusters sugg√©r√©: {optimal_clusters}")
```

---

### 5. Clustering avec contrainte de connectivit√©

**Code pour forcer des contraintes de voisinage** :
```python
from sklearn.neighbors import kneighbors_graph

# Donn√©es
X, _ = make_blobs(n_samples=100, centers=3, random_state=42)

# Sans contrainte
model_no_constraint = AgglomerativeClustering(n_clusters=3, linkage='ward')
y_no_constraint = model_no_constraint.fit_predict(X)

# Avec contrainte de connectivit√© (seulement voisins peuvent fusionner)
connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)
model_constraint = AgglomerativeClustering(
    n_clusters=3, 
    linkage='ward',
    connectivity=connectivity
)
y_constraint = model_constraint.fit_predict(X)

# Visualisation
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

ax = axes[0]
ax.scatter(X[:, 0], X[:, 1], c=y_no_constraint, cmap='viridis', s=50)
ax.set_title('Sans contrainte')

ax = axes[1]
ax.scatter(X[:, 0], X[:, 1], c=y_constraint, cmap='viridis', s=50)
ax.set_title('Avec contrainte de connectivit√©')

plt.tight_layout()
plt.show()
```

---

### 6. Application : Clustering d'images

**Code pour regrouper des pixels par couleur** :
```python
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt

# Cr√©er une image simple
image = np.zeros((100, 100, 3))
image[:50, :50] = [1, 0, 0]  # Rouge
image[:50, 50:] = [0, 1, 0]  # Vert
image[50:, :50] = [0, 0, 1]  # Bleu
image[50:, 50:] = [1, 1, 0]  # Jaune
# Ajouter du bruit
image += np.random.randn(100, 100, 3) * 0.1

# Reshape en (n_pixels, 3)
pixels = image.reshape(-1, 3)

# Clustering hi√©rarchique
model = AgglomerativeClustering(n_clusters=4, linkage='ward')
labels = model.fit_predict(pixels)

# Reconstruire image segment√©e
segmented = labels.reshape(100, 100)

# Visualisation
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

axes[0].imshow(image)
axes[0].set_title('Image originale')
axes[0].axis('off')

axes[1].imshow(segmented, cmap='viridis')
axes[1].set_title('Segmentation (4 clusters)')
axes[1].axis('off')

plt.tight_layout()
plt.show()
```

## Quand l'utiliser

- ‚úÖ **Nombre de clusters inconnu** : Dendrogramme permet de choisir K apr√®s
- ‚úÖ **Hi√©rarchie utile** : Structure multi-niveaux informative
- ‚úÖ **Petits/moyens datasets** : < 10,000 points (O(n¬≥) en temps)
- ‚úÖ **Formes non-convexes** : Single linkage g√®re bien
- ‚úÖ **Interpr√©tabilit√©** : Dendrogramme facile √† expliquer
- ‚úÖ **D√©terministe** : Pas d'initialisation al√©atoire

**Cas d'usage typiques** :
- üß¨ **Biologie** : Phylog√©nie, taxonomie, clustering de g√®nes
- üìä **Analyse de donn√©es** : Exploration de structure
- üóÇÔ∏è **Organisation** : Hi√©rarchie de documents, cat√©gories
- üè• **M√©decine** : Classification de maladies
- üõí **Marketing** : Segmentation client avec sous-segments

**Quand NE PAS utiliser** :
- ‚ùå Tr√®s grandes donn√©es (>10k) ‚Üí K-Means ou Mini-Batch
- ‚ùå Besoin de rapidit√© ‚Üí K-Means beaucoup plus rapide
- ‚ùå Clusters de densit√© variable ‚Üí DBSCAN
- ‚ùå M√©moire limit√©e ‚Üí Complexit√© O(n¬≤) en espace

## Forces

‚úÖ **Pas de K √† sp√©cifier** : Dendrogramme montre toutes les options  
‚úÖ **Hi√©rarchie informative** : Structure multi-niveaux  
‚úÖ **D√©terministe** : Toujours m√™me r√©sultat (pas d'al√©atoire)  
‚úÖ **Plusieurs linkages** : Ward, single, complete, average  
‚úÖ **Visualisation claire** : Dendrogramme facile √† interpr√©ter  
‚úÖ **Formes flexibles** : Single linkage g√®re formes allong√©es

**Exemple d'exploration de K** :
```python
# Avec K-Means: besoin de tester plusieurs K
for k in [2, 3, 4, 5]:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(X)
    # Comparer...

# Avec Hierarchical: un seul dendrogramme
Z = linkage(X, method='ward')
dendrogram(Z)
# Choisir K visuellement en coupant √† diff√©rentes hauteurs
```

## Limites

‚ùå **Complexit√© O(n¬≥)** : Tr√®s lent pour grandes donn√©es  
‚ùå **Complexit√© O(n¬≤) m√©moire** : Matrice de distances compl√®te  
‚ùå **Pas de r√©affectation** : D√©cisions irr√©versibles (greedy)  
‚ùå **Sensible au bruit** : Outliers affectent la structure  
‚ùå **Choix du linkage** : R√©sultats tr√®s diff√©rents selon crit√®re  
‚ùå **Pas de probabilit√©s** : Hard assignment uniquement

**Temps d'ex√©cution** :
```python
import time

for n in [100, 500, 1000, 2000]:
    X = np.random.randn(n, 10)
    
    start = time.time()
    model = AgglomerativeClustering(n_clusters=5)
    model.fit(X)
    elapsed = time.time() - start
    
    print(f"n={n:5d}: {elapsed:.2f}s")

# Output typique:
# n=  100: 0.05s
# n=  500: 0.5s
# n= 1000: 3.5s
# n= 2000: 25s  (croissance cubique!)
```

**Probl√®me de d√©cisions irr√©versibles** :
```python
# Si 2 points fusionnent trop t√¥t (erreur)
# ‚Üí Impossible de les s√©parer plus tard
# ‚Üí Propagation de l'erreur dans tout l'arbre

# K-Means peut r√©assigner √† chaque it√©ration
```

## Variantes / liens

### Hyperparam√®tres cl√©s

```python
AgglomerativeClustering(
    n_clusters=2,           # Nombre de clusters (ou None)
    linkage='ward',         # 'ward', 'complete', 'average', 'single'
    distance_threshold=None,# Couper √† cette distance (si n_clusters=None)
    connectivity=None,      # Matrice de connectivit√© (contraintes)
    compute_full_tree=False # Si True, construit arbre complet
)
```

**Recommandations** :
- **linkage** : 
  - `'ward'` : Meilleur en g√©n√©ral (minimise variance)
  - `'single'` : Pour formes allong√©es, non-convexes
  - `'complete'` : Pour clusters compacts, sph√©riques
  - `'average'` : Compromis entre single et complete
- **n_clusters vs distance_threshold** : 
  - Sp√©cifier n_clusters OU distance_threshold (pas les deux)
- **connectivity** : Utiliser si contraintes spatiales

### Linkage criteria d√©taill√©s

**1. Single (nearest neighbor)** :
```python
d(A, B) = min{d(a, b) : a ‚àà A, b ‚àà B}
```
- Force : D√©tecte formes allong√©es
- Faible : Sensible au "cha√Ænage" (chaining effect)

**2. Complete (farthest neighbor)** :
```python
d(A, B) = max{d(a, b) : a ‚àà A, b ‚àà B}
```
- Force : Clusters compacts
- Faible : Sensible aux outliers

**3. Average (UPGMA)** :
```python
d(A, B) = (1/|A||B|) Œ£ Œ£ d(a, b)
                     a‚ààA b‚ààB
```
- Force : Compromis √©quilibr√©
- Faible : Pas d'optimisation claire

**4. Ward** :
```python
d(A, B) = Œî(variance intra-cluster apr√®s fusion)
```
- Force : Minimise variance (crit√®re optimal)
- Faible : Assume clusters gaussiens

**Visualisation des diff√©rences** :
```python
# Donn√©es avec forme allong√©e
from sklearn.datasets import make_moons
X, _ = make_moons(n_samples=100, noise=0.05)

linkages = ['single', 'complete', 'ward']
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

for ax, linkage in zip(axes, linkages):
    model = AgglomerativeClustering(n_clusters=2, linkage=linkage)
    y = model.fit_predict(X)
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')
    ax.set_title(f'Linkage: {linkage}')

plt.show()
```

### Relations avec d'autres mod√®les

- **K-Means** : Clustering plat (pas de hi√©rarchie), plus rapide
- **DBSCAN** : Density-based, g√®re bruit et formes complexes
- **GMM** : Clustering probabiliste (soft assignments)
- **BIRCH** : Hierarchical scalable (pour grandes donn√©es)
- **OPTICS** : Density-based hi√©rarchique

### Scipy vs Scikit-learn

**Scipy (pour dendrogrammes)** :
```python
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

# Calcul linkage
Z = linkage(X, method='ward')

# Dendrogramme
dendrogram(Z)

# Couper √† une hauteur
clusters = fcluster(Z, t=10, criterion='distance')
```

**Scikit-learn (pour pr√©dictions)** :
```python
from sklearn.cluster import AgglomerativeClustering

# Clustering
model = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels = model.fit_predict(X)
```

### Variantes avanc√©es

**1. BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)** :
```python
from sklearn.cluster import Birch

# Scalable pour grandes donn√©es
birch = Birch(n_clusters=3, threshold=0.5, branching_factor=50)
labels = birch.fit_predict(X)

# Beaucoup plus rapide que Agglomerative sur grandes donn√©es
```

**2. Dendrogramme circulaire** :
```python
from scipy.cluster.hierarchy import dendrogram

# Dendrogramme circulaire (pour beaut√©)
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='polar')
dendrogram(Z, ax=ax, orientation='left')
plt.show()
```

## R√©f√©rences

### Documentation et tutoriels
- **Scikit-learn** : [Hierarchical Clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)
- **Scipy** : [Hierarchical Clustering](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html)
- **StatQuest** : [Hierarchical Clustering](https://www.youtube.com/watch?v=7xHsRkOdVwo) (YouTube)

### Livres
- **"The Elements of Statistical Learning"** (Hastie et al., 2009) - Chapitre 14.3
- **"Pattern Recognition and Machine Learning"** (Bishop, 2006) - Chapitre 9
- **"An Introduction to Statistical Learning"** (James et al., 2021) - Chapitre 12

### Papers fondamentaux
- **Single Linkage** : Florek et al., 1951
- **Complete Linkage** : S√∏rensen, 1948
- **Ward's Method** : Ward, 1963 - "Hierarchical Grouping to Optimize an Objective Function"
- **BIRCH** : Zhang et al., 1996 - "BIRCH: An Efficient Data Clustering Method for Very Large Databases"

### Th√©orie

**Complexit√©** :
```
Temps: O(n¬≥) pour agglomeratif standard
       O(n¬≤ log n) avec optimisations
Espace: O(n¬≤) pour matrice de distances compl√®te
```

**Matrice de linkage (Scipy)** :
```
Z[i] = [cluster1, cluster2, distance, n_points]

cluster1, cluster2: indices des clusters fusionn√©s
distance: distance de Ward (ou autre)
n_points: nombre total de points dans nouveau cluster
```

**Comparaison de performance** :
```
Dataset: 1,000 points, 2D

Algorithme           Temps    M√©moire
K-Means              0.01s    1 MB
Hierarchical (Ward)  0.8s     8 MB
DBSCAN               0.15s    2 MB
GMM                  0.3s     3 MB

‚Üí Hierarchical: Lent mais informatif
```

### Tuning rapide (r√®gles empiriques)

**Workflow recommand√©** :
```python
# 1. Calculer linkage et visualiser dendrogramme
from scipy.cluster.hierarchy import linkage, dendrogram

Z = linkage(X, method='ward')
plt.figure(figsize=(12, 6))
dendrogram(Z)
plt.axhline(y=..., color='r')  # Ligne de coupure
plt.show()

# 2. Choisir K visuellement en regardant les grands sauts

# 3. Clustering avec K choisi
from sklearn.cluster import AgglomerativeClustering

model = AgglomerativeClustering(n_clusters=K, linkage='ward')
labels = model.fit_predict(X)
```

**Choix du linkage** :
```python
# Formes allong√©es, non-convexes ‚Üí 'single'
# Clusters compacts, sph√©riques ‚Üí 'complete' ou 'ward'
# Compromis g√©n√©ral ‚Üí 'average'
# Minimiser variance (meilleur souvent) ‚Üí 'ward'

# Tester visuellement
for linkage in ['single', 'complete', 'average', 'ward']:
    model = AgglomerativeClustering(n_clusters=3, linkage=linkage)
    model.fit(X)
    # Visualiser et comparer
```

**Optimisation pour grandes donn√©es** :
```python
# Si n > 10,000 ‚Üí Utiliser BIRCH au lieu de Agglomerative
from sklearn.cluster import Birch

birch = Birch(n_clusters=5)
labels = birch.fit_predict(X_large)

# Ou sous-√©chantillonner
X_sample = X[np.random.choice(len(X), 5000, replace=False)]
model = AgglomerativeClustering(n_clusters=5)
model.fit(X_sample)
```
