# K-means

Clustering par centroÃ¯des, rapide, nÃ©cessite K.

## IdÃ©e clÃ©

**K-Means** est un algorithme de **clustering par partitionnement** qui divise les donnÃ©es en K groupes (clusters) en assignant chaque point au centroÃ¯de le plus proche. C'est l'algorithme de clustering le plus populaire grÃ¢ce Ã  sa simplicitÃ© et sa rapiditÃ©.

**Principe** :
1. Initialiser K centroÃ¯des alÃ©atoirement
2. **Assignment** : Assigner chaque point au centroÃ¯de le plus proche
3. **Update** : Recalculer les centroÃ¯des comme la moyenne des points assignÃ©s
4. RÃ©pÃ©ter 2-3 jusqu'Ã  convergence (centroÃ¯des ne bougent plus)

**Algorithme de Lloyd** :
```
Input: X (donnÃ©es), K (nombre de clusters)

1. Initialiser K centroÃ¯des Î¼â‚, ..., Î¼â‚– alÃ©atoirement

2. RÃ©pÃ©ter jusqu'Ã  convergence:
   
   a. Assignment step:
      Pour chaque point xáµ¢:
        cáµ¢ = argmin ||xáµ¢ - Î¼â‚–||Â²
             k
   
   b. Update step:
      Pour chaque cluster k:
        Î¼â‚– = moyenne{xáµ¢ : cáµ¢ = k}

3. Retourner centroÃ¯des Î¼ et labels c
```

**Fonction objectif (inertie)** :
```
J = Î£ Î£ ||xáµ¢ - Î¼â‚–||Â²
    k xáµ¢âˆˆCâ‚–

Minimiser la somme des distances au carrÃ©
â†’ Lloyd garantit convergence vers optimum local
```

**Visualisation** :
```
Iteration 0:        Iteration 1:        Iteration 5:
   â€¢  â€¢  â€¢             â€¢â”€â”€â€¢â”€â”€â€¢             â€¢  â€¢  â€¢
  â€¢  Ã—  â€¢            â€¢   Ã—   â€¢           â€¢ Ã— Ã— â€¢ 
   â€¢  â€¢  â€¢             â€¢â”€â”€â€¢â”€â”€â€¢             â€¢  â€¢  â€¢
      Ã—                   Ã—                   Ã—

Ã— = centroÃ¯des      Assignment            ConvergÃ©!
â€¢ = points          + Update
```

**DiffÃ©rence avec autres mÃ©thodes** :
| Aspect | K-Means | Hierarchical | GMM | DBSCAN |
|--------|---------|--------------|-----|--------|
| **Besoin K?** | Oui | Non (dendrogramme) | Oui | Non |
| **Forme clusters** | SphÃ©riques | Flexible | EllipsoÃ¯dales | Arbitraire |
| **Vitesse** | TrÃ¨s rapide | Lent O(nÂ³) | Moyen | Moyen |
| **Soft/Hard** | Hard | Hard | Soft | Hard |
| **GÃ¨re bruit** | Non | Non | Peu | Oui |

## Exemples concrets

### 1. K-Means de base : Clustering 2D

**ScÃ©nario** : Grouper des donnÃ©es 2D en 3 clusters.

**Code Python avec K-Means** :
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# 1. GÃ©nÃ©rer donnÃ©es avec 3 clusters
X, y_true = make_blobs(
    n_samples=300, 
    centers=3,
    cluster_std=0.6,
    random_state=42
)

# 2. K-Means
kmeans = KMeans(
    n_clusters=3,
    init='k-means++',  # Initialisation intelligente (dÃ©faut)
    n_init=10,         # Nombre d'initialisations diffÃ©rentes
    max_iter=300,      # Nombre max d'itÃ©rations
    random_state=42
)
kmeans.fit(X)

# 3. PrÃ©dictions
y_pred = kmeans.predict(X)
centroids = kmeans.cluster_centers_

# 4. MÃ©triques
inertia = kmeans.inertia_  # Somme des distances au carrÃ©
print(f"Inertie (within-cluster sum of squares): {inertia:.2f}")
print(f"Nombre d'itÃ©rations: {kmeans.n_iter_}")
print(f"CentroÃ¯des:\n{centroids}")

# 5. Visualisation
plt.figure(figsize=(12, 5))

# DonnÃ©es originales
plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.6)
plt.title('DonnÃ©es originales (3 clusters)')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

# K-Means rÃ©sultat
plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.6)
plt.scatter(
    centroids[:, 0], 
    centroids[:, 1], 
    c='red', 
    s=300, 
    marker='X',
    edgecolors='black',
    linewidth=2,
    label='CentroÃ¯des'
)
plt.title(f'K-Means (K=3, inertie={inertia:.0f})')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

plt.tight_layout()
plt.show()
```

---

### 2. MÃ©thode du coude (Elbow Method) : Choisir K optimal

**Code pour dÃ©terminer le nombre de clusters** :
```python
# DonnÃ©es
X, _ = make_blobs(n_samples=300, centers=4, random_state=42)

# Tester diffÃ©rents K
K_range = range(1, 11)
inertias = []
silhouette_scores = []

from sklearn.metrics import silhouette_score

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)
    
    if k > 1:  # Silhouette nÃ©cessite au moins 2 clusters
        score = silhouette_score(X, kmeans.labels_)
        silhouette_scores.append(score)
    else:
        silhouette_scores.append(0)

# Visualisation
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# MÃ©thode du coude (inertie)
ax = axes[0]
ax.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
ax.set_xlabel('Nombre de clusters (K)')
ax.set_ylabel('Inertie (within-cluster SS)')
ax.set_title('MÃ©thode du coude')
ax.grid(True, alpha=0.3)
ax.axvline(x=4, color='r', linestyle='--', label='K optimal suggÃ©rÃ©')
ax.legend()

# Silhouette score
ax = axes[1]
ax.plot(K_range, silhouette_scores, 'gs-', linewidth=2, markersize=8)
ax.set_xlabel('Nombre de clusters (K)')
ax.set_ylabel('Silhouette Score')
ax.set_title('Silhouette Score (plus haut = meilleur)')
ax.grid(True, alpha=0.3)
ax.axvline(x=4, color='r', linestyle='--', label='K optimal suggÃ©rÃ©')
ax.legend()

plt.tight_layout()
plt.show()

print(f"Inertie pour K=4: {inertias[3]:.2f}")
print(f"Silhouette score pour K=4: {silhouette_scores[3]:.3f}")
```

---

### 3. Visualiser les itÃ©rations de K-Means

**Code pour montrer la convergence** :
```python
# GÃ©nÃ©rer donnÃ©es simples
np.random.seed(42)
X_simple = np.vstack([
    np.random.randn(30, 2) * 0.5 + [0, 0],
    np.random.randn(30, 2) * 0.5 + [3, 3],
    np.random.randn(30, 2) * 0.5 + [0, 3]
])

# K-Means avec max_iter=1 pour voir chaque Ã©tape
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

for i, max_iter in enumerate([0, 1, 2, 3, 5, 10]):
    kmeans = KMeans(n_clusters=3, init='random', max_iter=max_iter, n_init=1, random_state=42)
    
    if max_iter == 0:
        # Initialisation seulement
        kmeans = KMeans(n_clusters=3, init='random', max_iter=0, n_init=1, random_state=42)
        kmeans.fit(X_simple)
        centroids = kmeans.cluster_centers_
        labels = np.zeros(len(X_simple))
    else:
        kmeans.fit(X_simple)
        centroids = kmeans.cluster_centers_
        labels = kmeans.labels_
    
    ax = axes[i]
    ax.scatter(X_simple[:, 0], X_simple[:, 1], c=labels, cmap='viridis', s=50, alpha=0.6)
    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=300, marker='X', 
               edgecolors='black', linewidth=2)
    ax.set_title(f'ItÃ©ration {max_iter}' if max_iter > 0 else 'Initialisation')
    ax.set_xlim(-2, 5)
    ax.set_ylim(-2, 5)

plt.tight_layout()
plt.show()
```

---

### 4. K-Means++ vs initialisation alÃ©atoire

**Code pour comparer les initialisations** :
```python
X, _ = make_blobs(n_samples=300, centers=3, random_state=42)

# Initialisation alÃ©atoire (mauvaise)
kmeans_random = KMeans(n_clusters=3, init='random', n_init=1, random_state=0)
kmeans_random.fit(X)
inertia_random = kmeans_random.inertia_

# K-Means++ (intelligente)
kmeans_pp = KMeans(n_clusters=3, init='k-means++', n_init=1, random_state=0)
kmeans_pp.fit(X)
inertia_pp = kmeans_pp.inertia_

# Visualisation
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Random init
ax = axes[0]
ax.scatter(X[:, 0], X[:, 1], c=kmeans_random.labels_, cmap='viridis', s=50, alpha=0.6)
ax.scatter(kmeans_random.cluster_centers_[:, 0], kmeans_random.cluster_centers_[:, 1],
           c='red', s=300, marker='X', edgecolors='black', linewidth=2)
ax.set_title(f'Init Random\nInertie: {inertia_random:.0f}')

# K-Means++ init
ax = axes[1]
ax.scatter(X[:, 0], X[:, 1], c=kmeans_pp.labels_, cmap='viridis', s=50, alpha=0.6)
ax.scatter(kmeans_pp.cluster_centers_[:, 0], kmeans_pp.cluster_centers_[:, 1],
           c='red', s=300, marker='X', edgecolors='black', linewidth=2)
ax.set_title(f'Init K-Means++\nInertie: {inertia_pp:.0f}')

plt.tight_layout()
plt.show()

print(f"AmÃ©lioration avec K-Means++: {(inertia_random - inertia_pp) / inertia_random * 100:.1f}%")
```

---

### 5. Mini-Batch K-Means pour grandes donnÃ©es

**Code pour comparer vitesse** :
```python
from sklearn.cluster import MiniBatchKMeans
import time

# Grandes donnÃ©es
X_large = np.random.randn(100000, 50)

# K-Means standard
start = time.time()
kmeans = KMeans(n_clusters=10, n_init=3)
kmeans.fit(X_large)
time_kmeans = time.time() - start

# Mini-Batch K-Means
start = time.time()
mbkmeans = MiniBatchKMeans(n_clusters=10, batch_size=1000, n_init=3)
mbkmeans.fit(X_large)
time_mbkmeans = time.time() - start

print(f"K-Means:            {time_kmeans:.2f}s, inertie: {kmeans.inertia_:.0f}")
print(f"Mini-Batch K-Means: {time_mbkmeans:.2f}s, inertie: {mbkmeans.inertia_:.0f}")
print(f"Speedup: {time_kmeans / time_mbkmeans:.1f}x")
```

---

### 6. Application : Compression d'image

**Code pour rÃ©duire le nombre de couleurs** :
```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# CrÃ©er une image simple (ou charger une vraie image)
image = np.zeros((200, 200, 3))
image[:100, :100] = [1, 0, 0]  # Rouge
image[:100, 100:] = [0, 1, 0]  # Vert
image[100:, :100] = [0, 0, 1]  # Bleu
image[100:, 100:] = [1, 1, 0]  # Jaune
# Ajouter variations
image += np.random.randn(200, 200, 3) * 0.1

# Reshape pour K-Means
pixels = image.reshape(-1, 3)

# Compression avec diffÃ©rents K
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

k_values = [2, 4, 8, 16, 32, 64]

for idx, k in enumerate(k_values):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=3)
    labels = kmeans.fit_predict(pixels)
    compressed_pixels = kmeans.cluster_centers_[labels]
    compressed_image = compressed_pixels.reshape(200, 200, 3)
    
    ax = axes[idx]
    ax.imshow(np.clip(compressed_image, 0, 1))
    ax.set_title(f'K={k} couleurs')
    ax.axis('off')

plt.tight_layout()
plt.show()

# Calcul taux de compression
original_size = 200 * 200 * 3
compressed_size = k_values[-1] * 3 + 200 * 200  # K centroÃ¯des + labels
compression_ratio = original_size / compressed_size
print(f"Taux de compression (K={k_values[-1]}): {compression_ratio:.1f}x")
```

## Quand l'utiliser

- âœ… **Grandes donnÃ©es** : O(nÂ·KÂ·dÂ·i) trÃ¨s rapide (linÃ©aire en n)
- âœ… **Clusters sphÃ©riques** : Assume tailles et densitÃ©s similaires
- âœ… **K connu** : Nombre de clusters estimÃ© Ã  l'avance
- âœ… **SimplicitÃ©** : Facile Ã  implÃ©menter et interprÃ©ter
- âœ… **ScalabilitÃ©** : Mini-Batch K-Means pour millions de points
- âœ… **PremiÃ¨re approche** : Baseline rapide pour clustering

**Cas d'usage typiques** :
- ğŸ¨ **Compression d'images** : RÃ©duction de couleurs
- ğŸ“Š **Segmentation client** : Groupes de comportements
- ğŸ·ï¸ **PrÃ©traitement** : Features pour autre modÃ¨le
- ğŸ—ºï¸ **GÃ©olocalisation** : Zones gÃ©ographiques
- ğŸ“ **Text mining** : Clustering de documents (avec TF-IDF)

**Quand NE PAS utiliser** :
- âŒ Clusters non-sphÃ©riques (formes allongÃ©es) â†’ Hierarchical, DBSCAN
- âŒ DensitÃ©s variables â†’ DBSCAN, OPTICS
- âŒ PrÃ©sence de bruit/outliers â†’ DBSCAN robuste
- âŒ K inconnu â†’ Hierarchical (dendrogramme)
- âŒ Besoin soft assignments â†’ GMM (probabilitÃ©s)

## Forces

âœ… **TrÃ¨s rapide** : O(nÂ·KÂ·dÂ·i) linÃ©aire en n  
âœ… **Scalable** : Mini-Batch pour millions de points  
âœ… **Simple** : Facile Ã  comprendre et implÃ©menter  
âœ… **DÃ©terministe** : Avec mÃªme initialisation â†’ mÃªme rÃ©sultat  
âœ… **Convergence garantie** : Vers optimum local  
âœ… **Memory efficient** : Stocke seulement K centroÃ¯des

**Exemple de vitesse** :
```python
import time

# 1 million de points
X_huge = np.random.randn(1000000, 10)

start = time.time()
kmeans = KMeans(n_clusters=10, n_init=1)
kmeans.fit(X_huge)
print(f"K-Means sur 1M points: {time.time() - start:.2f}s")

# vs Hierarchical (impossible)
# AgglomerativeClustering prendrait des heures!
```

## Limites

âŒ **NÃ©cessite K** : Nombre de clusters Ã  spÃ©cifier Ã  l'avance  
âŒ **Optimum local** : Sensible Ã  l'initialisation  
âŒ **Assume sphÃ¨res** : Clusters de tailles/densitÃ©s similaires  
âŒ **Sensible outliers** : Points aberrants affectent centroÃ¯des  
âŒ **Hard clustering** : Pas de probabilitÃ©s d'appartenance  
âŒ **MÃ©trique euclidienne** : Distance L2 seulement (standard)

**ProblÃ¨me de forme** :
```python
from sklearn.datasets import make_moons

# DonnÃ©es en forme de lunes (non-sphÃ©riques)
X, _ = make_moons(n_samples=200, noise=0.05, random_state=42)

# K-Means Ã©choue
kmeans = KMeans(n_clusters=2)
y_kmeans = kmeans.fit_predict(X)

# DBSCAN rÃ©ussit
from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=5)
y_dbscan = dbscan.fit_predict(X)

# Visualisation
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis')
axes[0].set_title('K-Means (Ã©chec sur formes non-sphÃ©riques)')

axes[1].scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='viridis')
axes[1].set_title('DBSCAN (succÃ¨s)')

plt.show()
```

**SensibilitÃ© aux outliers** :
```python
# DonnÃ©es avec outliers
X_clean = np.random.randn(100, 2)
X_outliers = np.random.randn(5, 2) * 5 + 10  # Outliers loin
X_with_outliers = np.vstack([X_clean, X_outliers])

# K-Means tire les centroÃ¯des vers les outliers
kmeans = KMeans(n_clusters=1)
kmeans.fit(X_with_outliers)
centroid = kmeans.cluster_centers_[0]

print(f"CentroÃ¯de (avec outliers): {centroid}")
print(f"Moyenne vraies donnÃ©es: {X_clean.mean(axis=0)}")
# â†’ CentroÃ¯de biaisÃ© par outliers
```

## Variantes / liens

### HyperparamÃ¨tres clÃ©s

```python
KMeans(
    n_clusters=8,           # Nombre de clusters K
    init='k-means++',       # 'k-means++', 'random', ou array de centroÃ¯des
    n_init=10,              # Nombre d'initialisations diffÃ©rentes (garde meilleur)
    max_iter=300,           # Nombre max d'itÃ©rations par run
    tol=1e-4,               # Seuil de convergence
    random_state=None,      # Seed pour reproductibilitÃ©
    algorithm='lloyd'       # 'lloyd', 'elkan' (plus rapide pour certains cas)
)
```

**Recommandations** :
- **n_clusters** : Utiliser elbow method ou silhouette score
- **init** : Toujours `'k-means++'` (meilleur que random)
- **n_init** : Au moins 10 pour robustesse (essayer 10 fois, garder meilleur)
- **algorithm** : `'elkan'` si K petit et d Ã©levÃ©

### Algorithmes d'initialisation

**1. Random** :
```python
# Choisir K points alÃ©atoirement
centroids = X[np.random.choice(len(X), K, replace=False)]
```
- Simple mais peut donner mauvais rÃ©sultats

**2. K-Means++ (Arthur & Vassilvitskii, 2007)** :
```python
# 1. Choisir premier centroÃ¯de alÃ©atoirement
# 2. Pour chaque nouveau centroÃ¯de:
#    - Calculer distance min de chaque point aux centroÃ¯des existants
#    - Choisir nouveau centroÃ¯de avec probabilitÃ© âˆ distanceÂ²
```
- Meilleur: Spread out initial centroids
- DÃ©faut dans scikit-learn

**3. Manual** :
```python
# SpÃ©cifier manuellement
initial_centroids = np.array([[0, 0], [5, 5], [10, 0]])
kmeans = KMeans(n_clusters=3, init=initial_centroids, n_init=1)
```

### MÃ©triques de qualitÃ©

**1. Inertie (within-cluster sum of squares)** :
```python
inertia = kmeans.inertia_
# Plus bas = meilleur (mais dÃ©croÃ®t toujours avec K)
```

**2. Silhouette Score** :
```python
from sklearn.metrics import silhouette_score

score = silhouette_score(X, labels)
# Entre -1 et 1, plus haut = meilleur
# > 0.5 : clusters bien sÃ©parÃ©s
# < 0.2 : clusters se chevauchent
```

**3. Davies-Bouldin Index** :
```python
from sklearn.metrics import davies_bouldin_score

dbi = davies_bouldin_score(X, labels)
# Plus bas = meilleur
```

**4. Calinski-Harabasz Index** :
```python
from sklearn.metrics import calinski_harabasz_score

chi = calinski_harabasz_score(X, labels)
# Plus haut = meilleur
```

### Variantes de K-Means

**1. Mini-Batch K-Means** :
```python
from sklearn.cluster import MiniBatchKMeans

# 10-100x plus rapide sur grandes donnÃ©es
mbkmeans = MiniBatchKMeans(
    n_clusters=10,
    batch_size=1000,  # Nombre de points par batch
    n_init=3
)
mbkmeans.fit(X_large)
```

**2. K-Medoids (PAM)** :
```python
from sklearn_extra.cluster import KMedoids

# Utilise points rÃ©els comme centroÃ¯des (plus robuste aux outliers)
kmedoids = KMedoids(n_clusters=3, method='pam')
kmedoids.fit(X)
```

**3. Fuzzy C-Means** :
```python
# Soft clustering (chaque point a probabilitÃ© pour chaque cluster)
# Similaire Ã  GMM mais avec contrainte sur somme = 1
```

**4. K-Means avec contraintes** :
```python
# Contraintes: must-link, cannot-link
# GÃ©nÃ©ralement implÃ©mentation custom requise
```

### Relations avec d'autres modÃ¨les

- **GMM** : K-Means = cas spÃ©cial de GMM (covariances sphÃ©riques identiques)
- **Vector Quantization** : K-Means utilisÃ© pour compression
- **K-NN** : Utilise distances comme K-Means mais pour classification
- **Hierarchical** : Peut initialiser avec K-Means pour accÃ©lÃ©rer
- **DBSCAN** : Density-based, ne nÃ©cessite pas K

### Preprocessing recommandÃ©

**Normalisation importante** :
```python
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Pipeline: normalisation + K-Means
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Important pour K-Means!
    ('kmeans', KMeans(n_clusters=3))
])

pipeline.fit(X)
labels = pipeline.predict(X)
```

**Pourquoi normaliser** :
```python
# Sans normalisation: features avec grandes valeurs dominent
X = np.random.randn(100, 2)
X[:, 0] *= 1000  # Feature 1: 0-1000
X[:, 1] *= 1     # Feature 2: 0-1

# K-Means se base principalement sur feature 1
# â†’ Normaliser pour que chaque feature contribue Ã©quitablement
```

## RÃ©fÃ©rences

### Documentation et tutoriels
- **Scikit-learn** : [K-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means)
- **StatQuest** : [K-Means Clustering](https://www.youtube.com/watch?v=4b5d3muPQmA) (YouTube)
- **Andrew Ng** : [K-Means Algorithm](https://www.coursera.org/learn/machine-learning) (Coursera)

### Livres
- **"Pattern Recognition and Machine Learning"** (Bishop, 2006) - Chapitre 9
- **"The Elements of Statistical Learning"** (Hastie et al., 2009) - Chapitre 14.3
- **"Introduction to Data Mining"** (Tan et al., 2018) - Chapitre 8

### Papers fondamentaux
- **K-Means** : Lloyd, 1982 (publiÃ©) / MacQueen, 1967 - "Least Squares Quantization in PCM"
- **K-Means++** : Arthur & Vassilvitskii, 2007 - "k-means++: The Advantages of Careful Seeding"
- **Mini-Batch K-Means** : Sculley, 2010 - "Web-Scale K-Means Clustering"

### ThÃ©orie

**ComplexitÃ©** :
```
Temps: O(n Â· K Â· d Â· i)
  n = nombre de points
  K = nombre de clusters
  d = dimension
  i = nombre d'itÃ©rations (souvent < 20)

Espace: O(n Â· d + K Â· d)

â†’ LinÃ©aire en n (trÃ¨s scalable)
```

**Convergence** :
```
Lloyd garantit:
- Inertie diminue Ã  chaque itÃ©ration
- Convergence vers optimum local (pas global!)
- GÃ©nÃ©ralement < 20 itÃ©rations
```

**Benchmark de performance** :
```
Dataset: 100,000 points, 10 features, K=5

Algorithme           Temps    Inertie
K-Means (lloyd)      0.8s     12,450
K-Means (elkan)      0.5s     12,450
Mini-Batch (b=1000)  0.1s     12,680
GMM (full cov)       15s      N/A
Hierarchical         âˆ        N/A (trop lent)

â†’ K-Means: Imbattable en vitesse
```

### Tuning rapide (rÃ¨gles empiriques)

**Workflow recommandÃ©** :
```python
# 1. Normaliser
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. Elbow method pour choisir K
inertias = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

plt.plot(range(1, 11), inertias, 'bo-')
plt.xlabel('K')
plt.ylabel('Inertie')
plt.title('Elbow Method')
plt.show()

# 3. EntraÃ®ner modÃ¨le final
optimal_k = 4  # Choisir visuellement
kmeans_final = KMeans(n_clusters=optimal_k, n_init=10, random_state=42)
labels = kmeans_final.fit_predict(X_scaled)
```

**Choix de K** :
```python
# MÃ©thode 1: Elbow method (chercher "coude")
# MÃ©thode 2: Silhouette score (maximiser)
# MÃ©thode 3: Domain knowledge (nombre attendu)
# MÃ©thode 4: Gap statistic (comparer avec null model)
```

**Optimisation pour grandes donnÃ©es** :
```python
# Si n > 100,000 â†’ Mini-Batch K-Means
from sklearn.cluster import MiniBatchKMeans

mbkmeans = MiniBatchKMeans(
    n_clusters=10,
    batch_size=1000,
    n_init=3,
    random_state=42
)
labels = mbkmeans.fit_predict(X_large)

# Si n > 10 millions â†’ Utiliser Spark MLlib ou Dask-ML
```
