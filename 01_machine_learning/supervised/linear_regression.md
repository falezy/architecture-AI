# Linear Regression

R√©gression lin√©aire pour pr√©dire une variable continue.

## Id√©e cl√©

La r√©gression lin√©aire mod√©lise la relation entre une **variable cible** (y) et une ou plusieurs **variables explicatives** (X) par une fonction lin√©aire. L'objectif est de trouver les coefficients qui minimisent l'erreur de pr√©diction.

**Formule g√©n√©rale** :
```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅ¬∑x‚ÇÅ + Œ≤‚ÇÇ¬∑x‚ÇÇ + ... + Œ≤‚Çô¬∑x‚Çô + Œµ
```
- `y` : variable √† pr√©dire (ex: prix d'une maison)
- `x‚ÇÅ, x‚ÇÇ, ..., x‚Çô` : variables explicatives (ex: surface, nombre de chambres)
- `Œ≤‚ÇÄ` : intercept (ordonn√©e √† l'origine)
- `Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çô` : coefficients (pentes)
- `Œµ` : erreur (bruit)

**Objectif** : Minimiser la **Mean Squared Error (MSE)** :
```
MSE = (1/n) Œ£(y·µ¢ - ≈∑·µ¢)¬≤
```
o√π `≈∑·µ¢ = Œ≤‚ÇÄ + Œ≤‚ÇÅ¬∑x‚ÇÅ + ... + Œ≤‚Çô¬∑x‚Çô`

**M√©thode de r√©solution** :
- **√âquation normale** : `Œ≤ = (X·µÄX)‚Åª¬πX·µÄy` (solution analytique)
- **Gradient Descent** : Optimisation it√©rative (pour grandes donn√©es)

### √âquation normale vs Descente de gradient : Quand utiliser quoi ?

| Crit√®re | √âquation normale | Descente de gradient |
|---------|------------------|---------------------|
| **Formule** | `Œ≤ = (X·µÄX)‚Åª¬πX·µÄy` | It√©rations : `Œ∏ = Œ∏ - Œ±¬∑‚àáJ(Œ∏)` |
| **Complexit√©** | O(n¬≥) | O(knd) k=it√©rations |
| **Petites donn√©es** (n < 10,000) | ‚úÖ **Recommand√©** : rapide, solution exacte | Possible mais inutile |
| **Grandes donn√©es** (n > 100,000) | ‚ùå Trop lent | ‚úÖ **Recommand√©** : efficace |
| **Nombreuses features** (d > 10,000) | ‚ùå Impossible (inversion matricielle) | ‚úÖ Fonctionne bien |
| **Hyperparam√®tres** | Aucun | Learning rate Œ±, nb it√©rations |
| **Solution** | Exacte | Approximation |
| **Utilisation** | `LinearRegression()` | `SGDRegressor()` |

#### ‚úÖ Utilisez l'√©quation normale (solution directe)

**Quand** : R√©gression lin√©aire classique avec peu de donn√©es et features

```python
from sklearn.linear_model import LinearRegression

# ‚úÖ Par d√©faut, utilise l'√©quation normale
model = LinearRegression()
model.fit(X, y)  # Solution exacte en une √©tape
```

**Avantages** :
- Pas d'hyperparam√®tres √† tuner
- Solution exacte (pas d'approximation)
- Tr√®s rapide pour petits datasets

#### üîÑ Utilisez la descente de gradient

**Quand** :
1. **Grandes donn√©es** (n > 100,000)
2. **Nombreuses features** (d > 10,000)
3. **Logistic Regression** (pas de solution analytique)
4. **R√©gularisation Lasso** (pas de solution ferm√©e)
5. **Online learning** (donn√©es en flux)

```python
from sklearn.linear_model import SGDRegressor

# Stochastic Gradient Descent
model = SGDRegressor(
    max_iter=1000,
    learning_rate='adaptive',  # Ajuste Œ± automatiquement
    early_stopping=True
)
model.fit(X, y)
```

**Exemple avec descente de gradient manuelle** :
```python
import numpy as np

# Donn√©es
X = np.array([1, 2, 3, 4, 5])
y = 2 * X + 1 + np.random.randn(5) * 0.5

# Initialisation
theta_0, theta_1 = 0, 0  # Coefficients
alpha = 0.01  # Learning rate
n_iterations = 100

for i in range(n_iterations):
    # Pr√©dictions
    y_pred = theta_0 + theta_1 * X
    
    # Gradients (d√©riv√©es partielles de MSE)
    gradient_0 = (2/len(X)) * np.sum(y_pred - y)
    gradient_1 = (2/len(X)) * np.sum((y_pred - y) * X)
    
    # Mise √† jour
    theta_0 -= alpha * gradient_0
    theta_1 -= alpha * gradient_1

print(f"R√©sultat: y = {theta_0:.2f} + {theta_1:.2f}¬∑x")
```

#### üìä Variantes de gradient descent

| Type | Donn√©es/it√©ration | Quand l'utiliser |
|------|------------------|------------------|
| **Batch GD** | Toutes | Petits datasets (stable mais lent) |
| **Stochastic GD (SGD)** | 1 exemple | Tr√®s grandes donn√©es (rapide, bruyant) |
| **Mini-batch GD** | 32-256 exemples | ‚úÖ **Best practice** (compromis) |

#### üéØ En pratique avec scikit-learn

**Scikit-learn choisit automatiquement la meilleure m√©thode** :

```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression

# √âquation normale (solution directe)
LinearRegression()        # ‚úÖ √âquation normale
Ridge(alpha=1.0)         # ‚úÖ √âquation normale (solution analytique existe)

# Gradient descent (obligatoire ou recommand√©)
Lasso(alpha=0.1)         # ‚úÖ Coordinate Descent (variante de GD)
LogisticRegression()     # ‚úÖ L-BFGS (toujours, pas de solution ferm√©e)
SGDRegressor()           # ‚úÖ Stochastic GD explicite (grandes donn√©es)
```

#### üí° R√®gle pratique

```python
# Petites/moyennes donn√©es (n < 10,000) ‚Üí √âquation normale
if n_samples < 10000 and n_features < 1000:
    model = LinearRegression()  # Rapide, exact

# Grandes donn√©es ou nombreuses features ‚Üí Gradient descent  
else:
    model = SGDRegressor()  # Scalable
```

## Exemples concrets

### 1. R√©gression lin√©aire simple : Pr√©dire le prix d'une maison

**Sc√©nario** : Vous avez des donn√©es sur 100 maisons avec leur surface (m¬≤) et leur prix (‚Ç¨). Vous voulez pr√©dire le prix d'une nouvelle maison.

**Donn√©es d'exemple** :
```
Surface (m¬≤)  ‚Üí  Prix (‚Ç¨)
50            ‚Üí  150,000
80            ‚Üí  240,000
120           ‚Üí  360,000
```

**Code Python avec scikit-learn** :
```python
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# 1. Donn√©es d'entra√Ænement
X_train = np.array([50, 80, 100, 120, 150]).reshape(-1, 1)  # Surface (m¬≤)
y_train = np.array([150000, 240000, 300000, 360000, 450000])  # Prix (‚Ç¨)

# 2. Cr√©er et entra√Æner le mod√®le
model = LinearRegression()
model.fit(X_train, y_train)

# 3. Afficher les coefficients
print(f"Intercept (Œ≤‚ÇÄ): {model.intercept_:.2f} ‚Ç¨")
print(f"Coefficient (Œ≤‚ÇÅ): {model.coef_[0]:.2f} ‚Ç¨/m¬≤")
# R√©sultat : y = 0 + 3000¬∑x  (approximativement)

# 4. Pr√©dire le prix d'une maison de 90 m¬≤
nouvelle_surface = np.array([[90]])
prix_predit = model.predict(nouvelle_surface)
print(f"Prix pr√©dit pour 90 m¬≤: {prix_predit[0]:,.0f} ‚Ç¨")
# R√©sultat : ~270,000 ‚Ç¨

# 5. √âvaluer le mod√®le
from sklearn.metrics import r2_score, mean_squared_error
y_pred = model.predict(X_train)
print(f"R¬≤ Score: {r2_score(y_train, y_pred):.3f}")  # Proche de 1 = bon
print(f"RMSE: {np.sqrt(mean_squared_error(y_train, y_pred)):,.0f} ‚Ç¨")
```

**Visualisation** :
```python
plt.scatter(X_train, y_train, color='blue', label='Donn√©es r√©elles')
plt.plot(X_train, model.predict(X_train), color='red', label='R√©gression')
plt.xlabel('Surface (m¬≤)')
plt.ylabel('Prix (‚Ç¨)')
plt.legend()
plt.show()
```

---

### 2. R√©gression lin√©aire multiple : Pr√©dire un salaire

**Sc√©nario** : Pr√©dire le salaire d'un employ√© en fonction de son **exp√©rience** (ann√©es) et son **niveau d'√©ducation** (1=Bachelor, 2=Master, 3=PhD).

**Donn√©es d'exemple** :
```
Exp√©rience | √âducation | Salaire (k‚Ç¨)
2          | 1         | 35
5          | 2         | 50
10         | 3         | 75
```

**Code Python** :
```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# 1. Donn√©es
X = np.array([
    [2, 1],   # 2 ans, Bachelor
    [5, 2],   # 5 ans, Master
    [10, 3],  # 10 ans, PhD
    [3, 1],   # 3 ans, Bachelor
    [7, 2],   # 7 ans, Master
    [15, 3],  # 15 ans, PhD
])
y = np.array([35, 50, 75, 38, 55, 90])  # Salaire en k‚Ç¨

# 2. Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 3. Entra√Æner le mod√®le
model = LinearRegression()
model.fit(X_train, y_train)

# 4. Coefficients
print(f"Intercept: {model.intercept_:.2f} k‚Ç¨")
print(f"Coef Exp√©rience: {model.coef_[0]:.2f} k‚Ç¨/an")
print(f"Coef √âducation: {model.coef_[1]:.2f} k‚Ç¨/niveau")
# R√©sultat : Salaire = 20 + 4¬∑exp√©rience + 8¬∑√©ducation (approx.)

# 5. Pr√©dire pour un employ√© (8 ans, Master)
nouveau_profil = np.array([[8, 2]])
salaire_predit = model.predict(nouveau_profil)
print(f"Salaire pr√©dit: {salaire_predit[0]:.1f} k‚Ç¨")
# R√©sultat : ~56 k‚Ç¨

# 6. √âvaluation sur test set
from sklearn.metrics import r2_score, mean_absolute_error
y_pred_test = model.predict(X_test)
print(f"R¬≤ (test): {r2_score(y_test, y_pred_test):.3f}")
print(f"MAE (test): {mean_absolute_error(y_test, y_pred_test):.2f} k‚Ç¨")
```

---

### 3. Implementation from scratch (NumPy)

**Comprendre les math√©matiques** :
```python
import numpy as np

# Donn√©es simples
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([2, 4, 5, 4, 5])

# Ajouter une colonne de 1 pour l'intercept
X_b = np.c_[np.ones((len(X), 1)), X]  # [1, x]

# √âquation normale: Œ≤ = (X^T X)^-1 X^T y
beta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y
print(f"Œ≤‚ÇÄ (intercept): {beta[0]:.3f}")
print(f"Œ≤‚ÇÅ (pente): {beta[1]:.3f}")

# Pr√©diction
X_new = np.array([[0], [6]])
X_new_b = np.c_[np.ones((len(X_new), 1)), X_new]
y_pred = X_new_b @ beta
print(f"Pr√©dictions: {y_pred}")
```

## Quand l'utiliser

- ‚úÖ **Relation lin√©aire** : La relation entre X et y est approximativement lin√©aire
- ‚úÖ **Interpr√©tabilit√©** : Besoin de comprendre l'impact de chaque variable (coefficients)
- ‚úÖ **Pr√©diction continue** : Variable cible num√©rique (prix, temp√©rature, salaire)
- ‚úÖ **Baseline** : Mod√®le de r√©f√©rence simple avant d'essayer des mod√®les complexes
- ‚úÖ **Peu de donn√©es** : Fonctionne bien avec peu d'exemples (contrairement aux DNN)

**Cas d'usage typiques** :
- üè† **Immobilier** : Prix en fonction de surface, localisation, nombre de chambres
- üí∞ **Finance** : Pr√©dire le chiffre d'affaires en fonction du budget marketing
- üìà **√âconomie** : Relation entre PIB et ch√¥mage
- üå°Ô∏è **Sciences** : Temp√©rature en fonction de l'altitude
- üìä **A/B Testing** : Impact d'une variable sur une m√©trique

## Forces

‚úÖ **Simplicit√©** : Facile √† comprendre et √† impl√©menter  
‚úÖ **Rapide** : Entra√Ænement tr√®s rapide (solution analytique)  
‚úÖ **Interpr√©table** : Les coefficients indiquent l'importance de chaque variable  
‚úÖ **Peu de donn√©es** : Fonctionne avec peu d'exemples d'entra√Ænement  
‚úÖ **Pas d'hyperparam√®tres** : Aucun tuning n√©cessaire (version de base)  
‚úÖ **Inf√©rence instantan√©e** : Pr√©diction = simple multiplication matricielle  
‚úÖ **Robuste au surapprentissage** : Avec r√©gularisation (Ridge, Lasso)

## Limites

‚ùå **Hypoth√®se de lin√©arit√©** : Ne capture pas les relations non-lin√©aires (x¬≤)  
‚ùå **Sensible aux outliers** : Les valeurs extr√™mes biaisent les coefficients  
‚ùå **Multicolin√©arit√©** : Probl√®me si les variables X sont fortement corr√©l√©es  
‚ùå **Homosc√©dasticit√© requise** : Variance de l'erreur doit √™tre constante  
‚ùå **Dimensionnalit√©** : Probl√®me si p >> n (plus de variables que d'exemples)  
‚ùå **Features engineering** : N√©cessite parfois des transformations manuelles (log, polyn√¥mes)  
‚ùå **Pr√©dictions born√©es** : Peut pr√©dire des valeurs impossibles (prix n√©gatif)

**Exemple de limitation** :
```python
# Relation non-lin√©aire : y = x¬≤
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = X.flatten() ** 2  # [1, 4, 9, 16, 25]

# R√©gression lin√©aire √©choue (R¬≤ faible)
model = LinearRegression().fit(X, y)
print(f"R¬≤ Score: {model.score(X, y):.3f}")  # ~0.8 (pas terrible)

# Solution : Polynomial Features
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)  # [1, x, x¬≤]
model_poly = LinearRegression().fit(X_poly, y)
print(f"R¬≤ Score (poly): {model_poly.score(X_poly, y):.3f}")  # 1.0 (parfait)
```

## Variantes / liens

### Extensions de la r√©gression lin√©aire

**1. R√©gularisation** : P√©naliser les coefficients pour √©viter le surapprentissage
- **Ridge (L2)** : `Loss = MSE + Œ±¬∑Œ£Œ≤¬≤` ‚Üí coefficients plus petits
  ```python
  from sklearn.linear_model import Ridge
  model = Ridge(alpha=1.0)  # Œ± contr√¥le la r√©gularisation
  ```
- **Lasso (L1)** : `Loss = MSE + Œ±¬∑Œ£|Œ≤|` ‚Üí s√©lection de features (Œ≤=0)
  ```python
  from sklearn.linear_model import Lasso
  model = Lasso(alpha=0.1)  # Certains Œ≤ deviennent exactement 0
  ```
- **Elastic Net** : Combinaison de Ridge + Lasso
  ```python
  from sklearn.linear_model import ElasticNet
  model = ElasticNet(alpha=0.1, l1_ratio=0.5)
  ```

**2. R√©gression polynomiale** : Capturer les relations non-lin√©aires
```python
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=3)  # x, x¬≤, x¬≥
X_poly = poly.fit_transform(X)
model = LinearRegression().fit(X_poly, y)
```

**3. R√©gression robuste** : R√©sistant aux outliers
```python
from sklearn.linear_model import HuberRegressor, RANSACRegressor
model = HuberRegressor()  # Moins sensible aux outliers
```

### Relations avec d'autres mod√®les

- **Logistic Regression** : Version classification (sigmo√Øde au lieu de lin√©aire)
- **SVR (Support Vector Regression)** : R√©gression avec marge (kernel pour non-lin√©arit√©)
- **Decision Trees** : Capture automatiquement les non-lin√©arit√©s
- **Random Forest / Gradient Boosting** : Ensemble de trees (plus pr√©cis mais moins interpr√©table)
- **Neural Networks** : G√©n√©ralisation avec fonctions d'activation non-lin√©aires
- **GAM (Generalized Additive Models)** : Somme de fonctions non-lin√©aires

### Pr√©traitement associ√©

- **Standardisation** : Mettre les features √† la m√™me √©chelle
  ```python
  from sklearn.preprocessing import StandardScaler
  scaler = StandardScaler()
  X_scaled = scaler.fit_transform(X)
  ```
- **Encodage cat√©goriel** : One-hot encoding pour variables cat√©gorielles
  ```python
  from sklearn.preprocessing import OneHotEncoder
  encoder = OneHotEncoder()
  X_encoded = encoder.fit_transform(X_cat)
  ```

## R√©f√©rences

### Documentation et tutoriels
- **Scikit-learn** : [Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)
- **StatQuest** : [Linear Regression Explained](https://www.youtube.com/watch?v=nk2CQITm_eo) (YouTube)
- **Andrew Ng** : [ML Course - Linear Regression](https://www.coursera.org/learn/machine-learning)

### Livres
- **"An Introduction to Statistical Learning"** (James et al., 2021) - Chapitre 3
- **"The Elements of Statistical Learning"** (Hastie et al., 2009) - Chapitre 3
- **"Pattern Recognition and Machine Learning"** (Bishop, 2006) - Chapitre 3

### Outils Python
```python
# Scikit-learn (le plus populaire)
from sklearn.linear_model import LinearRegression, Ridge, Lasso

# Statsmodels (plus de statistiques d√©taill√©es)
import statsmodels.api as sm
model = sm.OLS(y, X).fit()
print(model.summary())  # P-values, intervalles de confiance, etc.

# NumPy (impl√©mentation manuelle)
beta = np.linalg.lstsq(X, y, rcond=None)[0]
```

### Tests statistiques associ√©s
- **Test de normalit√© des r√©sidus** : Shapiro-Wilk
- **Test d'h√©t√©rosc√©dasticit√©** : Breusch-Pagan
- **Test de multicolin√©arit√©** : VIF (Variance Inflation Factor)
- **Test de significativit√©** : P-values des coefficients
