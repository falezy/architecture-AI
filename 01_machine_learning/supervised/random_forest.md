# Random Forest

Ensemble d'arbres (bagging) robuste, peu de tuning.

## Id√©e cl√©

**Random Forest** est un **ensemble de nombreux arbres de d√©cision** entra√Æn√©s ind√©pendamment sur des sous-ensembles al√©atoires des donn√©es et des features. La pr√©diction finale est obtenue par **vote majoritaire** (classification) ou **moyenne** (r√©gression).

**Principe (Bagging + Feature Randomness)** :
1. **Bootstrap** : Cr√©er N √©chantillons al√©atoires avec remise (m√™me taille que dataset)
2. **Entra√Æner** : Pour chaque √©chantillon, entra√Æner un arbre de d√©cision
3. **Feature Randomness** : √Ä chaque split, consid√©rer seulement ‚àöd features al√©atoires (au lieu de toutes)
4. **Agr√©ger** : Vote majoritaire (classification) ou moyenne (r√©gression)

**Formule** :
```
Classification : ≈∑ = mode(h‚ÇÅ(x), h‚ÇÇ(x), ..., h‚Çô(x))
R√©gression     : ≈∑ = (1/N) Œ£ h·µ¢(x)
```
- `h·µ¢(x)` : pr√©diction de l'arbre i
- `N` : nombre d'arbres (typiquement 100-500)

**Pourquoi √ßa fonctionne ?**
- **Bootstrap** : Chaque arbre voit des donn√©es diff√©rentes ‚Üí diversit√©
- **Feature Randomness** : Arbres apprennent des patterns diff√©rents ‚Üí d√©corr√©lation
- **Moyenne** : R√©duit la variance (overfitting) sans augmenter le biais

**Diff√©rence avec un seul arbre** :
| Aspect | Arbre unique | Random Forest |
|--------|--------------|---------------|
| **Overfitting** | √âlev√© | Faible (moyenne de N arbres) |
| **Variance** | √âlev√©e | Faible |
| **Biais** | Faible | Faible |
| **Stabilit√©** | Instable | Tr√®s stable |
| **Interpr√©tabilit√©** | √âlev√©e | Moyenne (feature importance) |
| **Vitesse** | Rapide | Plus lent (N arbres) |

## Exemples concrets

### 1. Classification : Pr√©dire la survie sur le Titanic

**Sc√©nario** : Pr√©dire si un passager survit selon l'√¢ge, la classe, le sexe, et le tarif.

**Code Python avec Random Forest** :
```python
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

# 1. Donn√©es simul√©es (style Titanic)
data = {
    'Age': [22, 38, 26, 35, 28, 45, 31, 50, 18, 60, 25, 40],
    'Pclass': [3, 1, 3, 1, 3, 2, 1, 2, 3, 1, 2, 3],
    'Sex': [1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1],  # 1=Male, 0=Female
    'Fare': [7, 71, 8, 53, 8, 13, 50, 15, 7, 30, 25, 10],
    'Survived': [0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0]
}
df = pd.DataFrame(data)

X = df.drop('Survived', axis=1)
y = df['Survived']

# 2. Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

# 3. Cr√©er et entra√Æner Random Forest
model = RandomForestClassifier(
    n_estimators=100,        # Nombre d'arbres
    max_depth=10,            # Profondeur max par arbre
    min_samples_split=2,     # Min samples pour split
    min_samples_leaf=1,      # Min samples par feuille
    max_features='sqrt',     # ‚àöd features par split (d√©faut)
    bootstrap=True,          # Bootstrap sampling
    oob_score=True,          # Out-of-bag score (validation automatique)
    random_state=42,
    n_jobs=-1                # Utiliser tous les CPU
)
model.fit(X_train, y_train)

# 4. Pr√©dictions
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)

print(f"Accuracy (train): {model.score(X_train, y_train):.2%}")
print(f"Accuracy (test): {accuracy_score(y_test, y_pred):.2%}")
print(f"OOB Score: {model.oob_score_:.2%}")  # Validation automatique !

print("\nMatrice de confusion:")
print(confusion_matrix(y_test, y_pred))
print("\nRapport de classification:")
print(classification_report(y_test, y_pred, target_names=['D√©c√©d√©', 'Surv√©cu']))

# 5. Feature importance
features = X.columns
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

print("\nFeature Importance:")
for i in range(len(features)):
    print(f"{i+1}. {features[indices[i]]}: {importances[indices[i]]:.3f}")

# Visualisation
plt.figure(figsize=(10, 6))
plt.title("Feature Importance - Random Forest")
plt.bar(range(len(features)), importances[indices])
plt.xticks(range(len(features)), [features[i] for i in indices])
plt.ylabel('Importance')
plt.show()

# 6. Pr√©dire pour un nouveau passager
nouveau_passager = pd.DataFrame({
    'Age': [30],
    'Pclass': [1],
    'Sex': [0],  # Female
    'Fare': [50]
})
prediction = model.predict(nouveau_passager)[0]
proba = model.predict_proba(nouveau_passager)[0]
print(f"\nNouveau passager: {nouveau_passager.iloc[0].to_dict()}")
print(f"Pr√©diction: {'Surv√©cu' if prediction == 1 else 'D√©c√©d√©'}")
print(f"Probabilit√© de survie: {proba[1]:.2%}")
```

---

### 2. R√©gression : Pr√©dire le prix d'une maison

**Sc√©nario** : Pr√©dire le prix d'une maison selon ses caract√©ristiques.

**Code Python** :
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import numpy as np

# 1. Donn√©es
data = {
    'Surface': [50, 80, 120, 150, 70, 100, 60, 140, 90, 110, 75, 130],
    'Chambres': [1, 2, 3, 4, 2, 3, 1, 4, 2, 3, 2, 3],
    'Age': [10, 5, 2, 1, 15, 8, 20, 3, 12, 6, 18, 4],
    'Distance_centre': [5, 2, 1, 1, 10, 3, 15, 2, 8, 4, 12, 2],
    'Prix': [150, 240, 360, 450, 210, 300, 180, 420, 270, 330, 200, 380]
}
df = pd.DataFrame(data)

X = df.drop('Prix', axis=1)
y = df['Prix']

# 2. Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

# 3. Random Forest Regressor
model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=2,
    max_features='sqrt',
    oob_score=True,
    random_state=42,
    n_jobs=-1
)
model.fit(X_train, y_train)

# 4. Pr√©dictions
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

print(f"R¬≤ (train): {r2_score(y_train, y_pred_train):.3f}")
print(f"R¬≤ (test): {r2_score(y_test, y_pred_test):.3f}")
print(f"RMSE (test): {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f} k‚Ç¨")
print(f"MAE (test): {mean_absolute_error(y_test, y_pred_test):.2f} k‚Ç¨")
print(f"OOB Score: {model.oob_score_:.3f}")

# 5. Feature importance
importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)
print("\nFeature Importance:")
print(importances)

# 6. Pr√©dire pour une nouvelle maison
nouvelle_maison = pd.DataFrame({
    'Surface': [95],
    'Chambres': [3],
    'Age': [7],
    'Distance_centre': [3]
})
prix_predit = model.predict(nouvelle_maison)[0]
print(f"\nNouvelle maison: {nouvelle_maison.iloc[0].to_dict()}")
print(f"Prix pr√©dit: {prix_predit:.0f} k‚Ç¨")
```

---

### 3. Comparaison : Single Tree vs Random Forest

**Code pour montrer la r√©duction d'overfitting** :
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# Donn√©es avec du bruit
X, y = make_classification(
    n_samples=500, 
    n_features=20, 
    n_informative=15,
    n_redundant=5,
    random_state=42
)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# 1. Arbre unique (sans contrainte ‚Üí overfitting)
tree = DecisionTreeClassifier(random_state=42)
tree.fit(X_train, y_train)
print(f"Decision Tree:")
print(f"  Train accuracy: {tree.score(X_train, y_train):.2%}")
print(f"  Test accuracy: {tree.score(X_test, y_test):.2%}")
print(f"  ‚Üí Overfitting: {tree.score(X_train, y_train) - tree.score(X_test, y_test):.2%}")

# 2. Random Forest (r√©sistant √† l'overfitting)
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
print(f"\nRandom Forest:")
print(f"  Train accuracy: {rf.score(X_train, y_train):.2%}")
print(f"  Test accuracy: {rf.score(X_test, y_test):.2%}")
print(f"  ‚Üí Overfitting: {rf.score(X_train, y_train) - rf.score(X_test, y_test):.2%}")
```

**Output typique** :
```
Decision Tree:
  Train accuracy: 100.00%
  Test accuracy: 82.00%
  ‚Üí Overfitting: 18.00%

Random Forest:
  Train accuracy: 99.00%
  Test accuracy: 91.00%
  ‚Üí Overfitting: 8.00%
```

---

### 4. Hyperparameter Tuning avec GridSearchCV

**Code pour trouver les meilleurs hyperparam√®tres** :
```python
from sklearn.model_selection import GridSearchCV

# Grille de param√®tres
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

# Grid Search avec Cross-Validation
rf = RandomForestClassifier(random_state=42, n_jobs=-1)
grid_search = GridSearchCV(
    rf, 
    param_grid, 
    cv=5,                    # 5-fold cross-validation
    scoring='accuracy',
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print(f"Meilleurs param√®tres: {grid_search.best_params_}")
print(f"Meilleur score CV: {grid_search.best_score_:.3f}")

# Entra√Æner avec les meilleurs param√®tres
best_rf = grid_search.best_estimator_
test_score = best_rf.score(X_test, y_test)
print(f"Accuracy (test): {test_score:.2%}")
```

---

### 5. Out-of-Bag (OOB) Score : Validation gratuite

**Code pour utiliser OOB comme validation** :
```python
# OOB Score = validation automatique (pas besoin de validation set)
model = RandomForestClassifier(
    n_estimators=100,
    oob_score=True,  # Activer OOB
    random_state=42
)
model.fit(X_train, y_train)

# OOB score ‚âà validation score (gratuit !)
print(f"OOB Score: {model.oob_score_:.2%}")
print(f"Test Score: {model.score(X_test, y_test):.2%}")

# OOB predictions (pour chaque exemple du train set)
oob_predictions = model.oob_decision_function_  # Probabilit√©s OOB
print(f"OOB predictions shape: {oob_predictions.shape}")
```

**Explication OOB** :
- Chaque arbre est entra√Æn√© sur ~63% des donn√©es (bootstrap)
- Les 37% restants sont "out-of-bag" pour cet arbre
- On peut √©valuer l'arbre sur ces donn√©es OOB
- Moyenne des √©valuations OOB = OOB score (validation gratuite !)

## Quand l'utiliser

- ‚úÖ **Baseline solide** : Excellent point de d√©part, souvent difficile √† battre
- ‚úÖ **Peu de tuning** : Fonctionne bien avec param√®tres par d√©faut
- ‚úÖ **Donn√©es avec bruit** : Robuste aux outliers et donn√©es manquantes
- ‚úÖ **Feature importance** : Identifier les variables importantes
- ‚úÖ **Classification ET r√©gression** : Mod√®le polyvalent
- ‚úÖ **Donn√©es tabulaires** : Tr√®s bon sur donn√©es structur√©es (CSV, bases de donn√©es)
- ‚úÖ **Pas de normalisation** : Insensible √† l'√©chelle des features

**Cas d'usage typiques** :
- üí≥ **Finance** : Scoring de cr√©dit, d√©tection de fraude
- üè• **Sant√©** : Diagnostic m√©dical, pr√©diction de risque
- üéØ **Marketing** : Pr√©diction de churn, segmentation client
- üè≠ **Industrie** : Maintenance pr√©dictive, contr√¥le qualit√©
- üåæ **Agriculture** : Pr√©diction de rendement, classification de maladies

**Quand NE PAS utiliser** :
- ‚ùå Comp√©titions Kaggle top performance ‚Üí XGBoost/LightGBM
- ‚ùå Images/audio/vid√©o ‚Üí Deep Learning (CNN, RNN)
- ‚ùå Texte brut ‚Üí Transformers (BERT, GPT)
- ‚ùå Besoin d'interpr√©tabilit√© totale ‚Üí Decision Tree unique, Regression lin√©aire

## Forces

‚úÖ **Tr√®s robuste** : R√©sistant √† l'overfitting (moyenne de N arbres)  
‚úÖ **Peu de tuning** : Fonctionne bien "out of the box"  
‚úÖ **Pas de normalisation** : Insensible √† l'√©chelle des features  
‚úÖ **G√®re donn√©es manquantes** : Peut g√©rer NaN (avec strat√©gie)  
‚úÖ **Feature importance** : Identifie variables importantes  
‚úÖ **Parall√©lisable** : Entra√Ænement rapide avec n_jobs=-1  
‚úÖ **OOB Score** : Validation gratuite sans split s√©par√©

**Exemple de robustesse** :
```python
# Ajouter du bruit (outliers)
X_noisy = X.copy()
X_noisy[0, 0] = 1000  # Outlier extr√™me

# Decision Tree ‚Üí sensible
tree = DecisionTreeClassifier()
tree.fit(X_noisy, y)
print(f"Tree accuracy: {tree.score(X_test, y_test):.2%}")  # ~75%

# Random Forest ‚Üí robuste
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_noisy, y)
print(f"RF accuracy: {rf.score(X_test, y_test):.2%}")  # ~89%
```

## Limites

‚ùå **Moins performant que XGBoost** : Sur donn√©es tabulaires complexes  
‚ùå **M√©moire** : N arbres = N fois plus de m√©moire  
‚ùå **Lent en pr√©diction** : Doit interroger N arbres (vs 1 seul)  
‚ùå **Extrapolation** : Ne pr√©dit que dans la plage des valeurs d'entra√Ænement  
‚ùå **Interpr√©tabilit√©** : Moins qu'un seul arbre (100+ arbres)  
‚ùå **Biais pour features √† haute cardinalit√©** : Pr√©f√®re features avec beaucoup de valeurs  
‚ùå **Pas adapt√© aux s√©ries temporelles** : Sans features temporelles explicites

**Exemple d'extrapolation** :
```python
# Train sur prix 100-500k‚Ç¨
X_train = np.array([[100], [200], [300], [400], [500]])
y_train = np.array([100, 200, 300, 400, 500])

rf = RandomForestRegressor(n_estimators=100)
rf.fit(X_train, y_train)

# Pr√©dire pour 1000k‚Ç¨ (hors plage)
print(rf.predict([[1000]]))  # ~500 (max vu en train, pas 1000!)
```

**Temps de pr√©diction** :
```python
import time

# Single tree
tree = DecisionTreeClassifier()
tree.fit(X_train, y_train)
start = time.time()
tree.predict(X_test)
print(f"Tree predict: {time.time() - start:.4f}s")

# Random Forest (100 trees)
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)
start = time.time()
rf.predict(X_test)
print(f"RF predict: {time.time() - start:.4f}s")  # ~100x plus lent
```

## Variantes / liens

### Hyperparam√®tres cl√©s

```python
RandomForestClassifier(
    # Nombre d'arbres
    n_estimators=100,        # Plus = mieux (mais diminishing returns apr√®s 100-200)
    
    # Profondeur et complexit√©
    max_depth=None,          # None = arbres profonds (d√©faut)
    min_samples_split=2,     # Min samples pour split
    min_samples_leaf=1,      # Min samples par feuille
    max_leaf_nodes=None,     # Limiter nombre de feuilles
    
    # Feature sampling
    max_features='sqrt',     # ‚àöd pour classification (d√©faut)
                             # 'log2', 'auto', None, int, float
    
    # Bootstrap
    bootstrap=True,          # Bootstrap sampling (d√©faut)
    oob_score=False,         # Calculer OOB score
    
    # Parall√©lisation
    n_jobs=-1,               # Utiliser tous les CPU
    random_state=42          # Reproductibilit√©
)
```

**Recommandations** :
- **n_estimators** : 100-200 (bon compromis vitesse/performance)
- **max_features** : 'sqrt' (classification), 'auto' ou 1/3 (r√©gression)
- **max_depth** : None (laisser pousser) ou 10-30 si overfitting
- **min_samples_leaf** : 1-5 (augmenter si overfitting)

### Relations avec d'autres mod√®les

**1. Bagging (Bootstrap Aggregating)** :
```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# Random Forest = Bagging + Feature Randomness
bagging = BaggingClassifier(
    DecisionTreeClassifier(),
    n_estimators=100,
    max_samples=1.0,  # Bootstrap 100% des donn√©es
    bootstrap=True
)
```

**2. Extra Trees (Extremely Randomized Trees)** :
```python
from sklearn.ensemble import ExtraTreesClassifier

# Extra Trees = Random Forest + splits al√©atoires
extra_trees = ExtraTreesClassifier(
    n_estimators=100,
    max_features='sqrt',
    random_state=42
)
# Plus rapide que RF, parfois meilleur
```

**3. Gradient Boosting** (XGBoost, LightGBM) :
- **RF** : Arbres en parall√®le, ind√©pendants
- **Boosting** : Arbres s√©quentiels, correctifs
- **Performance** : Boosting > RF (mais plus sensible au tuning)

**4. Isolation Forest** (d√©tection d'anomalies) :
```python
from sklearn.ensemble import IsolationForest

# Utilise Random Forest pour anomaly detection
iso = IsolationForest(n_estimators=100, contamination=0.1)
anomalies = iso.fit_predict(X)
```

### Feature Importance avanc√©e

**Permutation Importance** (plus fiable que feature_importances_) :
```python
from sklearn.inspection import permutation_importance

# Entra√Æner le mod√®le
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)

# Permutation importance
perm_importance = permutation_importance(
    rf, X_test, y_test, 
    n_repeats=10,
    random_state=42
)

# Afficher
for i, feature in enumerate(X.columns):
    print(f"{feature}: {perm_importance.importances_mean[i]:.3f} "
          f"¬± {perm_importance.importances_std[i]:.3f}")
```

### Calibration des probabilit√©s

```python
from sklearn.calibration import CalibratedClassifierCV

# Random Forest non calibr√©
rf = RandomForestClassifier(n_estimators=100)

# Calibration
calibrated_rf = CalibratedClassifierCV(rf, cv=5, method='sigmoid')
calibrated_rf.fit(X_train, y_train)

# Probabilit√©s mieux calibr√©es
probas = calibrated_rf.predict_proba(X_test)
```

## R√©f√©rences

### Documentation et tutoriels
- **Scikit-learn** : [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest)
- **StatQuest** : [Random Forest Explained](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ) (YouTube)

### Livres
- **"An Introduction to Statistical Learning"** (James et al., 2021) - Chapitre 8
- **"The Elements of Statistical Learning"** (Hastie et al., 2009) - Chapitre 15
- **"Hands-On Machine Learning"** (G√©ron, 2019) - Chapitre 7

### Papers fondamentaux
- **Random Forests** : Breiman, 2001 - "Random Forests" (paper original)
- **Bagging** : Breiman, 1996 - "Bagging Predictors"
- **Feature Importance** : Breiman, 2001 - Mesure d'impuret√©

### Comparaison de performance

**Benchmark (Dataset : Credit Card Fraud Detection)** :
```
Algorithme              Accuracy    AUC-ROC    Temps
Logistic Regression       92%        0.85      1s
Decision Tree             85%        0.78      2s
Random Forest (100)       97%        0.95      15s
XGBoost                   98%        0.97      25s

‚Üí RF: Excellent compromis performance/simplicit√©
```

### Tuning rapide (r√®gles empiriques)

**Si overfitting** :
```python
# Augmenter r√©gularisation
RandomForestClassifier(
    n_estimators=100,
    max_depth=10,          # Limiter profondeur
    min_samples_leaf=5,    # Augmenter min samples
    max_features='sqrt'    # Moins de features
)
```

**Si underfitting** :
```python
# R√©duire r√©gularisation
RandomForestClassifier(
    n_estimators=200,      # Plus d'arbres
    max_depth=None,        # Arbres profonds
    min_samples_leaf=1,    # Moins de contraintes
    max_features='auto'    # Plus de features
)
```

**Si trop lent** :
```python
# Acc√©l√©rer
RandomForestClassifier(
    n_estimators=50,       # Moins d'arbres
    max_depth=10,          # Limiter profondeur
    max_samples=0.8,       # Sous-√©chantillonner
    n_jobs=-1              # Parall√©liser
)
```
