# SVM

Classification/r√©gression via marges maximales, kernels possibles.

## Id√©e cl√©

**SVM (Support Vector Machine)** est un algorithme de classification qui trouve la **fronti√®re de d√©cision optimale** en maximisant la **marge** entre les classes. La marge est la distance entre la fronti√®re et les points les plus proches de chaque classe (appel√©s **support vectors**).

**Principe** :
1. Trouver l'hyperplan qui s√©pare les classes avec la **plus grande marge possible**
2. Les points les plus proches de la fronti√®re sont les **support vectors**
3. Seuls les support vectors influencent la position de la fronti√®re
4. Utiliser des **kernels** pour g√©rer les donn√©es non-lin√©airement s√©parables

**Formule (cas lin√©aire)** :
```
f(x) = w¬∑x + b
Pr√©diction: sign(f(x)) = { +1 si f(x) ‚â• 0
                          { -1 sinon
```
- `w` : vecteur de poids (normal √† l'hyperplan)
- `b` : biais (intercept)
- Marge = `2/||w||`

**Objectif d'optimisation** :
```
Maximiser: marge = 2/||w||
√âquivalent √† minimiser: ||w||¬≤ / 2
Sous contrainte: y·µ¢(w¬∑x·µ¢ + b) ‚â• 1  pour tout i
```

**Visualisation (2D)** :
```
        Classe +1
          ‚Ä¢  ‚Ä¢  ‚Ä¢
         ‚ï±       ‚ï≤
   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Hyperplan
       ‚ï± Support   ‚ï≤
      ‚Ä¢  Vectors    ‚Ä¢
    Classe -1
    
    ‚Üê Marge ‚Üí
```

**Marge douce (Soft Margin)** :
- Param√®tre `C` : contr√¥le le compromis entre marge large et erreurs
- `C` grand : Marge √©troite, peu d'erreurs (peut overfitter)
- `C` petit : Marge large, tol√®re plus d'erreurs (plus r√©gularis√©)

**Kernel Trick** :
- Permet de transformer des donn√©es non-lin√©airement s√©parables
- Projette les donn√©es dans un espace de dimension sup√©rieure
- Kernels populaires : lin√©aire, polynomial, RBF (Gaussian), sigmo√Ød

## Exemples concrets

### 1. Classification lin√©aire : Donn√©es s√©parables

**Sc√©nario** : Classifier deux classes lin√©airement s√©parables.

**Code Python avec SVM lin√©aire** :
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# 1. G√©n√©rer donn√©es lin√©airement s√©parables
X, y = make_blobs(
    n_samples=100, 
    centers=2, 
    n_features=2,
    center_box=(-5, 5),
    random_state=42
)

# 2. Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 3. SVM lin√©aire
model = SVC(
    kernel='linear',    # Kernel lin√©aire
    C=1.0,              # Param√®tre de r√©gularisation
    random_state=42
)
model.fit(X_train, y_train)

# 4. Pr√©dictions
y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.2%}")
print(f"\nNombre de support vectors: {len(model.support_vectors_)}")
print(f"Indices des support vectors: {model.support_}")

# 5. Visualisation
def plot_svm_decision_boundary(model, X, y, title):
    # Cr√©er grille
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(
        np.arange(x_min, x_max, 0.02),
        np.arange(y_min, y_max, 0.02)
    )
    
    # Pr√©dire sur grille
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # Tracer
    plt.figure(figsize=(10, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')
    
    # Tracer les support vectors
    plt.scatter(
        model.support_vectors_[:, 0],
        model.support_vectors_[:, 1],
        s=200, 
        linewidth=2,
        facecolors='none', 
        edgecolors='green',
        label='Support Vectors'
    )
    
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title(title)
    plt.legend()
    plt.colorbar()
    plt.show()

plot_svm_decision_boundary(model, X, y, 'SVM Lin√©aire - Fronti√®re de d√©cision')

# 6. √âquation de l'hyperplan
w = model.coef_[0]
b = model.intercept_[0]
print(f"\n√âquation de l'hyperplan:")
print(f"  {w[0]:.3f}¬∑x‚ÇÅ + {w[1]:.3f}¬∑x‚ÇÇ + {b:.3f} = 0")
print(f"  Marge: {2 / np.linalg.norm(w):.3f}")
```

---

### 2. Classification non-lin√©aire : Kernel RBF

**Sc√©nario** : Donn√©es non-lin√©airement s√©parables (cercles concentriques).

**Code Python avec kernel RBF (Gaussian)** :
```python
from sklearn.datasets import make_circles

# 1. Donn√©es circulaires (non-lin√©aires)
X, y = make_circles(
    n_samples=200, 
    factor=0.5,      # Ratio entre cercles
    noise=0.1,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 2. Essayer SVM lin√©aire (va √©chouer)
svm_linear = SVC(kernel='linear')
svm_linear.fit(X_train, y_train)
print(f"SVM Lin√©aire accuracy: {svm_linear.score(X_test, y_test):.2%}")

# 3. SVM avec kernel RBF (Gaussian)
svm_rbf = SVC(
    kernel='rbf',    # Radial Basis Function
    C=1.0,           # R√©gularisation
    gamma='scale',   # Influence de chaque exemple (d√©faut: 1/(n_features * X.var()))
    random_state=42
)
svm_rbf.fit(X_train, y_train)
print(f"SVM RBF accuracy: {svm_rbf.score(X_test, y_test):.2%}")

# 4. Visualisation
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# SVM lin√©aire
ax = axes[0]
x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
Z = svm_linear.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')
ax.set_title('SVM Lin√©aire (√©chec)')

# SVM RBF
ax = axes[1]
Z = svm_rbf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')
ax.scatter(
    svm_rbf.support_vectors_[:, 0],
    svm_rbf.support_vectors_[:, 1],
    s=200, linewidth=2, facecolors='none', edgecolors='green',
    label='Support Vectors'
)
ax.set_title(f'SVM RBF (accuracy: {svm_rbf.score(X_test, y_test):.0%})')
ax.legend()

plt.tight_layout()
plt.show()

print(f"\nNombre de support vectors (RBF): {len(svm_rbf.support_vectors_)}")
```

---

### 3. Comparaison des kernels

**Code pour comparer lin√©aire, polynomial, RBF** :
```python
from sklearn.datasets import make_moons

# Donn√©es en forme de lunes (non-lin√©aires)
X, y = make_moons(n_samples=200, noise=0.15, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Tester diff√©rents kernels
kernels = {
    'Lin√©aire': SVC(kernel='linear', C=1),
    'Polynomial (deg=3)': SVC(kernel='poly', degree=3, C=1),
    'RBF': SVC(kernel='rbf', C=1, gamma='scale'),
    'Sigmo√Ød': SVC(kernel='sigmoid', C=1)
}

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.ravel()

for idx, (name, model) in enumerate(kernels.items()):
    model.fit(X_train, y_train)
    accuracy = model.score(X_test, y_test)
    
    # Visualiser
    ax = axes[idx]
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')
    ax.scatter(
        model.support_vectors_[:, 0],
        model.support_vectors_[:, 1],
        s=100, linewidth=1.5, facecolors='none', edgecolors='green'
    )
    ax.set_title(f'{name}\nAccuracy: {accuracy:.0%}, SV: {len(model.support_vectors_)}')

plt.tight_layout()
plt.show()
```

---

### 4. Tuning de C et gamma (kernel RBF)

**Code pour comprendre l'impact de C et gamma** :
```python
# Impact de C (r√©gularisation)
C_values = [0.1, 1, 10, 100]
fig, axes = plt.subplots(1, 4, figsize=(16, 4))

X, y = make_circles(n_samples=200, factor=0.5, noise=0.1, random_state=42)

for idx, C in enumerate(C_values):
    model = SVC(kernel='rbf', C=C, gamma='scale')
    model.fit(X, y)
    
    ax = axes[idx]
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', s=30)
    ax.set_title(f'C={C}\nSV: {len(model.support_vectors_)}')

plt.tight_layout()
plt.show()

# Impact de gamma
gamma_values = [0.1, 1, 10, 100]
fig, axes = plt.subplots(1, 4, figsize=(16, 4))

for idx, gamma in enumerate(gamma_values):
    model = SVC(kernel='rbf', C=1, gamma=gamma)
    model.fit(X, y)
    
    ax = axes[idx]
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', s=30)
    ax.set_title(f'gamma={gamma}\nSV: {len(model.support_vectors_)}')

plt.tight_layout()
plt.show()
```

**Interpr√©tation** :
- **C petit** : Marge large, tol√®re erreurs ‚Üí underfitting
- **C grand** : Marge √©troite, peu d'erreurs ‚Üí overfitting
- **gamma petit** : Influence large (fronti√®re lisse)
- **gamma grand** : Influence locale (fronti√®re complexe) ‚Üí overfitting

---

### 5. SVM pour r√©gression (SVR)

**Code pour Support Vector Regression** :
```python
from sklearn.svm import SVR
import numpy as np

# 1. Donn√©es de r√©gression avec bruit
np.random.seed(42)
X = np.sort(5 * np.random.rand(100, 1), axis=0)
y = np.sin(X).ravel() + np.random.randn(100) * 0.1

# 2. Comparer diff√©rents kernels
svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)
svr_lin = SVR(kernel='linear', C=100, epsilon=0.1)
svr_poly = SVR(kernel='poly', C=100, degree=3, epsilon=0.1)

# 3. Entra√Æner
models = [svr_rbf, svr_lin, svr_poly]
names = ['RBF', 'Linear', 'Polynomial']

X_test = np.linspace(0, 5, 300)[:, np.newaxis]

plt.figure(figsize=(12, 6))
plt.scatter(X, y, color='darkorange', label='Donn√©es')

for model, name, color in zip(models, names, ['navy', 'red', 'green']):
    model.fit(X, y)
    y_pred = model.predict(X_test)
    plt.plot(X_test, y_pred, color=color, linewidth=2, label=f'SVR {name}')

plt.xlabel('X')
plt.ylabel('y')
plt.title('Support Vector Regression - Comparaison kernels')
plt.legend()
plt.show()
```

---

### 6. Grid Search pour tuning optimal

**Code pour trouver les meilleurs hyperparam√®tres** :
```python
from sklearn.model_selection import GridSearchCV

# Donn√©es
X, y = make_circles(n_samples=200, factor=0.5, noise=0.1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Grille de param√®tres
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],
    'kernel': ['rbf', 'poly', 'sigmoid']
}

# Grid Search
svm = SVC()
grid_search = GridSearchCV(
    svm, 
    param_grid, 
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

print(f"Meilleurs param√®tres: {grid_search.best_params_}")
print(f"Meilleur score CV: {grid_search.best_score_:.3f}")
print(f"Score test: {grid_search.score(X_test, y_test):.3f}")

# Entra√Æner avec meilleurs param√®tres
best_model = grid_search.best_estimator_
print(f"\nNombre de support vectors: {len(best_model.support_vectors_)}")
```

## Quand l'utiliser

- ‚úÖ **Donn√©es moyenne/haute dimensionnalit√©** : Fonctionne bien avec beaucoup de features
- ‚úÖ **Fronti√®res complexes** : Kernels permettent des d√©cisions non-lin√©aires
- ‚úÖ **Petits/moyens datasets** : Moins de 10,000 exemples (scaling en O(n¬≤) ou O(n¬≥))
- ‚úÖ **Robustesse aux outliers** : Seuls les support vectors comptent
- ‚úÖ **Marges claires** : Classes bien s√©par√©es
- ‚úÖ **Classification binaire** : Excellente performance (extension multi-classe possible)

**Cas d'usage typiques** :
- üìù **Text classification** : Cat√©gorisation de documents, spam detection
- üß¨ **Bioinformatique** : Classification de prot√©ines, g√®nes
- üñºÔ∏è **Vision** : Reconnaissance de visages (avec features HOG/SIFT)
- üíä **M√©decine** : Diagnostic (nombreuses features, peu d'exemples)
- üí∞ **Finance** : Scoring de cr√©dit

**Quand NE PAS utiliser** :
- ‚ùå Tr√®s grandes donn√©es (>100,000) ‚Üí trop lent ‚Üí Logistic Regression, Random Forest
- ‚ùå Beaucoup de bruit ‚Üí Random Forest plus robuste
- ‚ùå Besoin de probabilit√©s calibr√©es ‚Üí predict_proba de SVM peu fiable (utiliser Platt scaling)
- ‚ùå Interpr√©tabilit√© critique ‚Üí Decision Tree, Linear Regression
- ‚ùå Images/texte brut ‚Üí Deep Learning (CNN, Transformers)

## Forces

‚úÖ **Fronti√®res complexes** : Kernels permettent s√©parations non-lin√©aires  
‚úÖ **Robuste en haute dimension** : Fonctionne bien avec d >> n  
‚úÖ **Memory efficient** : Utilise seulement les support vectors  
‚úÖ **Versatile** : Nombreux kernels (lin√©aire, RBF, polynomial, custom)  
‚úÖ **R√©gularisation int√©gr√©e** : Param√®tre C contr√¥le overfitting  
‚úÖ **Base th√©orique solide** : Optimisation convexe bien d√©finie

**Exemple de robustesse en haute dimension** :
```python
from sklearn.datasets import make_classification

# 200 features, 100 exemples (d >> n)
X, y = make_classification(n_samples=100, n_features=200, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# SVM fonctionne bien
svm = SVC(kernel='linear')
svm.fit(X_train, y_train)
print(f"SVM accuracy (200 features): {svm.score(X_test, y_test):.2%}")

# Random Forest moins bon
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)
print(f"Random Forest accuracy: {rf.score(X_test, y_test):.2%}")
```

## Limites

‚ùå **Lent sur grandes donn√©es** : O(n¬≤) √† O(n¬≥) en complexit√©  
‚ùå **Choix du kernel difficile** : N√©cessite exp√©rimentation  
‚ùå **Tuning hyperparam√®tres** : C, gamma, kernel √† optimiser  
‚ùå **Pas de probabilit√©s** : predict_proba peu fiable (Platt scaling requis)  
‚ùå **Sensible √† l'√©chelle** : N√©cessite normalisation des features  
‚ùå **Pas d'interpr√©tabilit√©** : Difficile d'expliquer (sauf kernel lin√©aire)  
‚ùå **Multi-classe** : Extension OvO ou OvR (pas natif)

**Temps d'entra√Ænement** :
```python
import time
from sklearn.datasets import make_classification

# Comparer scaling avec taille des donn√©es
for n in [100, 1000, 5000]:
    X, y = make_classification(n_samples=n, n_features=20, random_state=42)
    
    start = time.time()
    svm = SVC(kernel='rbf')
    svm.fit(X, y)
    elapsed = time.time() - start
    
    print(f"n={n:5d}: {elapsed:.2f}s")

# Output typique:
# n=  100: 0.01s
# n= 1000: 0.15s
# n= 5000: 3.50s  (croissance rapide!)
```

**Normalisation obligatoire** :
```python
from sklearn.preprocessing import StandardScaler

# Sans normalisation
X, y = make_classification(n_samples=200, n_features=2, random_state=42)
X[:, 0] *= 1000  # Feature 1 entre 0-1000
X[:, 1] *= 0.01  # Feature 2 entre 0-0.01

svm_no_scale = SVC(kernel='rbf')
svm_no_scale.fit(X, y)
print(f"Sans normalisation: {svm_no_scale.score(X, y):.2%}")

# Avec normalisation
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
svm_scaled = SVC(kernel='rbf')
svm_scaled.fit(X_scaled, y)
print(f"Avec normalisation: {svm_scaled.score(X_scaled, y):.2%}")
```

## Variantes / liens

### Types de SVM

**1. SVC (Support Vector Classification)** :
```python
from sklearn.svm import SVC

# Classification binaire/multi-classe
svc = SVC(
    C=1.0,              # R√©gularisation (plus grand = moins de r√©gularisation)
    kernel='rbf',       # 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'
    degree=3,           # Degr√© du polynomial (si kernel='poly')
    gamma='scale',      # Coefficient du kernel ('scale', 'auto', float)
    class_weight=None,  # 'balanced' pour classes d√©s√©quilibr√©es
    probability=False,  # Activer predict_proba (mais plus lent)
    random_state=42
)
```

**2. SVR (Support Vector Regression)** :
```python
from sklearn.svm import SVR

# R√©gression
svr = SVR(
    kernel='rbf',
    C=1.0,
    epsilon=0.1,    # Tube epsilon (tol√©rance d'erreur)
    gamma='scale'
)
```

**3. LinearSVC** (plus rapide pour kernel lin√©aire) :
```python
from sklearn.svm import LinearSVC

# Optimis√© pour kernel lin√©aire (beaucoup plus rapide)
linear_svc = LinearSVC(
    C=1.0,
    max_iter=1000,
    dual=True  # dual=False si n_samples > n_features
)
```

**4. NuSVC** (alternative √† C) :
```python
from sklearn.svm import NuSVC

# Utilise nu au lieu de C (interpr√©tation diff√©rente)
nu_svc = NuSVC(
    nu=0.5,  # Borne sup√©rieure sur fraction d'erreurs (0 < nu ‚â§ 1)
    kernel='rbf'
)
```

### Kernels disponibles

**Formules des kernels** :

1. **Lin√©aire** : `K(x, x') = x ¬∑ x'`
   ```python
   SVC(kernel='linear')
   ```

2. **Polynomial** : `K(x, x') = (gamma¬∑x¬∑x' + coef0)^degree`
   ```python
   SVC(kernel='poly', degree=3, gamma='scale', coef0=0)
   ```

3. **RBF (Gaussian)** : `K(x, x') = exp(-gamma¬∑||x - x'||¬≤)`
   ```python
   SVC(kernel='rbf', gamma='scale')
   ```

4. **Sigmo√Ød** : `K(x, x') = tanh(gamma¬∑x¬∑x' + coef0)`
   ```python
   SVC(kernel='sigmoid', gamma='scale', coef0=0)
   ```

5. **Custom kernel** :
   ```python
   def my_kernel(X, Y):
       # Impl√©menter votre propre kernel
       return np.dot(X, Y.T)
   
   SVC(kernel=my_kernel)
   ```

### Hyperparam√®tres cl√©s

**C (r√©gularisation)** :
- `C` grand (ex: 100) ‚Üí Marge √©troite, peu d'erreurs ‚Üí risque overfitting
- `C` petit (ex: 0.1) ‚Üí Marge large, tol√®re erreurs ‚Üí risque underfitting
- Par d√©faut: `C=1.0`

**gamma (kernel RBF)** :
- `gamma` grand ‚Üí Influence locale √©troite ‚Üí risque overfitting
- `gamma` petit ‚Üí Influence large ‚Üí fronti√®re lisse
- `'scale'` : `1 / (n_features * X.var())` (recommand√©)
- `'auto'` : `1 / n_features`

**degree (kernel polynomial)** :
- `degree=2` : Fronti√®re quadratique
- `degree=3` : Fronti√®re cubique (d√©faut)
- Plus le degr√© augmente, plus la fronti√®re est complexe

### Relations avec d'autres mod√®les

- **Logistic Regression** : Similaire au SVM lin√©aire mais avec loss diff√©rent
- **Perceptron** : Anc√™tre de SVM (pas de marge maximale)
- **Neural Networks** : Kernel RBF ‚âà couche cach√©e RBF
- **Kernel Methods** : Kernel PCA, Kernel Ridge Regression utilisent m√™me principe
- **AdaBoost** : Autre approche pour fronti√®res complexes

### Preprocessing recommand√©

**Pipeline complet** :
```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Pipeline: normalisation + SVM
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Indispensable pour SVM
    ('svm', SVC(kernel='rbf', C=1, gamma='scale'))
])

pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
```

### Probabilit√©s avec Platt Scaling

```python
# Activer predict_proba (mais plus lent)
svm_proba = SVC(kernel='rbf', probability=True)
svm_proba.fit(X_train, y_train)

# Probabilit√©s calibr√©es
probas = svm_proba.predict_proba(X_test)
print(probas[:5])  # Probabilit√©s pour chaque classe
```

## R√©f√©rences

### Documentation et tutoriels
- **Scikit-learn** : [SVM Guide](https://scikit-learn.org/stable/modules/svm.html)
- **StatQuest** : [SVM Explained](https://www.youtube.com/watch?v=efR1C6CvhmE) (YouTube)
- **Andrew Ng** : [ML Course - SVM](https://www.coursera.org/learn/machine-learning) (Coursera)

### Livres
- **"An Introduction to Statistical Learning"** (James et al., 2021) - Chapitre 9
- **"The Elements of Statistical Learning"** (Hastie et al., 2009) - Chapitre 12
- **"Pattern Recognition and Machine Learning"** (Bishop, 2006) - Chapitre 7

### Papers fondamentaux
- **SVM original** : Cortes & Vapnik, 1995 - "Support-Vector Networks"
- **Kernel Trick** : Boser, Guyon & Vapnik, 1992 - "Training Algorithm for Optimal Margin Classifiers"
- **SMO Algorithm** : Platt, 1998 - "Sequential Minimal Optimization"

### Th√©orie

**Kernel Trick** :
```
Au lieu de calculer œÜ(x) (projection haute dimension),
on calcule K(x, x') = œÜ(x) ¬∑ œÜ(x') directement

Exemple RBF:
- Projection explicite: dimension infinie !
- Kernel: simple fonction exp(-||x-x'||¬≤)
```

**Dualit√©** :
```
Probl√®me primal: minimiser ||w||¬≤ / 2
Probl√®me dual: maximiser Œ£Œ±·µ¢ - ¬ΩŒ£Œ£Œ±·µ¢Œ±‚±ºy·µ¢y‚±ºK(x·µ¢,x‚±º)

‚Üí R√©solution du dual (plus efficace avec kernels)
```

### Comparaison de performance

**Benchmark (MNIST digits, 10 classes)** :
```
Algorithme          Accuracy    Temps
Logistic Regression   92%       5s
Decision Tree         87%       3s
Random Forest         96%       45s
SVM (linear)          94%       120s
SVM (RBF)             98%       350s
Neural Network        99%       180s

‚Üí SVM RBF: Excellente accuracy mais lent
```

### Tuning rapide (r√®gles empiriques)

**Recommandations** :
1. **Toujours normaliser** : `StandardScaler`
2. **Commencer avec RBF** : kernel='rbf', C=1, gamma='scale'
3. **Si trop lent** : Utiliser `LinearSVC` ou sous-√©chantillonner
4. **Grid Search** : Tester C=[0.1, 1, 10, 100], gamma=[0.001, 0.01, 0.1, 1]
5. **Classes d√©s√©quilibr√©es** : `class_weight='balanced'`

**Exemple tuning rapide** :
```python
# √âtape 1: Tester kernel lin√©aire (rapide)
svm_lin = SVC(kernel='linear', C=1)
svm_lin.fit(X_train, y_train)
score_lin = svm_lin.score(X_test, y_test)

# √âtape 2: Si lin√©aire insuffisant, tester RBF
if score_lin < 0.85:
    svm_rbf = SVC(kernel='rbf', C=1, gamma='scale')
    svm_rbf.fit(X_train, y_train)
    score_rbf = svm_rbf.score(X_test, y_test)
    
    # √âtape 3: Si RBF meilleur, tuner C et gamma
    if score_rbf > score_lin:
        # Grid search sur C et gamma
        pass
```
