# Naive Bayes

Classifieur probabiliste simple, efficace sur texte.

## Id√©e cl√©

**Naive Bayes** est un classifieur probabiliste bas√© sur le **th√©or√®me de Bayes** avec l'hypoth√®se "na√Øve" que toutes les features sont **ind√©pendantes** entre elles. Malgr√© cette hypoth√®se souvent fausse, il fonctionne remarquablement bien en pratique, surtout pour la classification de texte.

**Th√©or√®me de Bayes** :
```
P(Classe|Features) = P(Features|Classe) ¬∑ P(Classe) / P(Features)
```

Simplifi√© pour la classification :
```
P(C|x‚ÇÅ,x‚ÇÇ,...,x‚Çô) ‚àù P(C) ¬∑ P(x‚ÇÅ|C) ¬∑ P(x‚ÇÇ|C) ¬∑ ... ¬∑ P(x‚Çô|C)
```

- `P(C)` : **Prior** (probabilit√© a priori de la classe)
- `P(x·µ¢|C)` : **Likelihood** (probabilit√© de la feature sachant la classe)
- `P(C|x)` : **Posterior** (probabilit√© de la classe sachant les features)

**Hypoth√®se "na√Øve" (ind√©pendance conditionnelle)** :
```
P(x‚ÇÅ,x‚ÇÇ,...,x‚Çô|C) = P(x‚ÇÅ|C) ¬∑ P(x‚ÇÇ|C) ¬∑ ... ¬∑ P(x‚Çô|C)
```

**D√©cision** :
```
Classe pr√©dite = argmax P(C) ¬∑ ‚àè P(x·µ¢|C)
                    C        i
```

### Exemple simple : M√©teo et Tennis

**Question** : Jouer au tennis selon la m√©t√©o ?

| M√©t√©o | Jouer Tennis |
|-------|--------------|
| Soleil | Oui |
| Pluie | Non |
| Nuageux | Oui |
| Soleil | Oui |
| Pluie | Non |

**Calculer** : P(Oui | Soleil) vs P(Non | Soleil)

```
P(Oui | Soleil) ‚àù P(Soleil | Oui) ¬∑ P(Oui)
                = (2/3) ¬∑ (3/5) = 0.4

P(Non | Soleil) ‚àù P(Soleil | Non) ¬∑ P(Non)
                = (0/2) ¬∑ (2/5) = 0

‚Üí Pr√©diction : OUI
```

## Exemples concrets

### 1. Classification de texte : D√©tection de spam

**Sc√©nario** : Classifier  emails comme spam/non-spam selon les mots pr√©sents.

**Code Python avec Multinomial Naive Bayes** :
```python
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. Donn√©es d'exemple
emails = [
    "Win free money now",
    "Meeting at 3pm tomorrow",
    "Claim your prize today",
    "Project deadline next week",
    "Congratulations you won",
    "Lunch with team on Friday",
    "Limited offer act now",
    "Review the quarterly report",
    "Get rich quick scheme",
    "Conference call at 2pm"
]
labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1=spam, 0=non-spam

# 2. Convertir texte en features (bag-of-words)
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(emails)
y = np.array(labels)

print(f"Vocabulaire: {vectorizer.get_feature_names_out()[:10]}...")
print(f"Matrice features shape: {X.shape}")

# 3. Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 4. Entra√Æner Multinomial Naive Bayes
model = MultinomialNB(alpha=1.0)  # alpha = lissage de Laplace
model.fit(X_train, y_train)

# 5. Pr√©dire pour un nouvel email
nouvel_email = ["Win a free vacation now"]
X_new = vectorizer.transform(nouvel_email)
prediction = model.predict(X_new)[0]
proba = model.predict_proba(X_new)[0]

print(f"\nNouvel email: {nouvel_email[0]}")
print(f"Pr√©diction: {'SPAM' if prediction == 1 else 'NON-SPAM'}")
print(f"Probabilit√© spam: {proba[1]:.2%}")

# 6. √âvaluation
y_pred = model.predict(X_test)
print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.2%}")
print("\nMatrice de confusion:")
print(confusion_matrix(y_test, y_pred))
print("\nRapport de classification:")
print(classification_report(y_test, y_pred, target_names=['Non-spam', 'Spam']))

# 7. Probabilit√©s par classe (log probabilities)
print("\nLog probabilit√©s des mots (spam):")
feature_names = vectorizer.get_feature_names_out()
log_probs = model.feature_log_prob_[1]  # Classe spam
top_indices = np.argsort(log_probs)[-5:]  # Top 5 mots
for idx in top_indices:
    print(f"  {feature_names[idx]}: {np.exp(log_probs[idx]):.3f}")
```

---

### 2. Sentiment Analysis avec TF-IDF

**Sc√©nario** : Classifier des avis clients comme positifs ou n√©gatifs.

**Code Python** :
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

# 1. Donn√©es
avis = [
    "Ce produit est excellent, je recommande",
    "Tr√®s d√©√ßu, qualit√© m√©diocre",
    "Parfait, correspond √† mes attentes",
    "Arnaque, ne fonctionne pas du tout",
    "Superbe qualit√©, excellent rapport qualit√©-prix",
    "Service client horrible, produit cass√©",
    "Incroyable, meilleur achat de l'ann√©e",
    "√Ä √©viter absolument, perte de temps",
]
sentiments = [1, 0, 1, 0, 1, 0, 1, 0]  # 1=positif, 0=n√©gatif

# 2. Pipeline: TF-IDF + Naive Bayes
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=100)),  # TF-IDF au lieu de CountVectorizer
    ('nb', MultinomialNB())
])

# 3. Entra√Æner
pipeline.fit(avis, sentiments)

# 4. Pr√©dire
nouveaux_avis = [
    "Produit de tr√®s bonne qualit√©",
    "Compl√®tement nul, je regrette"
]

for avis_text in nouveaux_avis:
    prediction = pipeline.predict([avis_text])[0]
    proba = pipeline.predict_proba([avis_text])[0]
    sentiment = "POSITIF" if prediction == 1 else "N√âGATIF"
    print(f"\nAvis: {avis_text}")
    print(f"Sentiment: {sentiment} (confiance: {max(proba):.2%})")
```

---

### 3. Gaussian Naive Bayes : Classification num√©rique

**Sc√©nario** : Classifier des fleurs (Iris dataset) selon longueur/largeur des p√©tales.

**Code Python** :
```python
from sklearn.datasets import load_iris
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# 1. Charger dataset Iris
iris = load_iris()
X = iris.data[:, :2]  # Utiliser seulement 2 features pour visualisation
y = iris.target

# 2. Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 3. Gaussian Naive Bayes (pour features continues)
model = GaussianNB()
model.fit(X_train, y_train)

# 4. Pr√©dire
y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.2%}")
print("\nRapport de classification:")
print(classification_report(
    y_test, y_pred, 
    target_names=iris.target_names
))

# 5. Probabilit√©s pour une nouvelle fleur
nouvelle_fleur = [[5.1, 3.5]]  # Longueur/largeur s√©pale
probas = model.predict_proba(nouvelle_fleur)[0]
print("\nNouvelle fleur:", nouvelle_fleur[0])
for i, classe in enumerate(iris.target_names):
    print(f"  P({classe}): {probas[i]:.2%}")
```

**Visualisation des fronti√®res de d√©cision** :
```python
import matplotlib.pyplot as plt
import numpy as np

def plot_decision_boundary_gaussian(X, y, model):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(
        np.arange(x_min, x_max, 0.02),
        np.arange(y_min, y_max, 0.02)
    )
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.figure(figsize=(10, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Gaussian Naive Bayes - Fronti√®res de d√©cision')
    plt.colorbar()
    plt.show()

plot_decision_boundary_gaussian(X, y, model)
```

---

### 4. Comparaison des 3 variantes

**Code pour comparer Gaussian, Multinomial, Bernoulli** :
```python
from sklearn.datasets import make_classification
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.preprocessing import MinMaxScaler

# Donn√©es synth√©tiques
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Normaliser pour Multinomial (features positives)
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Comparer les 3 variantes
models = {
    'Gaussian': GaussianNB(),
    'Multinomial': MultinomialNB(),
    'Bernoulli': BernoulliNB()
}

for name, model in models.items():
    if name == 'Gaussian':
        model.fit(X_train, y_train)
        score = model.score(X_test, y_test)
    else:
        model.fit(X_train_scaled, y_train)
        score = model.score(X_test_scaled, y_test)
    
    print(f"{name} NB: {score:.2%}")
```

## Quand l'utiliser

- ‚úÖ **Classification de texte** : Spam, sentiment analysis, cat√©gorisation de documents
- ‚úÖ **Donn√©es haute dimensionnalit√©** : Nombreuses features (ex: bag-of-words)
- ‚úÖ **Baseline rapide** : Entra√Ænement tr√®s rapide, bon point de d√©part
- ‚úÖ **Temps r√©el** : Pr√©dictions instantan√©es, faible latence
- ‚úÖ **Peu de donn√©es d'entra√Ænement** : Fonctionne avec petits datasets
- ‚úÖ **Interpr√©tabilit√©** : Probabilit√©s faciles √† comprendre
- ‚úÖ **Online learning** : Mise √† jour incr√©mentale possible (`partial_fit`)

**Cas d'usage typiques** :
- üìß **Email** : Filtrage de spam, classification automatique
- üí¨ **NLP** : Sentiment analysis, classification de topics, d√©tection de langue
- üì∞ **M√©dias** : Cat√©gorisation d'articles, recommandations
- üè• **M√©decine** : Diagnostic bas√© sur sympt√¥mes (si ind√©pendance raisonnable)
- üîê **S√©curit√©** : D√©tection d'intrusion, classification de malware

**Quand NE PAS utiliser** :
- ‚ùå Features fortement corr√©l√©es (viole l'hypoth√®se d'ind√©pendance)
- ‚ùå Besoin de performance maximale sur donn√©es tabulaires ‚Üí XGBoost
- ‚ùå Relations complexes non-lin√©aires ‚Üí Deep Learning

## Forces

‚úÖ **Tr√®s rapide** : Entra√Ænement et pr√©diction quasi-instantan√©s  
‚úÖ **Peu de donn√©es** : Fonctionne bien avec petits datasets  
‚úÖ **Scalable** : G√®re bien grandes dimensions (millions de features)  
‚úÖ **Pas d'hyperparam√®tres** : Juste le lissage de Laplace (alpha)  
‚úÖ **Probabilit√©s** : Fournit des probabilit√©s calibr√©es  
‚úÖ **Multi-classe natif** : Pas besoin de One-vs-Rest  
‚úÖ **Online learning** : Mise √† jour incr√©mentale avec `partial_fit()`

**Exemple de vitesse** :
```python
import time
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

# Dataset texte (18,000 documents)
newsgroups = fetch_20newsgroups(subset='all')
X = TfidfVectorizer(max_features=10000).fit_transform(newsgroups.data)
y = newsgroups.target

# Entra√Ænement
start = time.time()
model = MultinomialNB()
model.fit(X, y)
print(f"Temps d'entra√Ænement: {time.time() - start:.2f}s")  # ~0.1s !

# Pr√©diction
start = time.time()
predictions = model.predict(X[:1000])
print(f"Temps pr√©diction (1000 docs): {time.time() - start:.4f}s")  # ~0.001s !
```

## Limites

‚ùå **Hypoth√®se d'ind√©pendance** : Rarement vraie (features souvent corr√©l√©es)  
‚ùå **Performance limit√©e** : Moins bon que XGBoost/Neural Nets sur donn√©es tabulaires  
‚ùå **Sensible aux features inutiles** : Toutes les features affectent le score  
‚ùå **Zero-frequency problem** : Mot jamais vu ‚Üí P=0 (r√©solu par lissage)  
‚ùå **Pas de r√©gularisation** : Peut overfitter avec trop de features  
‚ùå **Relations complexes** : Ne capture pas interactions entre features  
‚ùå **Calibration des probabilit√©s** : Probas peuvent √™tre biais√©es

**Exemple du probl√®me zero-frequency** :
```python
# Si "gratuit" n'appara√Æt jamais dans les emails non-spam
# ‚Üí P("gratuit" | non-spam) = 0
# ‚Üí P(non-spam | email avec "gratuit") = 0 (m√™me si autres mots l√©gitimes)

# Solution: Lissage de Laplace (alpha)
model = MultinomialNB(alpha=1.0)  # Ajoute pseudo-count de 1 partout

# alpha=0 ‚Üí Pas de lissage (risque zero-frequency)
# alpha=1 ‚Üí Lissage de Laplace (d√©faut, recommand√©)
# alpha>1 ‚Üí Lissage fort (plus conservateur)
```

**Impact de l'hypoth√®se d'ind√©pendance** :
```python
# Exemple: "Bon" et "Excellent" sont corr√©l√©s (souvent ensemble)
# Naive Bayes compte leur co-occurrence 2 fois (surpoids)
# ‚Üí Probabilit√©s biais√©es mais souvent classification correcte quand m√™me !
```

## Variantes / liens

### Les 3 variantes principales

**1. Gaussian Naive Bayes** - Features continues (distribution normale)
```python
from sklearn.naive_bayes import GaussianNB

model = GaussianNB(
    var_smoothing=1e-9  # Lissage de la variance (stabilit√© num√©rique)
)
```

**Quand** : Features num√©riques continues (temp√©rature, taille, poids, etc.)  
**Hypoth√®se** : Chaque feature suit une distribution normale (Gaussienne)

**2. Multinomial Naive Bayes** - Comptages discrets (texte)
```python
from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB(
    alpha=1.0,       # Lissage de Laplace (d√©faut: 1.0)
    fit_prior=True   # Apprendre P(C) des donn√©es (d√©faut: True)
)
```

**Quand** : Bag-of-words, TF-IDF, fr√©quences de mots  
**Hypoth√®se** : Features repr√©sentent des comptages (entiers positifs)

**3. Bernoulli Naive Bayes** - Features binaires
```python
from sklearn.naive_bayes import BernoulliNB

model = BernoulliNB(
    alpha=1.0,
    binarize=0.0  # Seuil pour binariser les features (0.0 = d√©j√† binaires)
)
```

**Quand** : Features binaires (pr√©sence/absence de mots)  
**Hypoth√®se** : Chaque feature est 0 ou 1

### Tableau comparatif

| Variante | Type de features | Distribution | Use case | Exemple |
|----------|-----------------|--------------|----------|---------|
| **Gaussian** | Continues | Normale | Donn√©es num√©riques | Iris, temp√©rature |
| **Multinomial** | Comptages | Multinomiale | Fr√©quences de mots | TF-IDF, bag-of-words |
| **Bernoulli** | Binaires | Bernoulli | Pr√©sence/absence | Document contient "free"? |

### Online Learning (partial_fit)

**Entra√Ænement incr√©mental sur flux de donn√©es** :
```python
from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()

# Entra√Æner par batches
for batch_X, batch_y in data_stream:
    model.partial_fit(
        batch_X, 
        batch_y, 
        classes=np.array([0, 1])  # Classes possibles (requis au 1er appel)
    )

# Pr√©dire en continu
predictions = model.predict(new_data)
```

**Avantage** : Mise √† jour du mod√®le sans tout r√©-entra√Æner (adaptatif)

### Relations avec d'autres mod√®les

- **R√©gression logistique** : Mod√®le discriminatif (vs NB = g√©n√©ratif)
- **LDA** (Linear Discriminant Analysis) : Similaire mais assume covariance partag√©e
- **K-Nearest Neighbors** : Autre baseline rapide mais plus lent en pr√©diction
- **TF-IDF + Cosine Similarity** : Alternative pour classification texte
- **Deep Learning (BERT)** : Meilleur pour texte mais beaucoup plus lent

### Preprocessing pour texte

**Pipeline complet** :
```python
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(
        max_features=5000,      # Top 5000 mots
        stop_words='english',   # Retirer mots vides
        ngram_range=(1, 2),     # Unigrammes + bigrammes
        min_df=2,               # Ignorer mots trop rares
        max_df=0.8              # Ignorer mots trop fr√©quents
    )),
    ('nb', MultinomialNB(alpha=1.0))
])

pipeline.fit(texts_train, y_train)
predictions = pipeline.predict(texts_test)
```

## R√©f√©rences

### Documentation
- **Scikit-learn** : [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)
- **StatQuest** : [Naive Bayes Explained](https://www.youtube.com/watch?v=O2L2Uv9pdDA) (YouTube)

### Livres
- **"An Introduction to Statistical Learning"** (James et al., 2021) - Chapitre 4
- **"Pattern Recognition and Machine Learning"** (Bishop, 2006) - Chapitre 8
- **"Speech and Language Processing"** (Jurafsky & Martin, 2023) - Chapitre 4

### Papers fondamentaux
- **Bayes' Theorem** : Thomas Bayes, 1763 (posthume)
- **"Naive Bayes at Forty"** (Lewis, 1998) - Analyse de performance
- **"Spam Filtering with Naive Bayes"** (Sahami et al., 1998)

### Th√©or√®me de Bayes

**Formulation compl√®te** :
```
P(A|B) = P(B|A) ¬∑ P(A) / P(B)

O√π:
- P(A|B) : Posterior (ce qu'on cherche)
- P(B|A) : Likelihood (vraisemblance)
- P(A) : Prior (probabilit√© a priori)
- P(B) : Evidence (normalisation)
```

**Exemple m√©dical** :
```
Maladie M, Test T positif
P(M|T+) = P(T+|M) ¬∑ P(M) / P(T+)

- P(M) = 0.01 (1% population malade)
- P(T+|M) = 0.99 (sensibilit√©: 99%)
- P(T+|¬¨M) = 0.05 (5% faux positifs)

P(T+) = P(T+|M)¬∑P(M) + P(T+|¬¨M)¬∑P(¬¨M)
      = 0.99¬∑0.01 + 0.05¬∑0.99 = 0.0594

P(M|T+) = 0.99¬∑0.01 / 0.0594 = 0.167 (16.7%)

‚Üí M√™me avec test positif, seulement 16.7% de chance d'√™tre malade !
```

### Tuning des hyperparam√®tres

```python
from sklearn.model_selection import GridSearchCV

# Grid search pour alpha (lissage)
param_grid = {
    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0]
}

grid = GridSearchCV(
    MultinomialNB(), 
    param_grid, 
    cv=5, 
    scoring='accuracy'
)
grid.fit(X_train, y_train)

print(f"Meilleur alpha: {grid.best_params_['alpha']}")
print(f"Meilleur score: {grid.best_score_:.3f}")
```

### Comparaison performance (dataset 20newsgroups)

```
Algorithme                 Accuracy    Temps
Naive Bayes (Multinomial)    77%      0.1s
Logistic Regression          82%      2.5s
SVM (Linear)                 83%      8.2s
Random Forest                75%      15s
XGBoost                      84%      25s

‚Üí NB: 2e meilleur rapport performance/vitesse !
```
