# Decision Trees

Mod√®le interpr√©table bas√© sur des r√®gles de split.

## Id√©e cl√©

Un **arbre de d√©cision** est un mod√®le qui apprend une s√©quence de **questions binaires** (if/else) pour pr√©dire une valeur. Il divise r√©cursivement l'espace des features en r√©gions homog√®nes.

**Fonctionnement** :
1. **Root node** : Choisir la meilleure feature pour diviser les donn√©es
2. **Split** : Cr√©er deux branches (gauche/droite) selon une condition
3. **R√©p√©ter** : Pour chaque branche, choisir la prochaine meilleure question
4. **Leaf nodes** : Arr√™ter quand crit√®re de puret√© atteint (ou profondeur max)

**Visualisation conceptuelle** :
```
                    [Surface < 70m¬≤?]
                    /              \
                 OUI               NON
                /                    \
        [Prix < 200k‚Ç¨]         [Chambres < 3?]
         /        \              /          \
      OUI        NON          OUI          NON
       /          \           /             \
   Petit      Moyen      Grand          Tr√®s Grand
```

**Crit√®res de split** :
- **Classification** : Gini impurity, Entropy (Information Gain)
- **R√©gression** : MSE, MAE

**Formule Gini Impurity** :
```
Gini = 1 - Œ£(p·µ¢)¬≤
```
o√π `p·µ¢` = proportion de la classe i

**Formule Entropy** :
```
Entropy = -Œ£(p·µ¢ ¬∑ log‚ÇÇ(p·µ¢))
```

## Exemples concrets

### 1. Classification : Pr√©dire l'approbation d'un pr√™t bancaire

**Sc√©nario** : Une banque veut automatiser l'approbation de pr√™ts selon le revenu, l'√¢ge et l'historique de cr√©dit.

**Donn√©es d'exemple** :
```
Revenu (k‚Ç¨) | √Çge | Cr√©dit | Pr√™t approuv√©?
30          | 25  | Bon    | Non
70          | 35  | Bon    | Oui
50          | 45  | Mauvais| Non
90          | 50  | Bon    | Oui
```

**Code Python avec scikit-learn** :
```python
import numpy as np
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# 1. Donn√©es d'entra√Ænement
X = np.array([
    [30, 25, 1],  # Revenu, √Çge, Cr√©dit (1=Bon, 0=Mauvais)
    [70, 35, 1],
    [50, 45, 0],
    [90, 50, 1],
    [40, 30, 0],
    [80, 40, 1],
    [35, 28, 1],
    [60, 38, 0],
])
y = np.array([0, 1, 0, 1, 0, 1, 0, 0])  # 0=Refus√©, 1=Approuv√©

# 2. Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

# 3. Cr√©er et entra√Æner l'arbre
model = DecisionTreeClassifier(
    max_depth=3,           # Profondeur maximale
    criterion='gini',      # ou 'entropy'
    min_samples_split=2,   # Min samples pour split
    random_state=42
)
model.fit(X_train, y_train)

# 4. Pr√©dire pour un nouveau client
nouveau_client = np.array([[55, 32, 1]])  # 55k‚Ç¨, 32 ans, bon cr√©dit
prediction = model.predict(nouveau_client)
proba = model.predict_proba(nouveau_client)

print(f"Pr√©diction: {'Approuv√©' if prediction[0] == 1 else 'Refus√©'}")
print(f"Probabilit√©: {proba[0][1]:.2%}")

# 5. √âvaluation
from sklearn.metrics import accuracy_score, classification_report
y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.2%}")
print(classification_report(y_test, y_pred, target_names=['Refus√©', 'Approuv√©']))

# 6. Feature importance
features = ['Revenu', '√Çge', 'Cr√©dit']
importances = model.feature_importances_
for feature, importance in zip(features, importances):
    print(f"{feature}: {importance:.3f}")
```

**Visualisation de l'arbre** :
```python
plt.figure(figsize=(15, 8))
plot_tree(
    model, 
    feature_names=['Revenu', '√Çge', 'Cr√©dit'],
    class_names=['Refus√©', 'Approuv√©'],
    filled=True,           # Couleurs selon classe
    rounded=True,
    fontsize=10
)
plt.title("Arbre de d√©cision - Approbation de pr√™t")
plt.show()
```

**Interpr√©tation** :
```
Si Revenu > 60k‚Ç¨ ‚Üí Approuv√©
Sinon:
    Si Cr√©dit = Bon ET √Çge > 30 ‚Üí Approuv√©
    Sinon ‚Üí Refus√©
```

---

### 2. R√©gression : Pr√©dire le prix d'une maison

**Sc√©nario** : Pr√©dire le prix d'une maison selon sa surface et le nombre de chambres.

**Code Python** :
```python
from sklearn.tree import DecisionTreeRegressor
import numpy as np

# 1. Donn√©es
X = np.array([
    [50, 1],   # 50m¬≤, 1 chambre
    [80, 2],   # 80m¬≤, 2 chambres
    [120, 3],  # 120m¬≤, 3 chambres
    [150, 4],
    [70, 2],
    [100, 3],
    [60, 1],
    [140, 4],
])
y = np.array([150, 240, 360, 450, 210, 300, 180, 420])  # Prix en k‚Ç¨

# 2. Cr√©er l'arbre de r√©gression
model = DecisionTreeRegressor(
    max_depth=3,
    min_samples_leaf=2,
    random_state=42
)
model.fit(X, y)

# 3. Pr√©dire pour une nouvelle maison
nouvelle_maison = np.array([[90, 2]])  # 90m¬≤, 2 chambres
prix_predit = model.predict(nouvelle_maison)
print(f"Prix pr√©dit pour 90m¬≤, 2 chambres: {prix_predit[0]:.0f}k‚Ç¨")

# 4. Visualiser l'arbre
plt.figure(figsize=(15, 8))
plot_tree(
    model,
    feature_names=['Surface', 'Chambres'],
    filled=True,
    rounded=True
)
plt.show()

# 5. Feature importance
print(f"Importance Surface: {model.feature_importances_[0]:.3f}")
print(f"Importance Chambres: {model.feature_importances_[1]:.3f}")
```

---

### 3. Visualisation des fronti√®res de d√©cision

**Code pour visualiser comment l'arbre divise l'espace** :
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification

# Donn√©es 2D pour visualisation
X, y = make_classification(
    n_samples=100, 
    n_features=2, 
    n_redundant=0, 
    n_clusters_per_class=1,
    random_state=42
)

# Entra√Æner l'arbre
model = DecisionTreeClassifier(max_depth=3)
model.fit(X, y)

# Cr√©er une grille pour visualiser les fronti√®res
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(
    np.arange(x_min, x_max, 0.02),
    np.arange(y_min, y_max, 0.02)
)

# Pr√©dire pour chaque point de la grille
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Visualisation
plt.figure(figsize=(10, 6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='k')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Fronti√®res de d√©cision (lignes rectangulaires)')
plt.show()
```

**Observation** : Les arbres cr√©ent des **fronti√®res rectangulaires** (perpendiculaires aux axes).

## Quand l'utiliser

- ‚úÖ **Interpr√©tabilit√© requise** : Besoin d'expliquer les d√©cisions (m√©decine, finance, juridique)
- ‚úÖ **Donn√©es mixtes** : Features num√©riques ET cat√©gorielles (pas de preprocessing n√©cessaire)
- ‚úÖ **Relations non-lin√©aires** : Capture automatiquement les interactions complexes
- ‚úÖ **Peu de preprocessing** : Pas besoin de normalisation ou d'encodage one-hot
- ‚úÖ **Baseline rapide** : Mod√®le simple √† mettre en place pour tester rapidement
- ‚úÖ **Feature importance** : Identifier les variables les plus importantes

**Cas d'usage typiques** :
- üè• **M√©decine** : Diagnostic bas√© sur sympt√¥mes (arbre de d√©cision clinique)
- üí≥ **Finance** : Approbation de cr√©dit, d√©tection de fraude
- üéØ **Marketing** : Segmentation client, pr√©diction de churn
- üè≠ **Industrie** : Maintenance pr√©dictive, contr√¥le qualit√©
- üìä **Sciences** : Classification d'esp√®ces, analyse d'images

## Forces

‚úÖ **Tr√®s interpr√©table** : Visualisation simple, r√®gles if/else compr√©hensibles  
‚úÖ **Pas de preprocessing** : G√®re directement les features cat√©gorielles et num√©riques  
‚úÖ **Pas de normalisation** : Insensible √† l'√©chelle des features  
‚úÖ **Capture non-lin√©arit√©s** : Relations complexes sans feature engineering  
‚úÖ **Feature importance** : Identifie automatiquement les variables importantes  
‚úÖ **Rapide √† entra√Æner** : Complexit√© O(n¬∑d¬∑log(n))  
‚úÖ **Robuste aux outliers** : Bas√© sur des splits, pas des distances

**Exemple de force** :
```python
# Pas besoin de preprocessing !
X = pd.DataFrame({
    'Surface': [50, 80, 120],        # √âchelle 0-200
    'Distance_centre': [1, 10, 50],  # √âchelle 0-100
    'Type': ['Appartement', 'Maison', 'Villa']  # Cat√©goriel
})

# Fonctionne directement avec :
from sklearn.tree import DecisionTreeRegressor
# Apr√®s encodage basique des cat√©gories
```

## Limites

‚ùå **Surapprentissage** : Tr√®s sujet √† l'overfitting (m√©morise le bruit)  
‚ùå **Instabilit√©** : Petite variation des donn√©es ‚Üí arbre compl√®tement diff√©rent  
‚ùå **Fronti√®res rectangulaires** : Inefficace pour fronti√®res diagonales/circulaires  
‚ùå **Biais vers features √† forte cardinalit√©** : Pr√©f√®re les features avec beaucoup de valeurs  
‚ùå **Probl√®me XOR** : Difficile de capturer certaines relations g√©om√©triques  
‚ùå **Pr√©dictions discontinues** : Changements brusques aux fronti√®res  
‚ùå **Pas d'extrapolation** : Pr√©dit uniquement des valeurs vues (r√©gression)

**Exemple de surapprentissage** :
```python
# Arbre sans contrainte ‚Üí overfitting
model_overfit = DecisionTreeClassifier()  # Pas de max_depth
model_overfit.fit(X_train, y_train)
print(f"Train accuracy: {model_overfit.score(X_train, y_train):.2%}")  # 100%
print(f"Test accuracy: {model_overfit.score(X_test, y_test):.2%}")    # ~70%

# Arbre r√©gularis√© ‚Üí meilleur
model_regularized = DecisionTreeClassifier(
    max_depth=5,         # Limiter la profondeur
    min_samples_leaf=10  # Min samples par feuille
)
model_regularized.fit(X_train, y_train)
print(f"Test accuracy: {model_regularized.score(X_test, y_test):.2%}")  # ~85%
```

**Hyperparam√®tres pour contr√¥ler l'overfitting** :
```python
DecisionTreeClassifier(
    max_depth=5,              # Profondeur max (d√©faut: None)
    min_samples_split=20,     # Min samples pour split (d√©faut: 2)
    min_samples_leaf=10,      # Min samples par feuille (d√©faut: 1)
    max_features='sqrt',      # Nb features √† consid√©rer (d√©faut: all)
    max_leaf_nodes=50,        # Nb max de feuilles
    min_impurity_decrease=0.01  # Min gain d'impuret√© pour split
)
```

## Variantes / liens

### Ensembles d'arbres (solutions au surapprentissage)

**1. Random Forest** : Moyenne de plusieurs arbres al√©atoires
```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=100,    # Nombre d'arbres
    max_depth=10,
    random_state=42
)
model.fit(X_train, y_train)
```

**Avantages** :
- ‚úÖ Moins de surapprentissage qu'un seul arbre
- ‚úÖ Meilleure g√©n√©ralisation
- ‚úÖ Robuste au bruit
- ‚ùå Perd en interpr√©tabilit√©

**2. Gradient Boosting** : Arbres s√©quentiels qui corrigent les erreurs
```python
from sklearn.ensemble import GradientBoostingClassifier
# Ou XGBoost, LightGBM, CatBoost (plus performants)

model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3
)
```

**Librairies modernes** :
```python
# XGBoost (populaire en comp√©titions Kaggle)
import xgboost as xgb
model = xgb.XGBClassifier(n_estimators=100, max_depth=5)

# LightGBM (tr√®s rapide)
import lightgbm as lgb
model = lgb.LGBMClassifier(n_estimators=100)

# CatBoost (meilleur pour features cat√©gorielles)
from catboost import CatBoostClassifier
model = CatBoostClassifier(iterations=100, verbose=0)
```

### Relations avec d'autres mod√®les

- **CART** : Classification And Regression Trees (algorithme standard)
- **ID3, C4.5** : Arbres bas√©s sur Information Gain (plus anciens)
- **Extra Trees** : Arbres avec splits al√©atoires
- **Isolation Forest** : Arbres pour d√©tection d'anomalies

### Visualisation avanc√©e

**Export en format texte** :
```python
from sklearn.tree import export_text
tree_rules = export_text(model, feature_names=['Surface', 'Chambres'])
print(tree_rules)
```

**Export en Graphviz** :
```python
from sklearn.tree import export_graphviz
import graphviz

dot_data = export_graphviz(
    model,
    feature_names=['Surface', 'Chambres'],
    class_names=['Petit', 'Grand'],
    filled=True,
    rounded=True
)
graph = graphviz.Source(dot_data)
graph.render("decision_tree")  # Sauvegarde en PDF
```

## R√©f√©rences

### Documentation et tutoriels
- **Scikit-learn** : [Decision Trees](https://scikit-learn.org/stable/modules/tree.html)
- **StatQuest** : [Decision Trees Explained](https://www.youtube.com/watch?v=_L39rN6gz7Y) (YouTube)
- **Visualisation interactive** : [R2D3 Visual Intro](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)

### Livres
- **"An Introduction to Statistical Learning"** (James et al., 2021) - Chapitre 8
- **"The Elements of Statistical Learning"** (Hastie et al., 2009) - Chapitre 9
- **"Hands-On Machine Learning"** (G√©ron, 2019) - Chapitre 6

### Algorithmes et impl√©mentations
- **CART** : Breiman et al., 1984 (algorithme de base)
- **ID3** : Quinlan, 1986 (Information Gain)
- **C4.5** : Quinlan, 1993 (am√©lioration de ID3)
- **Scikit-learn** : Implementation optimis√©e en Cython

### Librairies pour ensembles
```python
# Scikit-learn (basique)
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# XGBoost (performance)
pip install xgboost

# LightGBM (rapidit√©)
pip install lightgbm

# CatBoost (cat√©gories)
pip install catboost
```

### M√©triques d'√©valuation des arbres
- **Gini Impurity** : Mesure de "puret√©" des n≈ìuds
- **Entropy / Information Gain** : Mesure de r√©duction d'incertitude
- **Variance Reduction** : Pour la r√©gression (MSE)
