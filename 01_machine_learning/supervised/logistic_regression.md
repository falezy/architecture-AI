# Logistic Regression

Classification lin√©aire (probabilit√©s) pour binaire/multi-classe.

## Id√©e cl√©

La **r√©gression logistique** est un mod√®le de **classification** (pas de r√©gression malgr√© son nom !) qui pr√©dit la **probabilit√©** qu'un exemple appartienne √† une classe. Elle utilise la fonction **sigmo√Øde** pour transformer une combinaison lin√©aire en probabilit√© entre 0 et 1.

**Formule** :
```
z = Œ≤‚ÇÄ + Œ≤‚ÇÅ¬∑x‚ÇÅ + Œ≤‚ÇÇ¬∑x‚ÇÇ + ... + Œ≤‚Çô¬∑x‚Çô
P(y=1|x) = œÉ(z) = 1 / (1 + e^(-z))
```

- `z` : score lin√©aire (logit)
- `œÉ(z)` : fonction sigmo√Øde
- `P(y=1|x)` : probabilit√© que y=1 sachant x

**Fonction sigmo√Øde** :
```
      1 |           ________
        |         /
  P(y=1)|       /
        |     /
      0 |___/________________
        -‚àû    0    +‚àû
             z (logit)
```

**Propri√©t√©s** :
- Si `z ‚Üí +‚àû` alors `P(y=1) ‚Üí 1`
- Si `z ‚Üí -‚àû` alors `P(y=1) ‚Üí 0`
- Si `z = 0` alors `P(y=1) = 0.5`

**D√©cision** :
```
Si P(y=1) ‚â• 0.5 ‚Üí Classe 1
Sinon ‚Üí Classe 0
```

**Diff√©rence avec r√©gression lin√©aire** :
| Aspect | R√©gression lin√©aire | R√©gression logistique |
|--------|-------------------|----------------------|
| **T√¢che** | R√©gression (pr√©dire valeur continue) | Classification (pr√©dire classe) |
| **Output** | Valeur r√©elle (-‚àû √† +‚àû) | Probabilit√© (0 √† 1) |
| **Fonction** | Lin√©aire : `y = Œ≤x` | Sigmo√Øde : `P = œÉ(Œ≤x)` |
| **Loss** | MSE | Log Loss (Cross-Entropy) |

**Fonction de co√ªt (Log Loss)** :
```
Loss = -[y¬∑log(p) + (1-y)¬∑log(1-p)]
```
- P√©nalise fortement les mauvaises pr√©dictions confiantes

## Exemples concrets

### 1. Classification binaire : D√©tection de spam

**Sc√©nario** : Classifier un email comme spam (1) ou non-spam (0) selon le nombre de mots suspects et la longueur.

**Code Python avec scikit-learn** :
```python
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# 1. Donn√©es d'exemple
X = np.array([
    [5, 100],   # 5 mots suspects, 100 mots au total
    [2, 50],
    [15, 200],
    [1, 30],
    [20, 250],
    [3, 80],
    [18, 180],
    [0, 40],
    [25, 300],
    [8, 150]
])
y = np.array([0, 0, 1, 0, 1, 0, 1, 0, 1, 1])  # 0=non-spam, 1=spam

# 2. Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 3. Cr√©er et entra√Æner le mod√®le
model = LogisticRegression(
    solver='lbfgs',      # Algorithme d'optimisation (L-BFGS)
    max_iter=1000,
    random_state=42
)
model.fit(X_train, y_train)

# 4. Coefficients
print(f"Intercept (Œ≤‚ÇÄ): {model.intercept_[0]:.3f}")
print(f"Coefficients: {model.coef_[0]}")
print(f"  Œ≤‚ÇÅ (mots suspects): {model.coef_[0][0]:.3f}")
print(f"  Œ≤‚ÇÇ (longueur): {model.coef_[0][1]:.3f}")

# 5. Pr√©dire pour un nouvel email
nouvel_email = np.array([[10, 120]])  # 10 mots suspects, 120 mots
probabilite = model.predict_proba(nouvel_email)[0]
prediction = model.predict(nouvel_email)[0]

print(f"\nNouvel email: {nouvel_email[0]}")
print(f"Probabilit√© spam: {probabilite[1]:.2%}")
print(f"Pr√©diction: {'SPAM' if prediction == 1 else 'NON-SPAM'}")

# 6. √âvaluation
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.2%}")
print(f"AUC-ROC: {roc_auc_score(y_test, y_proba):.3f}")
print("\nMatrice de confusion:")
print(confusion_matrix(y_test, y_pred))
print("\nRapport de classification:")
print(classification_report(y_test, y_pred, target_names=['Non-spam', 'Spam']))
```

**Visualisation de la fronti√®re de d√©cision** :
```python
# Fonction pour tracer la fronti√®re
def plot_decision_boundary(X, y, model):
    # Cr√©er une grille
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 10, X[:, 1].max() + 10
    xx, yy = np.meshgrid(
        np.arange(x_min, x_max, 0.1),
        np.arange(y_min, y_max, 1)
    )
    
    # Pr√©dire pour chaque point
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # Tracer
    plt.figure(figsize=(10, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='k')
    plt.xlabel('Nombre de mots suspects')
    plt.ylabel('Longueur du message')
    plt.title('Fronti√®re de d√©cision - R√©gression Logistique')
    plt.colorbar(label='Classe')
    plt.show()

plot_decision_boundary(X, y, model)
```

---

### 2. Courbe ROC et choix du seuil

**Code pour analyser les performances et ajuster le seuil** :
```python
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# 1. Obtenir les probabilit√©s
y_proba = model.predict_proba(X_test)[:, 1]

# 2. Calculer la courbe ROC
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
auc = roc_auc_score(y_test, y_proba)

# 3. Tracer la courbe ROC
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Al√©atoire')
plt.xlabel('Taux de faux positifs (FPR)')
plt.ylabel('Taux de vrais positifs (TPR)')
plt.title('Courbe ROC')
plt.legend()
plt.grid(True)
plt.show()

# 4. Ajuster le seuil (au lieu de 0.5 par d√©faut)
seuil_optimal = 0.3  # Exemple: favoriser le rappel (d√©tecter plus de spam)
y_pred_seuil = (y_proba >= seuil_optimal).astype(int)

print(f"Avec seuil = 0.5:")
print(classification_report(y_test, model.predict(X_test)))

print(f"\nAvec seuil = {seuil_optimal}:")
print(classification_report(y_test, y_pred_seuil))
```

**Interpr√©tation** :
- **AUC = 1.0** : Mod√®le parfait
- **AUC = 0.5** : Mod√®le al√©atoire (ligne diagonale)
- **Seuil** : Ajuster selon le co√ªt des faux positifs vs faux n√©gatifs

---

### 3. Classification multi-classe : Diagnostic m√©dical

**Sc√©nario** : Classifier une maladie (A, B, C) selon la temp√©rature et le pouls.

**Code Python** :
```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
import numpy as np

# 1. Donn√©es simul√©es (3 classes)
X, y = make_classification(
    n_samples=300,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_classes=3,
    n_clusters_per_class=1,
    random_state=42
)

# Labels: 0=Maladie A, 1=Maladie B, 2=Maladie C

# 2. Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 3. R√©gression logistique multi-classe
model = LogisticRegression(
    multi_class='multinomial',  # One-vs-Rest ou multinomial
    solver='lbfgs',
    max_iter=1000
)
model.fit(X_train, y_train)

# 4. Pr√©diction avec probabilit√©s pour chaque classe
nouveau_patient = np.array([[0.5, 1.2]])
probas = model.predict_proba(nouveau_patient)[0]
prediction = model.predict(nouveau_patient)[0]

print("Probabilit√©s:")
for i, p in enumerate(probas):
    print(f"  Maladie {chr(65+i)}: {p:.2%}")
print(f"\nDiagnostic: Maladie {chr(65+prediction)}")

# 5. √âvaluation
from sklearn.metrics import accuracy_score, classification_report
y_pred = model.predict(X_test)
print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.2%}")
print("\nRapport de classification:")
print(classification_report(
    y_test, y_pred, 
    target_names=['Maladie A', 'Maladie B', 'Maladie C']
))
```

**Visualisation des fronti√®res multi-classes** :
```python
def plot_multiclass_decision_boundary(X, y, model):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(
        np.arange(x_min, x_max, 0.02),
        np.arange(y_min, y_max, 0.02)
    )
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.figure(figsize=(10, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Fronti√®res de d√©cision - Classification multi-classe')
    plt.colorbar(label='Classe')
    plt.show()

plot_multiclass_decision_boundary(X, y, model)
```

---

### 4. R√©gularisation : Ridge (L2) et Lasso (L1)

**Code pour √©viter le surapprentissage** :
```python
from sklearn.linear_model import LogisticRegression

# Donn√©es avec beaucoup de features (risque d'overfitting)
X, y = make_classification(n_samples=100, n_features=50, n_informative=10)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# 1. Sans r√©gularisation
model_none = LogisticRegression(penalty=None, max_iter=1000)
model_none.fit(X_train, y_train)
print(f"Sans r√©gularisation:")
print(f"  Train: {model_none.score(X_train, y_train):.2%}")
print(f"  Test: {model_none.score(X_test, y_test):.2%}")

# 2. Avec L2 (Ridge) - par d√©faut
model_l2 = LogisticRegression(penalty='l2', C=1.0, max_iter=1000)
model_l2.fit(X_train, y_train)
print(f"\nAvec L2 (C=1.0):")
print(f"  Train: {model_l2.score(X_train, y_train):.2%}")
print(f"  Test: {model_l2.score(X_test, y_test):.2%}")

# 3. Avec L1 (Lasso) - s√©lection de features
model_l1 = LogisticRegression(penalty='l1', C=1.0, solver='saga', max_iter=1000)
model_l1.fit(X_train, y_train)
print(f"\nAvec L1 (C=1.0):")
print(f"  Train: {model_l1.score(X_train, y_train):.2%}")
print(f"  Test: {model_l1.score(X_test, y_test):.2%}")
print(f"  Features s√©lectionn√©es: {np.sum(model_l1.coef_[0] != 0)}/{X.shape[1]}")
```

**Param√®tre C** :
- `C` grand (ex: 100) ‚Üí Peu de r√©gularisation (peut overfitter)
- `C` petit (ex: 0.01) ‚Üí Forte r√©gularisation (peut underfitter)
- `C = 1.0` ‚Üí Bon point de d√©part

## Quand l'utiliser

- ‚úÖ **Classification binaire** : Spam/non-spam, fraude/l√©gal, malade/sain
- ‚úÖ **Probabilit√©s n√©cessaires** : Besoin de `P(y=1)` plut√¥t qu'une simple classe
- ‚úÖ **Interpr√©tabilit√©** : Comprendre l'impact de chaque feature (coefficients)
- ‚úÖ **Baseline** : Mod√®le simple et rapide pour commencer
- ‚úÖ **Donn√©es lin√©airement s√©parables** : Classes s√©parables par une ligne/hyperplan
- ‚úÖ **Peu de donn√©es** : Fonctionne bien avec petits datasets (contrairement aux deep learning)

**Cas d'usage typiques** :
- üè• **M√©decine** : Diagnostic (malade/sain), risque de r√©admission
- üí≥ **Finance** : Approbation de pr√™t, d√©tection de fraude, d√©faut de paiement
- üìß **Marketing** : Classification spam, pr√©diction de clic (CTR), churn
- üéì **√âducation** : Pr√©diction de r√©ussite/√©chec d'un √©tudiant
- üîê **S√©curit√©** : D√©tection d'intrusion, authentification

## Forces

‚úÖ **Simple et rapide** : Entra√Ænement tr√®s rapide, peu de ressources  
‚úÖ **Interpr√©table** : Coefficients indiquent l'impact de chaque feature  
‚úÖ **Probabilit√©s calibr√©es** : Donne des probabilit√©s (pas juste des classes)  
‚úÖ **Peu de donn√©es** : Fonctionne avec petits datasets  
‚úÖ **R√©gularisation int√©gr√©e** : L1/L2 pour √©viter overfitting  
‚úÖ **Multi-classe natif** : One-vs-Rest ou Multinomial  
‚úÖ **Pas de tuning** : Peu d'hyperparam√®tres (contrairement √† XGBoost)

**Exemple d'interpr√©tabilit√©** :
```python
# Comprendre l'impact des features
coefficients = model.coef_[0]
features = ['Mots suspects', 'Longueur']

for feature, coef in zip(features, coefficients):
    impact = "augmente" if coef > 0 else "diminue"
    print(f"{feature}: {impact} la probabilit√© de spam de {abs(coef):.3f}")
    
# Output:
# Mots suspects: augmente la probabilit√© de spam de 0.245
# Longueur: augmente la probabilit√© de spam de 0.018
```

## Limites

‚ùå **Hypoth√®se de lin√©arit√©** : Assume une fronti√®re lin√©aire (ligne droite)  
‚ùå **Features engineering** : N√©cessite de cr√©er des features pertinentes  
‚ùå **Pas pour relations complexes** : XOR, fronti√®res circulaires difficiles  
‚ùå **Sensible aux outliers** : Peut biaiser la fronti√®re de d√©cision  
‚ùå **Multicolin√©arit√©** : Probl√®me si features corr√©l√©es  
‚ùå **Pas adapt√© aux images/texte brut** : Mieux avec features extraites  
‚ùå **D√©s√©quilibre de classes** : Requiert `class_weight='balanced'`

**Exemple de limitation (XOR problem)** :
```python
# Probl√®me XOR (non-lin√©airement s√©parable)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 1, 1, 0])  # XOR

model = LogisticRegression()
model.fit(X, y)
print(f"Accuracy: {model.score(X, y):.2%}")  # ~50% (al√©atoire!)

# Solution: Ajouter des features non-lin√©aires
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)  # Ajoute x1*x2, x1¬≤, x2¬≤

model_poly = LogisticRegression()
model_poly.fit(X_poly, y)
print(f"Accuracy avec features polynomiales: {model_poly.score(X_poly, y):.2%}")  # 100%
```

**G√©rer le d√©s√©quilibre de classes** :
```python
# Dataset d√©s√©quilibr√©: 90% classe 0, 10% classe 1
model = LogisticRegression(
    class_weight='balanced',  # P√©nalise plus les erreurs sur classe minoritaire
    max_iter=1000
)
model.fit(X_train, y_train)
```

## Variantes / liens

### Solveurs (algorithmes d'optimisation)

```python
LogisticRegression(
    solver='...',  # Choix du solveur
    max_iter=1000
)
```

| Solveur | R√©gularisation | Vitesse | Multi-classe | Quand l'utiliser |
|---------|---------------|---------|--------------|------------------|
| **lbfgs** | L2 | Rapide | ‚úÖ | **D√©faut** : petites/moyennes donn√©es |
| **liblinear** | L1, L2 | Moyen | ‚ùå (OvR) | Grandes donn√©es + binaire |
| **saga** | L1, L2, ElasticNet | Lent | ‚úÖ | Tr√®s grandes donn√©es |
| **newton-cg** | L2 | Rapide | ‚úÖ | Peu de features |
| **sag** | L2 | Rapide | ‚úÖ | Grandes donn√©es |

**Recommandation** :
- Donn√©es < 10,000 ‚Üí `lbfgs` (d√©faut)
- Donn√©es > 100,000 ‚Üí `saga` ou `sag`
- Besoin de L1 (feature selection) ‚Üí `saga` ou `liblinear`

### Multi-classe : One-vs-Rest vs Multinomial

```python
# One-vs-Rest (OvR): N mod√®les binaires (1 par classe)
model_ovr = LogisticRegression(multi_class='ovr')

# Multinomial: 1 mod√®le avec softmax
model_multi = LogisticRegression(multi_class='multinomial', solver='lbfgs')
```

**Diff√©rences** :
- **OvR** : Plus simple, plus rapide, mais probabilit√©s non calibr√©es
- **Multinomial** : Meilleur pour probabilit√©s, plus lent

### Relations avec d'autres mod√®les

- **R√©gression lin√©aire** : Version r√©gression (output continu)
- **Perceptron** : Anc√™tre sans probabilit√©s (classification binaire)
- **SVM** : Classification avec marge (plus robuste aux outliers)
- **Naive Bayes** : Classification probabiliste (suppose ind√©pendance)
- **Neural Network** : G√©n√©ralisation avec couches cach√©es
- **Softmax Regression** : Extension multi-classe (√©quivalent √† multinomial logistic)

### M√©triques d'√©valuation

```python
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    log_loss
)

# Pr√©dictions
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# M√©triques
print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")
print(f"Precision: {precision_score(y_test, y_pred):.3f}")
print(f"Recall: {recall_score(y_test, y_pred):.3f}")
print(f"F1-Score: {f1_score(y_test, y_pred):.3f}")
print(f"AUC-ROC: {roc_auc_score(y_test, y_proba):.3f}")
print(f"Log Loss: {log_loss(y_test, y_proba):.3f}")
```

**Quelle m√©trique choisir ?**
- **Accuracy** : Donn√©es √©quilibr√©es
- **Precision** : Minimiser faux positifs (ex: spam)
- **Recall** : Minimiser faux n√©gatifs (ex: d√©tection de cancer)
- **F1-Score** : Compromis precision/recall
- **AUC-ROC** : Performance globale (invariant au seuil)

## R√©f√©rences

### Documentation et tutoriels
- **Scikit-learn** : [Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)
- **StatQuest** : [Logistic Regression Explained](https://www.youtube.com/watch?v=yIYKR4sgzI8) (YouTube)
- **Andrew Ng** : [ML Course - Classification](https://www.coursera.org/learn/machine-learning)

### Livres
- **"An Introduction to Statistical Learning"** (James et al., 2021) - Chapitre 4
- **"The Elements of Statistical Learning"** (Hastie et al., 2009) - Chapitre 4
- **"Pattern Recognition and Machine Learning"** (Bishop, 2006) - Chapitre 4

### Papers et th√©orie
- **Logistic function** : Pierre Fran√ßois Verhulst, 1838 (fonction sigmo√Øde)
- **Maximum Likelihood Estimation** : R.A. Fisher, 1922
- **Cross-Entropy Loss** : Kullback-Leibler divergence

### Outils Python
```python
# Scikit-learn (le plus populaire)
from sklearn.linear_model import LogisticRegression

# Statsmodels (plus de statistiques)
import statsmodels.api as sm
model = sm.Logit(y, X).fit()
print(model.summary())  # P-values, odds ratios, etc.

# PyTorch (deep learning framework)
import torch.nn as nn
model = nn.Sequential(
    nn.Linear(n_features, 1),
    nn.Sigmoid()
)
```

### Hyperparam√®tres cl√©s

```python
LogisticRegression(
    penalty='l2',           # 'l1', 'l2', 'elasticnet', None
    C=1.0,                  # Inverse de la r√©gularisation (plus grand = moins de r√©gularisation)
    solver='lbfgs',         # 'lbfgs', 'liblinear', 'saga', 'newton-cg', 'sag'
    max_iter=100,           # Nombre max d'it√©rations
    multi_class='auto',     # 'ovr', 'multinomial', 'auto'
    class_weight=None,      # 'balanced' pour classes d√©s√©quilibr√©es
    random_state=42
)
```

**Tuning de C** (r√©gularisation) :
```python
from sklearn.model_selection import GridSearchCV

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}
grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid.fit(X_train, y_train)
print(f"Meilleur C: {grid.best_params_['C']}")
```
