# CNN

R√©seaux convolutionnels pour images/vision et signaux locaux.

## Id√©e cl√©

**CNN (Convolutional Neural Network)** utilise des **convolutions** au lieu de connexions fully-connected pour exploiter la **structure locale** et la **translation invariance** des images. C'est l'architecture dominante en vision par ordinateur.

**Op√©ration de convolution** :
```
Image (H√óW√óC) * Kernel (K√óK√óC) = Feature map (H'√óW'√óF)

Exemple 3√ó3:
Input:           Kernel:         Output:
1  2  3          -1  0  1        
4  5  6    *     -2  0  2   =    (somme pond√©r√©e)
7  8  9          -1  0  1

‚Üí D√©tecte patterns locaux (edges, textures, formes)
```

**Architecture typique** :
```
Input Image
    ‚Üì
[Conv + ReLU + Pool] √óN  ‚Üê Feature extraction
    ‚Üì
[Flatten]
    ‚Üì
[FC Layers]              ‚Üê Classification
    ‚Üì
Output (classes)

Exemple:
28√ó28√ó1 ‚Üí Conv(3√ó3, 32) ‚Üí 26√ó26√ó32
        ‚Üí MaxPool(2√ó2)  ‚Üí 13√ó13√ó32
        ‚Üí Conv(3√ó3, 64) ‚Üí 11√ó11√ó64
        ‚Üí MaxPool(2√ó2)  ‚Üí 5√ó5√ó64
        ‚Üí Flatten       ‚Üí 1600
        ‚Üí FC(128)       ‚Üí 128
        ‚Üí FC(10)        ‚Üí 10 classes
```

**Composants cl√©s** :
1. **Convolution** : D√©tecte features locales
2. **Activation** (ReLU) : Non-lin√©arit√©
3. **Pooling** : R√©duction dimension + invariance
4. **Fully Connected** : Classification finale

**Avantages vs MLP** :
- **Moins de param√®tres** : Poids partag√©s (shared weights)
- **Translation invariance** : D√©tecte features partout
- **Hi√©rarchie** : Features simples ‚Üí complexes

**Calcul des param√®tres** :
```python
# Conv layer: kernel_size √ó kernel_size √ó in_channels √ó out_channels + bias
Conv(3√ó3, 32‚Üí64) = 3√ó3√ó32√ó64 + 64 = 18,496

# FC layer: in_features √ó out_features + bias  
FC(1600‚Üí128) = 1600√ó128 + 128 = 204,928
```

## Exemples concrets

### 1. CNN simple : MNIST digits

**Code PyTorch complet** :
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# 1. D√©finir CNN
class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        # Convolution layers
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)   # 28√ó28√ó1 ‚Üí 26√ó26√ó32
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)  # 13√ó13√ó32 ‚Üí 11√ó11√ó64
        
        # Fully connected layers
        self.fc1 = nn.Linear(64 * 5 * 5, 128)
        self.fc2 = nn.Linear(128, 10)
        
        self.dropout = nn.Dropout(0.25)
    
    def forward(self, x):
        # Conv block 1
        x = F.relu(self.conv1(x))           # 28√ó28√ó1 ‚Üí 26√ó26√ó32
        x = F.max_pool2d(x, 2)              # ‚Üí 13√ó13√ó32
        
        # Conv block 2
        x = F.relu(self.conv2(x))           # 13√ó13√ó32 ‚Üí 11√ó11√ó64
        x = F.max_pool2d(x, 2)              # ‚Üí 5√ó5√ó64
        
        # Flatten
        x = x.view(-1, 64 * 5 * 5)          # ‚Üí 1600
        
        # FC layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# 2. Charger donn√©es
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000)

# 3. Entra√Ænement
model = SimpleCNN()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(5):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        if batch_idx % 100 == 0:
            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')

# 4. √âvaluation
model.eval()
correct = 0
with torch.no_grad():
    for data, target in test_loader:
        output = model(data)
        pred = output.argmax(dim=1)
        correct += pred.eq(target).sum().item()

accuracy = 100. * correct / len(test_loader.dataset)
print(f'\nTest Accuracy: {accuracy:.2f}%')
```

---

### 2. CNN moderne : CIFAR-10 avec ResNet blocks

**Code avec skip connections** :
```python
import torch
import torch.nn as nn

class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
    
    def forward(self, x):
        residual = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual  # Skip connection
        out = F.relu(out)
        return out

class ResNetCIFAR(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        # Initial conv
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        
        # Residual blocks
        self.layer1 = nn.Sequential(
            ResidualBlock(64),
            ResidualBlock(64)
        )
        
        self.layer2 = nn.Sequential(
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            ResidualBlock(128),
            ResidualBlock(128)
        )
        
        self.layer3 = nn.Sequential(
            nn.Conv2d(128, 256, 3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            ResidualBlock(256),
            ResidualBlock(256)
        )
        
        # Global average pooling + FC
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, num_classes)
    
    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))  # 32√ó32√ó3 ‚Üí 32√ó32√ó64
        x = self.layer1(x)                    # 32√ó32√ó64
        x = self.layer2(x)                    # 16√ó16√ó128
        x = self.layer3(x)                    # 8√ó8√ó256
        x = self.avgpool(x)                   # 1√ó1√ó256
        x = x.view(x.size(0), -1)            # 256
        x = self.fc(x)                        # num_classes
        return x

model = ResNetCIFAR(num_classes=10)
print(f"Param√®tres: {sum(p.numel() for p in model.parameters()):,}")
```

---

### 3. Visualiser feature maps

**Code pour voir ce que le CNN apprend** :
```python
import torch
import matplotlib.pyplot as plt
from torchvision import datasets, transforms

# Charger mod√®le entra√Æn√©
model = SimpleCNN()
# model.load_state_dict(torch.load('model.pth'))
model.eval()

# Charger une image
transform = transforms.ToTensor()
dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)
image, label = dataset[0]
image_batch = image.unsqueeze(0)  # Ajouter batch dimension

# Hook pour capturer activations
activations = {}
def get_activation(name):
    def hook(model, input, output):
        activations[name] = output.detach()
    return hook

model.conv1.register_forward_hook(get_activation('conv1'))
model.conv2.register_forward_hook(get_activation('conv2'))

# Forward pass
with torch.no_grad():
    output = model(image_batch)

# Visualiser feature maps
fig, axes = plt.subplots(4, 8, figsize=(15, 8))
axes = axes.ravel()

# Premi√®re couche conv (32 feature maps)
act = activations['conv1'][0]  # Shape: (32, 26, 26)
for i in range(min(32, len(axes))):
    axes[i].imshow(act[i].cpu(), cmap='viridis')
    axes[i].axis('off')
    axes[i].set_title(f'Filter {i}')

plt.suptitle('Feature Maps de Conv1')
plt.tight_layout()
plt.show()

print(f"Conv1 output shape: {activations['conv1'].shape}")
print(f"Conv2 output shape: {activations['conv2'].shape}")
```

---

### 4. Transfer Learning avec mod√®les pr√©-entra√Æn√©s

**Code utilisant ResNet pre-trained** :
```python
import torch
import torchvision.models as models
import torch.nn as nn

# 1. Charger ResNet18 pr√©-entra√Æn√© sur ImageNet
resnet = models.resnet18(pretrained=True)

# 2. Remplacer derni√®re couche pour notre t√¢che (10 classes)
num_features = resnet.fc.in_features
resnet.fc = nn.Linear(num_features, 10)

# 3. Geler les couches pr√©-entra√Æn√©es (optionnel)
for param in resnet.parameters():
    param.requires_grad = False

# D√©geler seulement la derni√®re couche
for param in resnet.fc.parameters():
    param.requires_grad = True

# 4. Entra√Æner seulement FC layer
optimizer = torch.optim.Adam(resnet.fc.parameters(), lr=0.001)

# Alternative: Fine-tuning complet avec petit LR
# for param in resnet.parameters():
#     param.requires_grad = True
# optimizer = torch.optim.Adam(resnet.parameters(), lr=0.0001)

print(f"Mod√®le modifi√©:")
print(f"  Trainable params: {sum(p.numel() for p in resnet.parameters() if p.requires_grad):,}")
print(f"  Total params: {sum(p.numel() for p in resnet.parameters()):,}")
```

---

### 5. Data Augmentation

**Code avec transformations** :
```python
from torchvision import transforms

# Training transforms (avec augmentation)
train_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

# Test transforms (sans augmentation)
test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

# Visualiser augmentation
import matplotlib.pyplot as plt
from PIL import Image

img = Image.open('sample.jpg')
fig, axes = plt.subplots(2, 4, figsize=(12, 6))
axes = axes.ravel()

for i in range(8):
    augmented = train_transform(img)
    axes[i].imshow(augmented.permute(1, 2, 0))
    axes[i].axis('off')

plt.suptitle('Exemples de Data Augmentation')
plt.tight_layout()
plt.show()
```

---

### 6. Architectures classiques compar√©es

**Code comparant LeNet, AlexNet style, VGG style** :
```python
class LeNet5(nn.Module):
    """LeNet-5 (1998) - Premier CNN successful"""
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16*4*4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
    
    def forward(self, x):
        x = F.max_pool2d(F.relu(self.conv1(x)), 2)
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, 16*4*4)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class VGGStyle(nn.Module):
    """VGG-like: blocs r√©p√©t√©s de Conv + Pool"""
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            # Block 1: 64 filters
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            # Block 2: 128 filters
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            # Block 3: 256 filters
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(256 * 4 * 4, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 10)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# Comparer tailles
models_dict = {
    'LeNet-5': LeNet5(),
    'VGG-Style': VGGStyle(),
}

for name, model in models_dict.items():
    params = sum(p.numel() for p in model.parameters())
    print(f"{name}: {params:,} param√®tres")
```

## Quand l'utiliser

- ‚úÖ **Images** : Classification, d√©tection, segmentation
- ‚úÖ **Signaux 1D** : Audio, s√©ries temporelles, ECG
- ‚úÖ **Donn√©es avec structure locale** : Pixels voisins corr√©l√©s
- ‚úÖ **Translation invariance** : Features doivent √™tre d√©tect√©es partout
- ‚úÖ **Hi√©rarchie de features** : Edges ‚Üí textures ‚Üí objets

**Cas d'usage typiques** :
- üñºÔ∏è **Vision** : Classification d'images (ImageNet)
- üöó **V√©hicules autonomes** : D√©tection d'objets
- üè• **M√©dical** : Analysis de radiographies, IRM
- üì∏ **Reconnaissance faciale** : FaceNet, DeepFace
- üé® **Style transfer** : Neural style, GANs
- üîä **Audio** : Spectrogrammes ‚Üí CNN 2D

**Quand NE PAS utiliser** :
- ‚ùå Donn√©es tabulaires ‚Üí MLP, XGBoost
- ‚ùå S√©quences longues ‚Üí Transformers
- ‚ùå Graphes ‚Üí GNN
- ‚ùå Tr√®s petites images (<20√ó20) ‚Üí MLP suffit
- ‚ùå Besoin interpretabilit√© forte ‚Üí Decision trees

## Forces

‚úÖ **Translation invariance** : D√©tecte features n'importe o√π  
‚úÖ **Param√®tres partag√©s** : Moins de params que MLP  
‚úÖ **Hi√©rarchie** : Features simples ‚Üí complexes  
‚úÖ **Prouv√©** : √âtat de l'art en vision depuis 2012  
‚úÖ **Transfer learning** : Pr√©-entra√Ænement ImageNet  
‚úÖ **GPU-friendly** : Parall√©lisation efficace

**Exemple de r√©duction de param√®tres** :
```python
# MLP sur image 28√ó28
fc_params = 28*28 * 128 = 100,352

# CNN √©quivalent
conv_params = 3*3*1*32 + 3*3*32*64 = 18,720
# ‚Üí 5x moins de param√®tres!
```

## Limites

‚ùå **Beaucoup de donn√©es** : N√©cessite milliers d'exemples  
‚ùå **Computationally expensive** : Training lent  
‚ùå **Hyperparam√®tres** : Kernel size, nb layers, filters  
‚ùå **Interpr√©tabilit√©** : Bo√Æte noire  
‚ùå **Adversarial attacks** : Vuln√©rable √† perturbations  
‚ùå **Rotation/scale** : Pas naturellement invariant

**Limitation : Position sensible** :
```python
# CNN d√©tecte "chat" au centre
# Mais si on translate l'image, peut √©chouer si:
# - Pooling trop agressif
# - Pas assez de data augmentation
# ‚Üí Besoin de beaucoup d'exemples ou augmentation
```

## Variantes / liens

### Architectures historiques

**1. LeNet-5 (1998)** :
```
32√ó32 ‚Üí Conv(5√ó5, 6) ‚Üí Pool ‚Üí Conv(5√ó5, 16) ‚Üí Pool ‚Üí FC(120) ‚Üí FC(84) ‚Üí FC(10)
```

**2. AlexNet (2012)** - ImageNet breakthrough :
```
227√ó227 ‚Üí Conv(11√ó11, 96, stride=4) ‚Üí Pool ‚Üí ... ‚Üí FC(4096) ‚Üí FC(4096) ‚Üí FC(1000)
- ReLU activation
- Dropout
- Data augmentation
```

**3. VGG-16/19 (2014)** :
```
Stacks de Conv 3√ó3 + Pool
- Simple et deep
- 138M param√®tres
```

**4. ResNet (2015)** :
```python
# Skip connections r√©solvent vanishing gradient
x_out = F.relu(conv(x) + x)  # Residual connection
```

**5. EfficientNet (2019)** :
```
Compound scaling: depth, width, resolution
√âtat de l'art accuracy/efficiency
```

### Types de couches

**Convolution** :
```python
nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)

# Exemples
nn.Conv2d(3, 64, 3, padding=1)      # 3√ó3, preserve size
nn.Conv2d(64, 128, 1)                # 1√ó1, change channels
nn.Conv2d(128, 256, 3, stride=2)    # Downsample
```

**Pooling** :
```python
nn.MaxPool2d(kernel_size, stride=None)    # Max pooling
nn.AvgPool2d(kernel_size)                  # Average pooling
nn.AdaptiveAvgPool2d((1, 1))              # Global pooling
```

**Normalization** :
```python
nn.BatchNorm2d(num_features)    # Batch norm (standard)
nn.GroupNorm(num_groups, num_features)  
nn.InstanceNorm2d(num_features) # Style transfer
```

### Techniques importantes

**1. Batch Normalization** :
```python
# Normalise activations par batch
x = conv(x)
x = bn(x)    # mean=0, std=1 par batch
x = relu(x)
```

**2. Dropout** :
```python
nn.Dropout2d(p=0.25)  # Drop 25% de feature maps
```

**3. Global Average Pooling** :
```python
# Remplace FC layers
x = AdaptiveAvgPool2d((1, 1))(x)  # H√óW√óC ‚Üí 1√ó1√óC
x = x.view(x.size(0), -1)          # ‚Üí C
```

### Mod√®les pr√©-entra√Æn√©s PyTorch

```python
from torchvision import models

# Classification
resnet18 = models.resnet18(pretrained=True)
resnet50 = models.resnet50(pretrained=True)
vgg16 = models.vgg16(pretrained=True)
efficientnet_b0 = models.efficientnet_b0(pretrained=True)
mobilenet_v2 = models.mobilenet_v2(pretrained=True)

# D√©tection d'objets
faster_rcnn = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

# Segmentation
fcn = models.segmentation.fcn_resnet50(pretrained=True)
```

## R√©f√©rences

### Papers fondamentaux
- **LeNet** : LeCun et al., 1998 - "Gradient-based learning applied to document recognition"
- **AlexNet** : Krizhevsky et al., 2012 - "ImageNet Classification with Deep CNNs"
- **VGG** : Simonyan & Zisserman, 2014 - "Very Deep Convolutional Networks"
- **ResNet** : He et al., 2015 - "Deep Residual Learning for Image Recognition"
- **Batch Normalization** : Ioffe & Szegedy, 2015
- **EfficientNet** : Tan & Le, 2019

### Documentation
- **PyTorch** : [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)
- **TensorFlow/Keras** : [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/)
- **CS231n** : [Stanford CNN course](http://cs231n.stanford.edu/)

### Best practices

**Architecture design** :
```
R√®gles empiriques:
- Kernel size: 3√ó3 (standard), 5√ó5, 7√ó7 (premi√®re couche)
- Filters: doubler apr√®s chaque pool (32‚Üí64‚Üí128‚Üí256)
- Stride: 1 pour conv, 2 pour pool
- Padding: 'same' pour pr√©server taille
```

**Training tips** :
```python
# 1. Data augmentation (critical!)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])

# 2. Learning rate schedule
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

# 3. Early stopping
# 4. Gradient clipping si instable
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**Combien de donn√©es** :
```
R√®gle empirique:
- From scratch: 50k-100k+ images
- Fine-tuning: 1k-10k images
- Transfer learning (freeze): 100-1k images

‚Üí Toujours utiliser pre-trained models si possible!
```
