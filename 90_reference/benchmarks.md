# Benchmarks & Datasets de r√©f√©rence

Guide des benchmarks standards pour √©valuer les mod√®les ML/AI par domaine.

---

## üñºÔ∏è Computer Vision

### Classification d'images

| Dataset | Description | Taille | M√©trique | SOTA |
|---------|-------------|--------|----------|------|
| **MNIST** | Chiffres manuscrits | 70K images (28√ó28) | Accuracy | >99.8% |
| **CIFAR-10** | 10 classes objets | 60K images (32√ó32) | Accuracy | ~99% |
| **CIFAR-100** | 100 classes objets | 60K images (32√ó32) | Top-1 Accuracy | ~95% |
| **ImageNet** | 1000 classes | 14M images | Top-1/Top-5 Accuracy | 90%/98% (ViT) |

### D√©tection d'objets

| Dataset | Description | M√©trique | SOTA |
|---------|-------------|----------|------|
| **COCO** | 80 classes, 200K images | mAP@50-95 | ~65 mAP (DINO, 2023) |
| **Pascal VOC** | 20 classes | mAP | ~90 mAP |
| **Open Images** | 600 classes, 9M images | mAP | Variable |

### Segmentation

| Dataset | Type | M√©trique | 
|---------|------|----------|
| **ADE20K** | Semantic segmentation | mIoU |
| **Cityscapes** | Urban scenes | mIoU |
| **COCO-Stuff** | Segmentation + stuff | mIoU |

---

## üìù Natural Language Processing

### Compr√©hension (GLUE, SuperGLUE)

| Benchmark | Tasks | M√©trique | SOTA |
|-----------|-------|----------|------|
| **GLUE** | 9 t√¢ches (sentiment, entailment, etc.) | Avg score | 90+ (GPT-4) |
| **SuperGLUE** | T√¢ches plus difficiles | Avg score | 90+ (GPT-4) |
| **SQuAD** | Question answering | F1/EM | 95/90 (Human: 91/82) |
| **RACE** | Reading comprehension | Accuracy | ~95% |

### G√©n√©ration de texte

| Dataset | Task | M√©trique |
|---------|------|----------|
| **Penn Treebank** | Language modeling | Perplexity (PPL) |
| **WikiText-103** | Language modeling | PPL |
| **CNN/DailyMail** | Summarization | ROUGE-L |
| **WMT** | Translation | BLEU |

### M√©triques NLP courantes

- **BLEU** : Translation quality (0-100)
- **ROUGE** : Summarization (ROUGE-1, ROUGE-2, ROUGE-L)
- **METEOR** : Translation/generation
- **Perplexity (PPL)** : Language modeling (lower is better)
- **F1 Score** : Named Entity Recognition, QA
- **Exact Match (EM)** : Question answering

---

## üéÆ Reinforcement Learning

### Environnements classiques

| Environment | Description | M√©trique | SOTA |
|-------------|-------------|----------|------|
| **CartPole** | Balance pole | Episode reward | 500 (perfect) |
| **MountainCar** | Reach flag | Episode reward | -110 to 0 |
| **Lunar Lander** | Safe landing | Episode reward | 200+ |

### Atari 2600

| Benchmark | # Games | M√©trique | SOTA |
|-----------|---------|----------|------|
| **Atari-57** | 57 jeux Atari | Human-normalized score | 1000%+ (MuZero, Agent57) |

**Metrics** :
- **Human-normalized score** : `(Agent - Random) / (Human - Random) √ó 100%`
- **Median/Mean score** : Across all games

### Contr√¥le continu

| Benchmark | Description | M√©trique |
|-----------|-------------|----------|
| **MuJoCo** | Robotics simulation (Ant, Humanoid, etc.) | Episode reward |
| **dm_control** | DeepMind control suite | Episode reward |
| **Meta-World** | Manipulation tasks | Success rate |

---

## üìä Recommendation Systems

### Datasets

| Dataset | Domain | Size | M√©trique |
|---------|--------|------|----------|
| **MovieLens** | Movies | 100K-25M ratings | RMSE, MAE, Precision@K |
| **Netflix Prize** | Movies | 100M ratings | RMSE |
| **Amazon Reviews** | Products | Millions | NDCG, HR@K |
| **Last.fm** | Music | User-artist plays | Precision@K, Recall@K |

### M√©triques

- **RMSE/MAE** : Rating prediction error
- **Precision@K / Recall@K** : Top-K recommendations
- **NDCG@K** : Normalized Discounted Cumulative Gain
- **Hit Rate (HR@K)** : % users with ‚â•1 relevant item in top-K
- **MRR** : Mean Reciprocal Rank

---

## ‚è±Ô∏è Time Series

### Datasets

| Dataset | Domain | M√©trique |
|---------|--------|----------|
| **M4 Competition** | Forecasting (100K series) | SMAPE, MASE |
| **Electricity** | Power consumption | MAE, RMSE |
| **Traffic** | Road occupancy | MAE, RMSE |
| **ETT** | Electricity transformer (hourly) | MSE, MAE |

### M√©triques

- **MSE/RMSE** : Mean Squared Error
- **MAE** : Mean Absolute Error
- **MAPE** : Mean Absolute Percentage Error
- **SMAPE** : Symmetric MAPE
- **MASE** : Mean Absolute Scaled Error

---

## üß† Probabilistic Models

### Benchmarks

| Task | Dataset | M√©trique |
|------|---------|----------|
| **Bayesian Optimization** | Synthetic functions, HPO | Regret, convergence |
| **Gaussian Processes** | UCI regression datasets | NLPD, RMSE |
| **Kalman Filtering** | Tracking benchmarks | RMSE, tracking error |

---

## üìà Graph Neural Networks

### Datasets

| Dataset | Type | # Nodes | # Edges | Task |
|---------|------|---------|---------|------|
| **Cora** | Citation network | 2.7K | 5.4K | Node classification |
| **Citeseer** | Citation network | 3.3K | 4.7K | Node classification |
| **PubMed** | Citation network | 19K | 44K | Node classification |
| **PROTEINS** | Protein structures | ~40/graph | - | Graph classification |
| **ogbn-arxiv** | Large citation | 169K | 1.2M | Node classification |

### M√©triques

- **Node classification** : Accuracy, F1-score
- **Link prediction** : AUC, Hits@K
- **Graph classification** : Accuracy

---

## üîç Ressources utiles

### Leaderboards

- **Papers with Code** : [paperswithcode.com](https://paperswithcode.com/) - Tous domaines
- **HuggingFace Leaderboard** : NLP, Vision models
- **OpenAI Gym Leaderboard** : RL environnements
- **Kaggle** : Competitions actives

### Benchmarking Libraries

```python
# Computer Vision
from torchvision.datasets import CIFAR10, ImageNet
from torchmetrics import Accuracy

# NLP
from datasets import load_dataset  # HuggingFace
dataset = load_dataset("glue", "sst2")

# RL
import gym
env = gym.make("CartPole-v1")

# Time Series
from gluonts.dataset.repository import get_dataset
dataset = get_dataset("electricity")
```

---

## üìå Notes

- **SOTA** = State-of-the-art (meilleurs r√©sultats publi√©s)
- Les scores √©voluent constamment avec les nouveaux mod√®les
- Toujours comparer avec les m√™mes protocoles d'√©valuation
- Attention au **overfitting** sur les benchmarks publics
- Utiliser plusieurs m√©triques pour une √©valuation compl√®te

**Derni√®re mise √† jour** : Janvier 2026
